{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#New-York-City-Taxi-Trip-Duration---with-preprocessed-data-set\" data-toc-modified-id=\"New-York-City-Taxi-Trip-Duration---with-preprocessed-data-set-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>New York City Taxi Trip Duration - with preprocessed data set</a></div><div class=\"lev2 toc-item\"><a href=\"#Credit-to:\" data-toc-modified-id=\"Credit-to:-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Credit to:</a></div><div class=\"lev1 toc-item\"><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load data</a></div><div class=\"lev3 toc-item\"><a href=\"#Combine-train-data-and-eval-data\" data-toc-modified-id=\"Combine-train-data-and-eval-data-201\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Combine train data and eval data</a></div><div class=\"lev1 toc-item\"><a href=\"#Pre-process-data\" data-toc-modified-id=\"Pre-process-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pre-process data</a></div><div class=\"lev2 toc-item\"><a href=\"#Check&amp;-Fill--NaN\" data-toc-modified-id=\"Check&amp;-Fill--NaN-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Check&amp; Fill  NaN</a></div><div class=\"lev3 toc-item\"><a href=\"#FillNA-for-total_distance\" data-toc-modified-id=\"FillNA-for-total_distance-311\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>FillNA for total_distance</a></div><div class=\"lev3 toc-item\"><a href=\"#FillNaN-for-number_of_streets\" data-toc-modified-id=\"FillNaN-for-number_of_streets-312\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>FillNaN for number_of_streets</a></div><div class=\"lev2 toc-item\"><a href=\"#Transform-object-data\" data-toc-modified-id=\"Transform-object-data-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Transform object data</a></div><div class=\"lev2 toc-item\"><a href=\"#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Feature engineering</a></div><div class=\"lev3 toc-item\"><a href=\"#Check-data\" data-toc-modified-id=\"Check-data-331\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Check data</a></div><div class=\"lev2 toc-item\"><a href=\"#Clearning-data\" data-toc-modified-id=\"Clearning-data-34\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Clearning data</a></div><div class=\"lev3 toc-item\"><a href=\"#Split-train_set-and-eval_set\" data-toc-modified-id=\"Split-train_set-and-eval_set-341\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Split train_set and eval_set</a></div><div class=\"lev3 toc-item\"><a href=\"#Trip-duration-outliner\" data-toc-modified-id=\"Trip-duration-outliner-342\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Trip duration outliner</a></div><div class=\"lev4 toc-item\"><a href=\"#Search-for-outliner\" data-toc-modified-id=\"Search-for-outliner-3421\"><span class=\"toc-item-num\">3.4.2.1&nbsp;&nbsp;</span>Search for outliner</a></div><div class=\"lev4 toc-item\"><a href=\"#Drop-trip-duration-outliner\" data-toc-modified-id=\"Drop-trip-duration-outliner-3422\"><span class=\"toc-item-num\">3.4.2.2&nbsp;&nbsp;</span>Drop trip-duration outliner</a></div><div class=\"lev4 toc-item\"><a href=\"#drop-trip_duration=0\" data-toc-modified-id=\"drop-trip_duration=0-3423\"><span class=\"toc-item-num\">3.4.2.3&nbsp;&nbsp;</span>drop trip_duration=0</a></div><div class=\"lev3 toc-item\"><a href=\"#Trip-duration-distribution\" data-toc-modified-id=\"Trip-duration-distribution-343\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>Trip duration distribution</a></div><div class=\"lev2 toc-item\"><a href=\"#Feature-scaling-(not-use)\" data-toc-modified-id=\"Feature-scaling-(not-use)-35\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Feature scaling (not use)</a></div><div class=\"lev2 toc-item\"><a href=\"#Feature-correlation\" data-toc-modified-id=\"Feature-correlation-36\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Feature correlation</a></div><div class=\"lev1 toc-item\"><a href=\"#Train-model\" data-toc-modified-id=\"Train-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Train model</a></div><div class=\"lev2 toc-item\"><a href=\"#Model-definition\" data-toc-modified-id=\"Model-definition-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Model definition</a></div><div class=\"lev3 toc-item\"><a href=\"#Split-train/test-set\" data-toc-modified-id=\"Split-train/test-set-411\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Split train/test set</a></div><div class=\"lev2 toc-item\"><a href=\"#Train-data\" data-toc-modified-id=\"Train-data-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Train data</a></div><div class=\"lev3 toc-item\"><a href=\"#Predict-for-test-data\" data-toc-modified-id=\"Predict-for-test-data-421\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Predict for test data</a></div><div class=\"lev3 toc-item\"><a href=\"#Feature-importances\" data-toc-modified-id=\"Feature-importances-422\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Feature importances</a></div><div class=\"lev1 toc-item\"><a href=\"#Predict-and-save-submission\" data-toc-modified-id=\"Predict-and-save-submission-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Predict and save submission</a></div><div class=\"lev2 toc-item\"><a href=\"#Analyse-output\" data-toc-modified-id=\"Analyse-output-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Analyse output</a></div><div class=\"lev2 toc-item\"><a href=\"#Save-prediction\" data-toc-modified-id=\"Save-prediction-52\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Save prediction</a></div><div class=\"lev2 toc-item\"><a href=\"#Leader-board\" data-toc-modified-id=\"Leader-board-53\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Leader board</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York City Taxi Trip Duration - with preprocessed data set\n",
    "- Share code and data to improve ride time predictions\n",
    "- https://www.kaggle.com/c/nyc-taxi-trip-duration/data\n",
    "- install: http://jupyter-contrib-nbextensions.readthedocs.io/en/latest/install.html\n",
    "- install: https://github.com/Jupyter-contrib/jupyter_nbextensions_configurator\n",
    "- install GDAL: https://sandbox.idre.ucla.edu/sandbox/tutorials/installing-gdal-for-windows\n",
    "- install osmnx: http://geoffboeing.com/2014/09/using-geopandas-windows/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit to:\n",
    "- https://www.kaggle.com/ankasor/driving-distance-using-open-street-maps-data/notebook\n",
    "- https://www.kaggle.com/oscarleo/new-york-city-taxi-with-osrm\n",
    "- https://www.kaggle.com/gaborfodor/from-eda-to-the-top-lb-0-368/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from numpy import sort\n",
    "\n",
    "# ML\n",
    "# # Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, learning_curve, validation_curve, KFold\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# # XGB\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "# # CatBoost\n",
    "#from catboost import Pool, CatBoostRegressor, cv, CatboostIpythonWidget\n",
    "# System\n",
    "import datetime as dtime\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from inspect import getsourcefile\n",
    "import os.path\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Other\n",
    "from geographiclib.geodesic import Geodesic\n",
    "#import osmnx as ox\n",
    "#import networkx as nx\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "# Input data files are available in the DATA_DIR directory.\n",
    "DATA_DIR = \"data-temp\"\n",
    "# Load data. Download from:https://www.kaggle.com/c/nyc-taxi-trip-duration/data\n",
    "train_data = pd.read_csv(DATA_DIR + \"/train_pre.csv\")\n",
    "eval_data =  pd.read_csv(DATA_DIR + \"/test_pre.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train size:\", train_data.shape, \" test size:\", eval_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cols = np.setdiff1d(train_data.columns.values, eval_data.columns.values)\n",
    "diff_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine train data and eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'trip_duration'\n",
    "features = eval_data.columns.values\n",
    "target = train_data[label]\n",
    "combine_data = pd.concat([train_data[features], eval_data], keys=['train','eval'])\n",
    "print(\"combine data:\", len(combine_data))\n",
    "combine_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_data\n",
    "del eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check& Fill  NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_null_data(data):\n",
    "    #Get high percent of NaN data\n",
    "    null_data = data.isnull()\n",
    "    total = null_data.sum().sort_values(ascending=False)\n",
    "    percent = (null_data.sum()/null_data.count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    high_percent_miss_data = missing_data[missing_data['Percent']>0]\n",
    "    #print(missing_data)\n",
    "    print(high_percent_miss_data)\n",
    "    miss_data_cols = high_percent_miss_data.index.values\n",
    "    return miss_data_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine data for null\n",
    "check_null_data(combine_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combine_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FillNA for total_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine_data['total_distance'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FillNaN for number_of_streets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine_data['number_of_streets'].fillna(1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform object data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = combine_data.loc['train'][:10].copy()\n",
    "#print(len(data))\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_null_data(combine_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clearning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train_set and eval_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combine_data\n",
    "train_set = data.loc['train']\n",
    "eval_set = data.loc['eval']\n",
    "data = train_set\n",
    "data[label] = target\n",
    "target_log = np.log(target)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip duration outliner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Search for outliner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data.index, data[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Note: Outliners are trip_duration> 1800000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_set\n",
    "data_ol = data[data[label] > 1800000]\n",
    "data_ol[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop trip-duration outliner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ol = data[data[label] < 1800000]\n",
    "plt.scatter(data_ol.index, data_ol[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = data_ol\n",
    "data = train_set\n",
    "# use np.log to balance distribution\n",
    "target_log = np.log(train_set[label].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop trip_duration=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop trip_duration=0\n",
    "data_zero = data[data[label]==0]\n",
    "print(len(data_zero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = train_set[train_set[label]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set[label].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = train_set[label]\n",
    "target_log = np.log(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip duration distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "sns.distplot(data[label], bins=50)\n",
    "plt.title('Original Data')\n",
    "plt.xlabel(label)\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(target_log, bins=50)\n",
    "plt.title('Natural Log of Data')\n",
    "plt.xlabel('Natural Log of ' + label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling (not use)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = combine_data_tf.drop('id', axis=1).astype(float)\n",
    "std_scale = data.apply(StandardScaler().fit_transform)\n",
    "combine_data_tf.update(std_scale)\n",
    "combine_data_tf[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "combine_data_tf['pickup_year'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Drop pickup year\n",
    "# combine_data_tf.drop('pickup_year', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = data.corr()[label].sort_values()[-20:]\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "corrmat = data.corr()\n",
    "k = 15 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, label)[label].index\n",
    "print(cols.values)\n",
    "cm = np.corrcoef(data[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of cols except label\n",
    "other_cols = np.setdiff1d(cols.values, label)\n",
    "cm = np.corrcoef(data[other_cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=other_cols, xticklabels=other_cols)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_set.drop(['id', label], axis=1).astype(float)\n",
    "print(data.shape)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function to calculate Root Mean Squared Logarithmic Error (RMSLE)\n",
    "# credit: https://www.kaggle.com/marknagelberg/rmsle-function\n",
    "\n",
    "\n",
    "def rmsle(y, y_pred, log=True):\n",
    "    assert len(y) == len(y_pred)\n",
    "    terms_to_sum = 0\n",
    "    if log:\n",
    "        terms_to_sum = [(math.log(math.fabs(y_pred[i]) + 1) -\n",
    "                         math.log(y[i] + 1)) ** 2.0 for i, pred in enumerate(y_pred)]\n",
    "    else:\n",
    "        terms_to_sum = [(math.fabs(y_pred[i]) - y[i]) **\n",
    "                        2.0 for i, pred in enumerate(y_pred)]\n",
    "    return (sum(terms_to_sum) * (1.0 / len(y))) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = CatBoostRegressor(iterations=100, depth=5, thread_count=4, use_best_model=True)\n",
    "model = XGBRegressor(n_estimators=10000, max_depth=5,\n",
    "                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    data, target_log, train_size=0.85, random_state=1234)\n",
    "print(\"X_train:\", X_train.shape, \" Y_train:\", Y_train.shape,\n",
    "      \" X_test:\", X_test.shape, \" Y_test:\", Y_test.shape)\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "early_stopping_rounds = 50\n",
    "model.fit(\n",
    "    X_train, Y_train, eval_set = [(X_test, Y_test)],\n",
    "    eval_metric=\"rmse\", early_stopping_rounds=early_stopping_rounds,\n",
    "    verbose=early_stopping_rounds\n",
    ")\n",
    "end = time.time() - start\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "end = time.time() - start\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate score\n",
    "print(y_pred[:5])\n",
    "score = rmsle(Y_test.values, y_pred)\n",
    "score1 = rmsle(Y_test.values, y_pred, log=False)\n",
    "print(\"RMSLE score:\", score, \" RMSLE without-log:\", score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "data = eval_set.drop('id', axis=1).astype(float)\n",
    "Y_eval_log = model.predict(data)\n",
    "Y_eval = np.exp(Y_eval_log.ravel())\n",
    "end = time.time() - start\n",
    "print(end)\n",
    "print(Y_eval_log[:5])\n",
    "print(Y_eval[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_output = eval_set.copy()\n",
    "eval_output.loc[:, 'trip_duration'] = Y_eval\n",
    "eval_output[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#eval_output = pd.DataFrame({'id': eval_set['id'], 'haversine_distance': eval_set['haversine_distance'], 'total_distance:': eval_set['total_distance'], 'trip_duration': Y_eval}, columns=['id', 'haversine_distance', 'total_distance', 'trip_duration'])\n",
    "print(len(eval_output))\n",
    "#eval_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_zero = eval_output[eval_output['haversine_distance']==0]\n",
    "print(len(data_zero))\n",
    "data_zero[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#eval_output.loc[eval_output['haversine_distance']==0, 'trip_duration'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "today = str(dtime.date.today())\n",
    "print(today)\n",
    "eval_output[['id', 'trip_duration']].to_csv(DATA_DIR +'/' +today+'-submission.csv.gz',index=False, compression='gzip')\n",
    "end = time.time() - start\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Leader board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Round 12:\n",
    "    + Base model: \n",
    "        * XGBRegressor(n_estimators=10000, max_depth=5,\n",
    "                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)\n",
    "\t* remove pickup_year (correl = 0)\n",
    "\t* remove outliners: train_set[label] > 1800000\n",
    "    * remove outliners: train_set['total_distance_mean] = 0\n",
    "\t* Added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)\n",
    "\t* Added distance between pickup and dropoff using haversine dataset\n",
    "\t* correct speed_mean formular\n",
    "\t* train_size = 0.85\n",
    "\t* target_log = np.log(target)\n",
    "\t* best model train round: \n",
    "\t* train time: \n",
    "\t* RMSLE without-log: 0.383703421188 => \n",
    "    + Submision score: 0.390 => \t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Round 13:\n",
    "    + Base model: \n",
    "        * XGBRegressor(n_estimators=10000, max_depth=5,\n",
    "                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)\n",
    "        * remove outliners: train_set[label] > 1800000\n",
    "        * added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)\n",
    "        * added distance between pickup and dropoff using haversine dataset\n",
    "        * added number_of_streets, starting_street, end_street\n",
    "        * predict total_distance (for total_distance=0)\n",
    "        * predict starting_street, end_street (for NaN starting_street, end_street)\n",
    "        * added pickup_hour_speed_mean, pickup_weekday_speed_mean, pickup_day_speed_mean, pickup_month_speed_mean\n",
    "        * train_size = 0.85\n",
    "        * target_log = np.log(target)\n",
    "        * best model train round: 2567\n",
    "        * train time: 1943.313801765442\n",
    "        * RMSLE without-log: 0.384153 => 0.36939\n",
    "    + Submision score:  0.428\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Round 14:\n",
    "    + Base model: \n",
    "        * XGBRegressor(n_estimators=10000, max_depth=5,\n",
    "                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)\n",
    "\t* remove outliners: train_set[label] > 1800000\n",
    "\t* added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)\n",
    "\t* added distance between pickup and dropoff using haversine dataset\n",
    "\t* added number_of_streets, starting_street, end_street\n",
    "\t* predict total_distance (for total_distance=0)\n",
    "\t* predict starting_street, end_street (for NaN starting_street, end_street)\n",
    "\t* added pickup_hour_speed_mean, pickup_weekday_speed_mean, pickup_day_speed_mean, pickup_month_speed_mean\n",
    "\t* added hour_duration_mean, weekday_duration_mean, day_duration_mean, month_duration_mean\n",
    "\t* train_size = 0.85\n",
    "\t* target_log = np.log(target)\n",
    "\t* best model train round: 3001\n",
    "\t* train time: 3092.225347518921\n",
    "\t* RMSLE without-log: 0.370031041363\n",
    "    + Submision score: 0.422 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Round 15:\n",
    "+ Base model: Round 14\n",
    "\t* Set trip_duration=0 if haversin_distance=0\n",
    "    + Submision score: 0.422 => 0.504"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Round 16:\n",
    "    + Base model: Round 14\n",
    "\t* No feature engineering for total_distance + trip_duration (check if haversine=0)\n",
    "\t* best model train round: 2263\n",
    "\t* train time: 2406.979811668396\n",
    "\t* RMSLE without-log: 0.3824341923\n",
    "    + Submision score: 0.388"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "537px",
    "left": "0px",
    "right": "1090px",
    "top": "107px",
    "width": "270px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "607px",
    "left": "1139px",
    "right": "20px",
    "top": "68px",
    "width": "310px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
