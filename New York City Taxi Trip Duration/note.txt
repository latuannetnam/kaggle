
0.: Requirements
 - use pipreqs ./ to regenerate requirements file
 - pip3 install -r requirments.txt
 - Ubuntu:
	+ sudo apt-get install libspatialindex-dev
	+ sudo apt-get install libgeos-dev
 - CentOS:
	+ yum install -y spatialindex-devel
	+ yum install -y geos-devel
	+ yum -y install python34-tkinter
0.1:
 - Best model: round 3: 0.417
 - Best model: round 8: 0.401 (added distance using haversine)
 - Best model: round 11: 0.390 (added distance using OSM + haversine distance, speed_mean, total_distance_mean)
 - Best model: round 21: 0.387 (removed feature: speed_mean and duration_mean by haversine)
 - Best model: round 22: 0.386 (KFold=5)
- Best model: round 24: 0.381 (optimized model hyper-params: 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10)
1. 2017-07-21
- Round 1:
    + Base model: 
        * XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,
       max_depth=5, min_child_weight=1, missing=None, n_estimators=2000,
       n_jobs=-1, nthread=None, objective='reg:linear', random_state=0,
       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
       silent=True, subsample=1)
	* use xgboost.cv
	* train_size = 0.85
	* target_log = np.log(target)
	* train time: 924.1382086277008
	* RMSLE without-log: 0.423621130054
    + Submision score: 0.427 
- Round 2:
    + Base model: 
        * XGBRegressor(n_estimators=5000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove outliner: train_set[label] > 5000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 5000
	* train time: 2399.4174466133118
	* RMSLE without-log: 0.374140060512
    + Submision score: 0.419
- Round 3:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.3, min_child_weight=1, n_jobs=-1)
	* remove outliner: train_set[label] > 5000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 3921
	* train time: 1854.232224702835
	* RMSLE without-log: 0.374140060512 => 0.371386768324
    + Submision score: 0.419 =>  0.417
- Round 4:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* all train data (not remove outliners)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 5093
	* train time: 2379.7198255062103
	* RMSLE without-log: 0.374140060512 => 0.41737086459
    + Submision score: 0.421
- Round 5:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.3, min_child_weight=1, n_jobs=-1)
	* feature scaling
	* remove pickup_year (correl = 0)
	* remove outliner: train_set[label] > 5000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1403
	* train time: 668.2613804340363
	* RMSLE without-log: 0.37665957093
    + Submision score: 0.417 => 0.422
2. 2017--7-23:
- Round 6:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* feature scaling
	* remove pickup_year (correl = 0)
	* remove outliner: train_set[label] > 5000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2413
	* train time: 1149.2097866535187
	* RMSLE without-log: 0.37665957093 => 0.379491160168
    + Submision score: 0.422 => 0.424
- Round 7:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 3107
	* train time: 1545.7049226760864
	* RMSLE without-log: 0.419480465158 
    + Submision score: 0.424
- Round 8:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* Added distance between pickup and dropoff using haversine
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2983 
	* train time: 1751.8312721252441
	* RMSLE without-log: 0.419480465158 => 0.395957129069
    + Submision score: 0.424 => 0.401
3: 2017-07-26:
- Round 9:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* Added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 3124
	* train time: 1665.9864552021027
	* RMSLE without-log: 0.388475183465
    + Submision score: 0.393
- Round 10:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* Added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* Added distance between pickup and dropoff using haversine dataset
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1874
	* train time: 1238.130537033081
	* RMSLE without-log: 0.388475183465 => 0.38546321876
    + Submision score: 0.393 => 0.391
- Round 11:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* Added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* Added distance between pickup and dropoff using haversine dataset
	* added speed_mean, total_distance_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2853
	* train time: 1934.7547209262848
	* RMSLE without-log: 0.38546321876 => 0.383703421188
    + Submision score: 0.391 => 0.390
4: 2017-07-27:
- Round 12:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* remove outliners: train_set['total_distance_mean] = 0
	* Added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* Added distance between pickup and dropoff using haversine dataset
	* correct speed_mean formular
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2985
	* train time: 1968.406958580017
	* RMSLE without-log: 0.383703421188 => 0.384153
    + Submision score:  
5: 2017-07-28:	
- Round 13:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove outliners: train_set[label] > 1800000
	* added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* added distance between pickup and dropoff using haversine dataset
	* added number_of_streets, starting_street, end_street
	* predict total_distance (for total_distance=0)
	* predict starting_street, end_street (for NaN starting_street, end_street)
	* added pickup_hour_speed_mean, pickup_weekday_speed_mean, pickup_day_speed_mean, pickup_month_speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2567
	* train time: 1943.313801765442
	* RMSLE without-log: 0.384153 => 0.369416883687
    + Submision score:  0.384153 => 0.428		
6: 2017-07-29:
- Round 14:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove outliners: train_set[label] > 1800000
	* added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* added distance between pickup and dropoff using haversine dataset
	* added number_of_streets, starting_street, end_street
	* predict total_distance (for total_distance=0)
	* predict starting_street, end_street (for NaN starting_street, end_street)
	* added pickup_hour_speed_mean, pickup_weekday_speed_mean, pickup_day_speed_mean, pickup_month_speed_mean
	* added hour_duration_mean, weekday_duration_mean, day_duration_mean, month_duration_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 3001
	* train time: 3092.225347518921
	* RMSLE without-log: 0.370031041363
    + Submision score: 0.422 
- Round 15:
    + Base model: Round 14
	* Set trip_duration=0 if haversin_distance=0
    + Submision score: 0.422 => 0.504
- Round 16:
    + Base model: Round 14
	* No feature engineering for total_distance + trip_duration (check if haversine=0)
	* best model train round: 2263
	* train time: 2406.979811668396
	* RMSLE without-log: 0.3824341923
    + Submision score: 0.388
- Round 17:
    + Base model: Round 16
	* change to haversine_np for faster computation time
	* best model train round: 2971
	* train time: 3083.501992702484
	* RMSLE without-log: 0.38154559936 
    + Submision score: 0.388 => 0.387
- Round 18:
    + Base model: Round 17
	* added new feature: speed_mean and duration_mean by haversine
	* best model train round: 
	* train time: 
	* RMSLE without-log: 0.38154559936 => 0.382880991297
    + Submision score: 0.387 => 0.389
7: 2017-07-30:
- Round 19:
    + Base model: Round 18
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.3, min_child_weight=1, n_jobs=-1)
	* best model train round: 723
	* train time: 1064.4129366874695
	* RMSLE without-log: 0.38513216986
    + Submision score: 0.390
- Round 20:
    + Base model: Round 18
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* new data set: fastest_route
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1130
	* train time: 1774.263885974884
	* RMSLE without-log: 0.382655151773
    + Submision score: 0.390
- Round 21:
    + Base model: Round 20
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* new data set: fastest_route
	* removed feature: speed_mean and duration_mean by haversine
	* removed pickup_year
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2546
	* train time: 2927.3975234031677
	* RMSLE without-log: 0.382655151773 => 0.378934064175
    + Submision score: 0.387 
- Round 22:
    + Base model: Round 21
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* new data set: fastest_route
	* used Kfold = 5
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2522 + 2127 + 2264 + 1953 + 2706 
	* train time: 2552.5379943847656 + 2201.945223093033 + 2397.255286216736 + 2121.966084718704 + 2982.0862629413605 =  12304.544
	* RMSLE without-log: (0.384708311796 + 0.380577074349 + 0.384441915744 + 0.38211835455 + 0.379893830343)/5 = 0.382347897357
    + Submision score: 0.387 => 0.386
- Round 23:
    + Base model: Round 22
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* new data set: fastest_route
	* used Kfold = 5 + aggregate results
	* stack train model 
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round:
	* best stack model train round: 434
	* train time: 257.1784813404083
	* RMSLE without-log:  0.331885656985 (stack model)
    + Submision score: 0.396

7: 2017-07-31:
- Round 24:
    + Base model: Round 21
	* model = XGBRegressor(n_estimators=10000, max_depth=10,
                     learning_rate=0.1, min_child_weight=5, n_jobs=-1)
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 805
	* train time: 1558.1954226493835
	* RMSLE without-log: 0.373046115317
    + Submision score: 0.381
- Round 25:
    + Base model: Round 24
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* used Kfold = 5
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1180 + 878 + 814 + 1100 + 1148 
	* train time: 2103.3532185554504 + 2112.0196216106415 + 1463.224598646164 + 2277.286563396454 + 1997.4008457660675 = 9953.284847974777
	* RMSLE without-log: (0.378574601563 + 0.374263204893 + 0.377778683159 + 0.374609691487 + 0.373672468293 )/5 = 0.375779729879
    + Submision score: 0.381 => 0.381	
- Round 26:
    + Base model: Round 24
	* new data set: fastest_route
	# 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 10}
	* remove haversin_distance
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2682
	* train time: 2100.6572234630585
	* RMSLE without-log: 0.383296609142
    + Submision score:  
- Round 27:
    + Base model: Round 24
	* new data set: fastest_route
	* 'max_depth': 5, 'learning_rate': 0.1, 'min_child_weight': 5
	* added starting_street_tf_speed_mean, end_street_tf_speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1669
	* train time: 1449.917546749115
	* RMSLE without-log: 0.3808412094
    + Submision score: 
- Round 28:
    + Base model: Round 27
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1054
	* train time: 2144.4699709415436
	* RMSLE without-log: 0.373046115317 => 0.372649050678
    + Submision score: 	
- Round 29:
    + Base model: Round 28
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* added  starting_street_tf_duration_mean, end_street_tf_duration_mean
	* cleanup data (trip_duration> 1800000) before pre-process
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1121
	* train time: 3381.792198896408
	* RMSLE without-log: 0.372649050678 => 0.373172177982
    + Submision score: 0.382		
- Round 30:
    + Base model: Round 29
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* remove duration_mean	
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 636
	* train time: 1033.3630764484406
	* RMSLE without-log: 0.373172177982 => 0.373016098776
    + Submision score: 
- Round 31:
    + Base model: Round 30
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* added number_of_steps_speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 611
	* train time: 1452.7844805717468
	* RMSLE without-log: 0.373172177982 => 0.372985129101
    + Submision score: 0.382
- Round 32:
    + Base model: Round 31
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* added distance_per_step
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 763
	* train time: 1941.8781354427338
	* RMSLE without-log: 0.372985129101 => 0.373206600463
    + Submision score: 0.382 =>  0.382	

8: 2017-08-01:
- Round 33:
    + Base model: Round 32
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* removed distance_per_step
	* added manhattan_distance
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 851
	* train time: 2343.4766597747803
	* RMSLE without-log: 0.373206600463 => 0.373096205348
    + Submision score: 0.381
- Round 34:
    + Base model: Round 33
	* new data set: fastest_route + weather
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 882
	* train time: 2633.0243830680847
	* RMSLE without-log: 0.373096205348 => 0.372926464895
    + Submision score: 0.381
- Round 35:
    + Base model: Round 34
	* new data set: fastest_route 
	* removed data set: weather
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* added new features: total_turns (from step_maneuvers)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1101
	* train time: 2497.3055131435394
	* RMSLE without-log: 0.373137069604
    + Submision score: 0.381
