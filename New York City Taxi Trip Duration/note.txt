
0.: Requirements
 - use pipreqs ./ to regenerate requirements file
 - pip3 install -r requirments.txt
 - Ubuntu:
	+ sudo apt-get install libspatialindex-dev
	+ sudo apt-get install libgeos-dev
 - CentOS:
	+ yum install -y spatialindex-devel
	+ yum install -y geos-devel
	+ yum -y install python34-tkinter
0.1:
 - Best model: round 3: 0.417
 - Best model: round 8: 0.401 (added distance using haversine)
 - Best model: round 11: 0.390 (added distance using OSM + haversine distance, speed_mean, total_distance_mean)
 - Best model: round 21: 0.387 (removed feature: speed_mean and duration_mean by haversine)
 - Best model: round 22: 0.386 (KFold=5)
 - Best model: round 24: 0.381 (optimized model hyper-params: 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10)
 - Best model: round 28: 0.380 (cleanup data: (data[label] < 22 * 3600) & (data[label] > 10) + 'learning_rate': 0.1, 'max_depth': 10, 'colsample_bytree': 0.9, 'min_child_weight': 1)
 - Best model: round 95: 0.37627 (Stack model: XGBRegressor (0.1) + LGBMRegressor (0.01) => XGBRegressor (0.03) )
 - Best model: round 96: 0.37399 (Stack model: XGBRegressor (0.1) + LGBMRegressor (0.01) => XGBRegressor (0.03)) + new feature: directon
 - Best model: round 98: 0.37370 (Stack model: CatBoostRegressor (0.1) + XGBRegressor (0.1) + LGBMRegressor (0.01) => XGBRegressor (0.03)) + new feature: directon 
 - Best model: round 108: 0.37330 (Stack model + kfold (base on round 98))
 - Best model: round 109: 0.37304 (Stack model + kfold (removed cluster_count)) 
 - Best model: round 110: 0.37273 (Stack model + kfold (added weather_data)) 
 - Best model: round 113: 0.37257 (Stack model + kfold (added weather_data, col_use= 'Temp.', 'Precip', 'Wind Speed', 'Dew Point', 'Visibility','snow', 'rain', 'fog', 'heavy_snow', 'heavy_rain')) 
 - Best model: round 114: 0.37157 (Stack model + kfold (added weather_data,PCA for location))  
 - Best model: round 115: 0.37050 (Stack model + kfold (added weather_data,PCA for location, removed: cleanup invalid trip_duration))=> reduce overfit 
 - Best model: round 116: 0.36829 (Stack model + kfold (added weather_data,PCA for location, cleanup trip_duration> 22*3600))
 
 
1. 2017-07-21
- Round 1:
    + Base model: 
        * XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,
       max_depth=5, min_child_weight=1, missing=None, n_estimators=2000,
       n_jobs=-1, nthread=None, objective='reg:linear', random_state=0,
       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
       silent=True, subsample=1)
	* use xgboost.cv
	* train_size = 0.85
	* target_log = np.log(target)
	* train time: 924.1382086277008
	* RMSLE without-log: 0.423621130054
    + Submision score: 0.427 
- Round 2:
    + Base model: 
        * XGBRegressor(n_estimators=5000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove outliner: train_set[label] > 5000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 5000
	* train time: 2399.4174466133118
	* RMSLE without-log: 0.374140060512
    + Submision score: 0.419
- Round 3:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.3, min_child_weight=1, n_jobs=-1)
	* remove outliner: train_set[label] > 5000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 3921
	* train time: 1854.232224702835
	* RMSLE without-log: 0.374140060512 => 0.371386768324
    + Submision score: 0.419 =>  0.417
- Round 4:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* all train data (not remove outliners)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 5093
	* train time: 2379.7198255062103
	* RMSLE without-log: 0.374140060512 => 0.41737086459
    + Submision score: 0.421
- Round 5:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.3, min_child_weight=1, n_jobs=-1)
	* feature scaling
	* remove pickup_year (correl = 0)
	* remove outliner: train_set[label] > 5000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1403
	* train time: 668.2613804340363
	* RMSLE without-log: 0.37665957093
    + Submision score: 0.417 => 0.422
2. 2017--7-23:
- Round 6:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* feature scaling
	* remove pickup_year (correl = 0)
	* remove outliner: train_set[label] > 5000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2413
	* train time: 1149.2097866535187
	* RMSLE without-log: 0.37665957093 => 0.379491160168
    + Submision score: 0.422 => 0.424
- Round 7:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 3107
	* train time: 1545.7049226760864
	* RMSLE without-log: 0.419480465158 
    + Submision score: 0.424
- Round 8:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* Added distance between pickup and dropoff using haversine
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2983 
	* train time: 1751.8312721252441
	* RMSLE without-log: 0.419480465158 => 0.395957129069
    + Submision score: 0.424 => 0.401
3: 2017-07-26:
- Round 9:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* Added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 3124
	* train time: 1665.9864552021027
	* RMSLE without-log: 0.388475183465
    + Submision score: 0.393
- Round 10:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* Added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* Added distance between pickup and dropoff using haversine dataset
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1874
	* train time: 1238.130537033081
	* RMSLE without-log: 0.388475183465 => 0.38546321876
    + Submision score: 0.393 => 0.391
- Round 11:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* Added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* Added distance between pickup and dropoff using haversine dataset
	* added speed_mean, total_distance_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2853
	* train time: 1934.7547209262848
	* RMSLE without-log: 0.38546321876 => 0.383703421188
    + Submision score: 0.391 => 0.390
4: 2017-07-27:
- Round 12:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* remove outliners: train_set['total_distance_mean] = 0
	* Added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* Added distance between pickup and dropoff using haversine dataset
	* correct speed_mean formular
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2985
	* train time: 1968.406958580017
	* RMSLE without-log: 0.383703421188 => 0.384153
    + Submision score:  
5: 2017-07-28:	
- Round 13:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove outliners: train_set[label] > 1800000
	* added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* added distance between pickup and dropoff using haversine dataset
	* added number_of_streets, starting_street, end_street
	* predict total_distance (for total_distance=0)
	* predict starting_street, end_street (for NaN starting_street, end_street)
	* added pickup_hour_speed_mean, pickup_weekday_speed_mean, pickup_day_speed_mean, pickup_month_speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2567
	* train time: 1943.313801765442
	* RMSLE without-log: 0.384153 => 0.369416883687
    + Submision score:  0.384153 => 0.428		
6: 2017-07-29:
- Round 14:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove outliners: train_set[label] > 1800000
	* added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* added distance between pickup and dropoff using haversine dataset
	* added number_of_streets, starting_street, end_street
	* predict total_distance (for total_distance=0)
	* predict starting_street, end_street (for NaN starting_street, end_street)
	* added pickup_hour_speed_mean, pickup_weekday_speed_mean, pickup_day_speed_mean, pickup_month_speed_mean
	* added hour_duration_mean, weekday_duration_mean, day_duration_mean, month_duration_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 3001
	* train time: 3092.225347518921
	* RMSLE without-log: 0.370031041363
    + Submision score: 0.422 
- Round 15:
    + Base model: Round 14
	* Set trip_duration=0 if haversin_distance=0
    + Submision score: 0.422 => 0.504
- Round 16:
    + Base model: Round 14
	* No feature engineering for total_distance + trip_duration (check if haversine=0)
	* best model train round: 2263
	* train time: 2406.979811668396
	* RMSLE without-log: 0.3824341923
    + Submision score: 0.388
- Round 17:
    + Base model: Round 16
	* change to haversine_np for faster computation time
	* best model train round: 2971
	* train time: 3083.501992702484
	* RMSLE without-log: 0.38154559936 
    + Submision score: 0.388 => 0.387
- Round 18:
    + Base model: Round 17
	* added new feature: speed_mean and duration_mean by haversine
	* best model train round: 
	* train time: 
	* RMSLE without-log: 0.38154559936 => 0.382880991297
    + Submision score: 0.387 => 0.389
7: 2017-07-30:
- Round 19:
    + Base model: Round 18
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.3, min_child_weight=1, n_jobs=-1)
	* best model train round: 723
	* train time: 1064.4129366874695
	* RMSLE without-log: 0.38513216986
    + Submision score: 0.390
- Round 20:
    + Base model: Round 18
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* new data set: fastest_route
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1130
	* train time: 1774.263885974884
	* RMSLE without-log: 0.382655151773
    + Submision score: 0.390
- Round 21:
    + Base model: Round 20
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* new data set: fastest_route
	* removed feature: speed_mean and duration_mean by haversine
	* removed pickup_year
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2546
	* train time: 2927.3975234031677
	* RMSLE without-log: 0.382655151773 => 0.378934064175
    + Submision score: 0.387 
- Round 22:
    + Base model: Round 21
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* new data set: fastest_route
	* used Kfold = 5
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2522 + 2127 + 2264 + 1953 + 2706 
	* train time: 2552.5379943847656 + 2201.945223093033 + 2397.255286216736 + 2121.966084718704 + 2982.0862629413605 =  12304.544
	* RMSLE without-log: (0.384708311796 + 0.380577074349 + 0.384441915744 + 0.38211835455 + 0.379893830343)/5 = 0.382347897357
    + Submision score: 0.387 => 0.386
- Round 23:
    + Base model: Round 22
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* new data set: fastest_route
	* used Kfold = 5 + aggregate results
	* stack train model 
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round:
	* best stack model train round: 434
	* train time: 257.1784813404083
	* RMSLE without-log:  0.331885656985 (stack model)
    + Submision score: 0.396

7: 2017-07-31:
- Round 24:
    + Base model: Round 21
	* model = XGBRegressor(n_estimators=10000, max_depth=10,
                     learning_rate=0.1, min_child_weight=5, n_jobs=-1)
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 805
	* train time: 1558.1954226493835
	* RMSLE without-log: 0.373046115317
    + Submision score: 0.381
- Round 25:
    + Base model: Round 24
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* used Kfold = 5
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1180 + 878 + 814 + 1100 + 1148 
	* train time: 2103.3532185554504 + 2112.0196216106415 + 1463.224598646164 + 2277.286563396454 + 1997.4008457660675 = 9953.284847974777
	* RMSLE without-log: (0.378574601563 + 0.374263204893 + 0.377778683159 + 0.374609691487 + 0.373672468293 )/5 = 0.375779729879
    + Submision score: 0.381 => 0.381	
- Round 26:
    + Base model: Round 24
	* new data set: fastest_route
	# 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 10}
	* remove haversin_distance
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2682
	* train time: 2100.6572234630585
	* RMSLE without-log: 0.383296609142
    + Submision score:  
- Round 27:
    + Base model: Round 24
	* new data set: fastest_route
	* 'max_depth': 5, 'learning_rate': 0.1, 'min_child_weight': 5
	* added starting_street_tf_speed_mean, end_street_tf_speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1669
	* train time: 1449.917546749115
	* RMSLE without-log: 0.3808412094
    + Submision score: 
- Round 28:
    + Base model: Round 27
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1054
	* train time: 2144.4699709415436
	* RMSLE without-log: 0.373046115317 => 0.372649050678
    + Submision score: 	
- Round 29:
    + Base model: Round 28
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* added  starting_street_tf_duration_mean, end_street_tf_duration_mean
	* cleanup data (trip_duration> 1800000) before pre-process
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1121
	* train time: 3381.792198896408
	* RMSLE without-log: 0.372649050678 => 0.373172177982
    + Submision score: 0.382		
- Round 30:
    + Base model: Round 29
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* remove duration_mean	
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 636
	* train time: 1033.3630764484406
	* RMSLE without-log: 0.373172177982 => 0.373016098776
    + Submision score: 
- Round 31:
    + Base model: Round 30
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* added number_of_steps_speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 611
	* train time: 1452.7844805717468
	* RMSLE without-log: 0.373172177982 => 0.372985129101
    + Submision score: 0.382
- Round 32:
    + Base model: Round 31
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* added distance_per_step
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 763
	* train time: 1941.8781354427338
	* RMSLE without-log: 0.372985129101 => 0.373206600463
    + Submision score: 0.382 =>  0.382	

8: 2017-08-01:
- Round 33:
    + Base model: Round 32
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* removed distance_per_step
	* added manhattan_distance
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 851
	* train time: 2343.4766597747803
	* RMSLE without-log: 0.373206600463 => 0.373096205348
    + Submision score: 0.381
- Round 34:
    + Base model: Round 33
	* new data set: fastest_route + weather
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 882
	* train time: 2633.0243830680847
	* RMSLE without-log: 0.373096205348 => 0.372926464895
    + Submision score: 0.381
- Round 35:
    + Base model: Round 34
	* new data set: fastest_route 
	* removed data set: weather
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* added new features: total_turns (from step_maneuvers)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1101
	* train time: 2497.3055131435394
	* RMSLE without-log: 0.373137069604
    + Submision score: 0.381
- Round 36:
    + Base model: Round 35
	* new data set: fastest_route 
	* removed data set: weather
	* 'learning_rate': 0.1, 'max_depth': 10, 'colsample_bytree': 0.9, 'min_child_weight': 1
	* removed features: total_turns (from step_maneuvers)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 891
	* train time: 2022.018221616745
	* RMSLE without-log: 0.372083580258
    + Submision score:	0.382
9: 2017-08-03:	
- Round 37:
    + Base model: Round 36
	* new data set: fastest_route 
		* 'learning_rate': 0.1, 'max_depth': 10, 'colsample_bytree': 0.9, 'min_child_weight': 1
	* added features: left_turns, right_turns (from step_direction)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 832
	* train time: 825.822839975357
	* RMSLE without-log: 0.372427951911 
    + Submision score:	0.382 => 0.381	
- Round 38:
    + Base model: Round 37
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'colsample_bytree': 0.9, 'min_child_weight': 1
	* cleanup data: (data[label] < 22 * 3600) & (data[label] > 10)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 990
	* train time: 964.5637571811676
	* RMSLE without-log: 0.372427951911 => 0.315914025687
    + Submision score:	0.381 => 0.380		
- Round 39:
    + Base model: Round 38
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1166
	* train time: 1204.0661432743073
	* RMSLE without-log: 0.315914025687 => 0.315582605322
    + Submision score:	0.380 => 0.380			
- Round 40:
    + Base model: Round 39
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* added new features: total_turns (from step_maneuvers)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1175
	* train time: 1242.2003087997437 
	* RMSLE without-log: 0.315582605322 => 0.316186051761
    + Submision score:	0.380 => 0.380				
- Round 41:
    + Base model: Round 40
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* removed features: total_turns (from step_maneuvers)
	* added *_hv_speed_mean (based on haversine_distance)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1400
	* train time: 1599.0449421405792
	* RMSLE without-log: 0.316186051761 => 0.315559158281
    + Submision score:	0.380 => 0.380 					
- Round 42:
    + Base model: Round 41
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* removed *_*_speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1199
	* train time: 1146.5380535125732
	* RMSLE without-log: 0.315559158281 => 0.316582022318
    + Submision score:	0.380 => 0.381						
- Round 43:
    + Base model: Round 42
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* added trip_delay_mean by columns (pickup_*, starting_street, end_street)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1031
	* train time: 1515.8221554756165
	* RMSLE without-log: 0.316582022318 => 0.316532398454
    + Submision score:	0.380
- Round 44:
    + Base model: Round 44
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* added different between total_distance and haversine_distance
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1666
	* train time: 1875.7382152080536
	* RMSLE without-log: 0.316532398454 => 0.315660428816
    + Submision score:	0.380

10: 2017-08-04:
- Round 45:
    + Base model: Round 44
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* removed hv_distance_diff, trip_delay_mean, speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1199
	* train time: 1082.713295698166
	* RMSLE without-log: 0.315660428816 => 0.316582022318
    + Submision score:	0.380 => 0.381
- Round 46:
    + Base model: Round 45
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* added trip_delay_mean, speed_mean, duration_mean, distance_per_step
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 919
	* train time: 2218.202323913574
	* RMSLE without-log: 0.316582022318 => 0.317227755451
    + Submision score:	0.381 => 0.382 	
11: 2017-08-06:
- Round 47:
    + Base model: Round 46
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* removed trip_delay_mean, speed_mean, duration_mean, distance_per_step
	* added pickup_whour
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1465
	* train time: 3059.0503962039948 
	* RMSLE without-log: 0.317227755451 => 0.316086616132
    + Submision score:	0.382 => 0.380
13: 2017-08-07:		
- Round 48:
    + Base model: VWRegressor
	* 'learning_rate': 0.5, passes = 10
	* target_log = np.log(target)
	* train time: 
	* RMSLE without-log: 0.52
    + Submision score:	0.380 => 0.653
14: 2017-08-07:			
- Round 49:
    + Base model: XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* correct right_turns feature
	* target_log = np.log(target)
	* best model train round: 1583
	* train time:  1914.6729001998901
	* RMSLE without-log: 0.315640291626
    + Submision score: 0.380
15: 2017-08-08:				
- Round 50:
    + Base model: XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 
	* added starting_street_cluster, end_street_cluster
	* target_log = np.log(target)
	* best model train round: 1180
	* train time: 1289.420259475708
	* RMSLE without-log: 0.315640291626 => 0.315394751588
    + Submision score: 0.380 => 0.380	
- Round 51:
    + Base model: XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 
	* added location_cluster ('starting_street_tf', 'end_street_tf'), N_CLUSTERS = 1000
	* target_log = np.log(target)
	* best model train round: 1710
	* train time: 1937.9622585773468
	* RMSLE without-log: 0.315640291626 => 0.314919531323
    + Submision score: 0.380 => 0.380		
- Round 52:
    + Base model: XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 
	* added location_cluster ('starting_street_tf', 'end_street_tf'), N_CLUSTERS = 200
	* added pickup_cluster ('pickup_latitude', 'pickup_longitude'), dropoff_cluster ('dropoff_latitude', 'dropoff_longitude')
	* target_log = np.log(target)
	* best model train round: 1495
	* train time: 1577.4168784618378
	* RMSLE without-log: 0.314919531323 => 0.31608909162
    + Submision score: 0.380 => 0.380			
#--------------------------------CatBoostRegressor-----------------------------------------------------	
- Round 53:
    + Base model: CatBoostRegressor
	* iterations=10000, learning_rate=0.1, depth=10 
	* removed Kmeans cluster 
	* target_log = np.log(target)
	* best model train round: 10000
	* train time: 2500
	* RMSLE without-log: 0.343456689581
    + Submision score: 0.380 => 0.406				
16: 2017-08-09:
- Round 54:
    + Base model: CatBoostRegressor
	* iterations=20000, learning_rate=0.1, depth=10 
	* target_log = np.log(target)
	* best model train round: 19990
	* train time: 5350.181067705154
	* RMSLE without-log: 0.328954457603
    + Submision score: 0.406 => 0.393
- Round 55:
    + Base model: CatBoostRegressor
	* iterations=50000, learning_rate=0.03, depth=10
	* target_log = np.log(target)
	* best model train round: 49887
	* train time: 14021.58697462082
	* RMSLE without-log: 0.349743035723
    + Submision score: 0.393 => 0.412
17: 2017-08-10:	
- Round 56:
    + Base model: CatBoostRegressor
	* iterations=30000, learning_rate=0.1, depth=10 
	* cat_features = ['vendor_id', 'store_and_fwd_flag', 'pickup_month',
                        'pickup_weekday', 'pickup_day', 'pickup_hour',
                        'pickup_whour', 'pickup_minute'
                        ]
	* target_log = np.log(target)
	* best model train round: 
	* train time: 
	* RMSLE without-log: 
    + Submision score: 
- Round 97:
    + Base model: CatBoostRegressor
	* iterations=30000, learning_rate=0.1, depth=10 
	* added cluster for location + direction
	* target_log = np.log(target)
	* best model train round: 44513
	* train time: 19388.68798017502
	* RMSLE without-log: 0.315358555895
    + Submision score: 	0.38284
#-------------------------------------------------------------------------------------	
- Round 57:
    + Base model: XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 
	* added location_cluster ('starting_street_tf', 'end_street_tf'), N_CLUSTERS = 200
	* added pickup_cluster ('pickup_latitude', 'pickup_longitude'), dropoff_cluster ('dropoff_latitude', 'dropoff_longitude')
	* target_log = np.log(target)
	* best model train round: 1387 
	* train time: 3020.7012190818787
	* RMSLE without-log: 0.315679178625
    + Submision score: 0.380 => 0.380
- Round 58:
    + Base model: LGBMRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 
	* added location_cluster ('starting_street_tf', 'end_street_tf'), N_CLUSTERS = 200
	* added pickup_cluster ('pickup_latitude', 'pickup_longitude'), dropoff_cluster ('dropoff_latitude', 'dropoff_longitude')
	* target_log = np.log(target)
	* best model train round: 4940
	* train time: 198.60693311691284
	* RMSLE without-log: 0.324367347584
    + Submision score: 0.388
- Round 59:
    + Base model: Round 58
	* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 5
	* target_log = np.log(target)
	* best model train round: 19252
	* train time: 752.7515609264374
	* RMSLE without-log: 0.324367347584 => 0.322236424969
    + Submision score: 0.388 => 0.386	
18: 2017-08-11:
- Round 60:
    + Base model: Round 59 
	* LGBMRegressor
	* 'learning_rate': 0.1, num_leaves=512
	* target_log = np.log(target)
	* best model train round:1451 
	* train time: 140.3382053375244
	* RMSLE without-log: 0.322236424969 => 0.321227
    + Submision score: 0.386 => 			
- Round 61:
    + Base model: Round 60 
	* LGBMRegressor
	* 'learning_rate': 0.1, num_leaves=1024
	* target_log = np.log(target)
	* best model train round: 818
	* train time: 140.1048858165741
	* RMSLE without-log: 0.321227 => 0.32146863452
    + Submision score: 0.386 => 0.387				
- Round 62:
    + Base model: Round 60 
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=512
	* target_log = np.log(target)
	* best model train round: 4629
	* train time: 446.7575452327728
	* RMSLE without-log: 0.32146863452 => 0.318067211916
    + Submision score: 0.387 => 0.384			
- Round 63:
    + Base model: Round 60 
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1024 => Best
	* target_log = np.log(target)
	* best model train round: 3553
	* train time: 562.9755690097809
	* RMSLE without-log: 0.318067211916 => 0.31787662896
    + Submision score: 0.384 => 0.383				
- Round 64:
    + Base model: Round 60 
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=2048
	* target_log = np.log(target)
	* best model train round: 1605
	* train time: 554.6046388149261
	* RMSLE without-log: 0.31787662896 => 0.318766680087
    + Submision score: 0.383 => 
- Round 65:
    + Base model: Round 60 
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1536
	* target_log = np.log(target)
	* best model train round: 2900
	* train time:650.9595715999603
	* RMSLE without-log: 0.31787662896 => 0.318134380655
    + Submision score: 0.383 => 			
- Round 66:
    + Base model: Round 60 
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1024
	*  cat_features = ['vendor_id', 'store_and_fwd_flag', 'pickup_month',
                        'pickup_weekday', 'pickup_day', 'pickup_hour',
                        'pickup_whour', 'pickup_minute'
                        ]
	* target_log = np.log(target)
	* best model train round: 1646
	* train time: 575.8021259307861
	* RMSLE without-log: 0.31787662896 => 0.335223829409
    + Submision score: 0.383 => 0.398				
- Round 67:
    + Base model: Round 66 
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=2048
	* target_log = np.log(target)
	* best model train round: 966
	* train time: 309.1839175224304
	* RMSLE without-log: 0.335223829409 => 0.339737112562
    + Submision score: 0.383 => 
- Round 68:
    + Base model: Round 66 
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1024, max_bin=512
	* target_log = np.log(target)
	* best model train round: 1408
	* train time: 317.5527174472809
	* RMSLE without-log: 0.31787662896 => 0.337128140915
    + Submision score: 0.383 => 	
- Round 69:
    + Base model: Round 66 
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1024, max_bin=1024
	* cat_features
	* target_log = np.log(target)
	* best model train round: 2025
	* train time: 433.9591565132141
	* RMSLE without-log: (0.31787662896) 0.337128140915 => 0.336225641011
    + Submision score: 0.383 => 		
- Round 70:
    + Base model: Round 66
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1024, max_bin=2048
	* cat_features
	* target_log = np.log(target)
	* best model train round: 1653
	* train time: 440.6618022918701
	* RMSLE without-log: (0.31787662896) 0.336225641011 => 0.335734921887
    + Submision score: 0.383 => 			
- Round 71:
    + Base model: Round 66
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1024, max_bin=8192
	* cat_features
	* target_log = np.log(target)
	* best model train round: 1826
	* train time: 896.5467207431793
	* RMSLE without-log: (0.31787662896) 0.335734921887 => 0.335780889774
    + Submision score: 0.383 => 				
- Round 72:
    + Base model: Round 66
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1024, max_bin=4096
	* cat_features
	* target_log = np.log(target)
	* best model train round: 1659
	* train time: 579.672604560852
	* RMSLE without-log: (0.31787662896) 0.335734921887 => 0.335573925783
    + Submision score: 0.383 => 					
- Round 73:
    + Base model: Round 66
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1024, max_bin=2048
	* no cat_features
	* target_log = np.log(target)
	* best model train round: 3673
	* train time: 795.7316427230835
	* RMSLE without-log: (0.31787662896) => 0.319262749763
    + Submision score: 0.383 => 						
- Round 74:
    + Base model: Round 73
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=2048, max_bin=2048
	* no cat_features
	* target_log = np.log(target)
	* best model train round: 2039
	* train time: 873.2032713890076
	* RMSLE without-log: (0.31787662896) =>0.319737233272
    + Submision score: 0.383 => 0.398							
- Round 75:
    + Base model: Round 74
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=4096, max_bin=1024
	* no cat_features
	* target_log = np.log(target)
	* best model train round: 1789
	* train time: 779.1963686943054
	* RMSLE without-log: (0.31787662896) => 0.320930091473
    + Submision score: 0.383 => 0.385
- Round 76:
    + Base model: Round 75
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=4096, max_bin=1024, min_data_in_leaf=100,
	* no cat_features
	* target_log = np.log(target)
	* best model train round: 651
	* train time: 430.4107961654663
	* RMSLE without-log: 0.320930091473 => 0.320515793758
    + Submision score: 0.383 => 0.384	
- Round 77:
    + Base model: Round 76
	* LGBMRegressor
	* 'learning_rate': 0.01, num_leaves=4096, max_bin=1024, min_data_in_leaf=100,
	* no cat_features
	* target_log = np.log(target)
	* best model train round: 1814
	* train time: 1210.5160865783691
	* RMSLE without-log:0.320515793758 => 0.320260748897
    + Submision score: 0.383 => 0.383
- Round 78:
    + Base model: Round 77
	* LGBMRegressor
	* 'learning_rate': 0.01, num_leaves=16384, max_bin=1024, min_data_in_leaf=100,
	* no cat_features
	* target_log = np.log(target)
	* best model train round: 1381
	* train time: 1887.764794588089
	* RMSLE without-log:0.320260748897 => 0.322163547467
    + Submision score: 0.383 => 0.385
- Round 79:
    + Base model: Round 78
	* LGBMRegressor
	* 'learning_rate': 0.01, num_leaves=8192, max_bin=1024, min_data_in_leaf=100,
	* no cat_features
	* target_log = np.log(target)
	* best model train round:1394 
	* train time: 1689.6206121444702
	* RMSLE without-log:0.320260748897 => 0.321646406478
    + Submision score: 0.383 => 0.384
19: 2017-08-11: XGBRegressor
- Round 80:
    + Base model: Round 57
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* added n_tree_limit in xgboost.predict	 
	* target_log = np.log(target)
	* best model train round: 1387 => 1492
	* train time: 2079.768365383148
	* RMSLE without-log: 0.315679178625 => 0.315402829663
    + Submision score: 0.380 => 0.38045
- Round 81:
    + Base model: Round 80
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5, 'gamma': 1
	* added n_tree_limit in xgboost.predict	 
	* target_log = np.log(target)
	* best model train round: 324
	* train time: 507.14076113700867
	* RMSLE without-log: 0.315402829663 => 0.321074187444
    + Submision score: 0.380 => 0.38488	
- Round 82:
    + Base model: Round 81
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5, 'gamma': 0.2
	* added n_tree_limit in xgboost.predict	 
	* target_log = np.log(target)
	* best model train round: 702
	* train time: 851.6674802303314
	* RMSLE without-log: 0.315402829663 => 0.317003309189
    + Submision score: 0.38045 => 0.38090
- Round 83:
    + Base model: Round 82
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 6, 'min_child_weight': 1, 'gamma': 0
	* added n_tree_limit in xgboost.predict	 
	* target_log = np.log(target)
	* best model train round: 2652
	* train time: 2110.9427256584167
	* RMSLE without-log: 0.317003309189 => 0.320264523865
    + Submision score: 0.38045 => 0.38090
- Round 84:
    + Base model: Round 82
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, 'gamma': 0
	* added n_tree_limit in xgboost.predict	 
	* target_log = np.log(target)
	* best model train round: 1513
	* train time: 2125.335698366165
	* RMSLE without-log: 0.320264523865 => 0.315268059834
    + Submision score: 0.38090 => 0.38017 => Best	
- Round 85:
    + Base model: Round 84
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 15, 'min_child_weight': 1, 'gamma': 0
	* added n_tree_limit in xgboost.predict	 
	* target_log = np.log(target)
	* best model train round: 510
	* train time: 1139.539 seconds
	* RMSLE without-log: 0.315268059834 => 0.316688266073
    + Submision score: 0.38017 => 0.38124 => Overfit		
- Round 86:
    + Base model: Round 85
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 20, 'min_child_weight': 5, 'gamma': 0
	* added n_tree_limit in xgboost.predict	 
	* target_log = np.log(target)
	* best model train round: 496
	* train time: 1277.2605113983154
	* RMSLE without-log: 0.315268059834 => 0.32044403701
    + Submision score: 0.38017 => 0.38381
- Round 86:
    + Base model: Round 85
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 2, 'gamma': 0
	* added n_tree_limit in xgboost.predict	 
	* target_log = np.log(target)
	* best model train round: 1312.
	* train time: 1429.1299285888672
	* RMSLE without-log: 0.315268059834 => 0.315325770346
    + Submision score: 0.38017 => 0.38053
- Round 99:
    + Base model: Round 98
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
	* added n_tree_limit in xgboost.predict	 
	* added feature cluster_count for (dropoff_cluster by pickup_hour + pickup_whour)
	* target_log = np.log(target)
	* best model train round: 1316
	* train time: 4442.041965007782
	* RMSLE without-log: 0.315268059834 => 0.312084220093
    + Submision score: 0.38017 => 0.37795
- Round 103:
    + Base model: Round 102
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
	* added n_tree_limit in xgboost.predict	 
	* added feature cluster_count for (dropoff_cluster by pickup_hour + pickup_whour)
	* added cluster + cluster_count for pickup, dropoff by weekday
	* target_log = np.log(target)
	* best model train round: 1131
	* train time: 4342.305188655853
	* RMSLE without-log: 0.312084220093 => 0.312379072638
    + Submision score: 0.37795	=> 0.37855 

20: 2017-08-15: LGBMRegressor
- Round 87:
    + Base model: Round 77
	* LGBMRegressor
	* 'learning_rate': 0.01, num_leaves=4096, max_bin=1024, min_data_in_leaf=100,
	* no cat_features
	* target_log = np.log(target)
	* best model train round: 1814
	* train time: 1510.8735432624817
	* RMSLE without-log:0.320260748897 => 0.317506379624
    + Submision score: 0.383 => 0.38132

#------------------------- Stack model-------------------------------------	
21: 2017-08-16: Stack models
- Round 88: Stack model
    + Base model: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.313979901429
			. LB: 0.38017
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=4096, max_bin=1024, min_data_in_leaf=100
			. RMSLE without-log: 0.316250790001
			. LB: 0.38132
		* train time: 19649.848478794098
		* target_log = np.log(target)
		* RMSLE without-log: 0.315115345715	
	+ Stack model: LGBMRegressor
		* 'learning_rate': 0.03, num_leaves=1024
		* best model train round: 146
		* train time: 15.97771143913269
		* RMSLE without-log: 0.315313
    + Submision score: 0.37789 
- Round 89: Stack model
    + Base model: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.313979901429
			. LB: 0.38017
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=4096, max_bin=1024, min_data_in_leaf=100
			. RMSLE without-log: 0.316250790001
			. LB: 0.38132
		* train time: 19649.848478794098
		* target_log = np.log(target)
		* RMSLE without-log: 0.315115345715	
	+ Stack model: LGBMRegressor
		* 'learning_rate': 0.01, num_leaves=1024  => Best 
		* best model train round: 454
		* train time: 35.55270266532898
		* RMSLE without-log: 0.315306
    + Submision score: 0.37789 => 0.37786 => Best	
- Round 90: Stack model
    + Base model: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024, random_state=1024
			. RMSLE without-log: 0.316645074098
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024, random_state=123
			. RMSLE without-log: 0.316771299548
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024, random_state=789
			. RMSLE without-log: 0.316723905823		
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, random_state=1000
			. RMSLE without-log: 0.313979901429	
		* train time: 27751.25788140297
		* target_log = np.log(target)
		* RMSLE without-log: 0.316030045225 
	+ Stack model: LGBMRegressor
		* 'learning_rate': 0.01, num_leaves=1024  => Best 
		* best model train round: 416
		* train time: 31.391589164733887
		* RMSLE without-log: 
    + Submision score: 0.37786 => 0.37848
- Round 91: Stack model
    + Base model: 
		* XGBRegressor:  
			. 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=456
			. RMSLE without-log: 0.311739403458 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, random_state=1000
			. RMSLE without-log: 0.313979901429 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024, random_state=1024
			. RMSLE without-log: 0.316645057696
		* train time: 38799.84138727188
		* target_log = np.log(target)
		* RMSLE without-log: 0.314121454194 
	+ Stack model: LGBMRegressor
		* 'learning_rate': 0.01, num_leaves=1024  => Best 
		* best model train round: 399
		* train time: 29.3829271793365
		* RMSLE without-log: 0.315103
    + Submision score: 0.37786 => 0.37798
- Round 92: Stack model
    + Base model: 
		* XGBRegressor:  
			. 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=456
			. RMSLE without-log: 0.31168934339
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024, random_state=1024
			. RMSLE without-log: 0.316246238577
		* train time: 31932.34571838379
		* target_log = np.log(target)
		* RMSLE without-log: 0.314192231109
	+ Stack model: LGBMRegressor
		* 'learning_rate': 0.01, num_leaves=1024  => Best 
		* best model train round: 432
		* train time: 34
		* RMSLE without-log: 0.314625 
    + Submision score: 0.37786 => 0.37807
- Round 93: Stack model
    + Base model: 
		* XGBRegressor:  
			. 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=456
			. RMSLE without-log: 0.31168934339
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024, random_state=1024
			. RMSLE without-log: 0.316246238577
		* train time: 31932.34571838379
		* target_log = np.log(target)
		* RMSLE without-log: 0.314192231109
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* best model train round: 64
		* train time: 35.700355768203735
		* RMSLE without-log: 0.313657
    + Submision score: 0.37786 => 0.37646 
- Round 94: Stack model
    + Base model: 
		* XGBRegressor:  
			. 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=456
			. RMSLE without-log: 0.31168934339
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024, random_state=1024
			. RMSLE without-log: 0.316246238577
		* train time: 31932.34571838379
		* target_log = np.log(target)
		* RMSLE without-log: 0.314192231109
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* best model train round: 227
		* train time: 106.15630888938904 
		* RMSLE without-log: 0.31359 
    + Submision score: 0.37646 => 0.37637 
- Round 95: Stack model
    + Base model: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.313979901429
			. LB: 0.38017
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.316250790001 => 0. 316645070975
			. LB: 0.38132
		* train time: 16250.022941589355
		* target_log = np.log(target)
		* RMSLE without-log: 0.315115345715	 => 0.315312486202
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* best model train round: 237 
		* train time: 
		* RMSLE without-log: 0.31359 => 0.314354
    + Submision score: 0.37637 =>  0.37627
2017-08-19:	
- Round 96: Stack model
    + Base model: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.313979901429 => 0.3112062235560
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0. 316645070975 => 0.311270633371
			. LB: 
		* train time: 15186.800484418869
		* target_log = np.log(target)
		* added cluster for location + direction
		* RMSLE without-log: 0.315312486202 => 0.311238428463
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* best model train round: 232
		* train time: 
		* RMSLE without-log: 0.314354 => 0.310241
    + Submision score: 0.37627 => 0.37399
2017-08-20:		
- Round 98: Stack model
    + Base model: 
		* CatBoostRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10,
			. RMSLE without-log: 0.329556229071
			. LB: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.3112062235560 => 0.311206223556
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.311270633371 => 0.311270630771
			. LB: 
		* train time: 38184.949978113174
		* target_log = np.log(target)
		* added cluster for location + direction
		* RMSLE without-log: 0.311238428463 => 0.317344361133
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* best model train round: 260
		* train time: 134.77944326400757
		* RMSLE without-log: 0.310241 => 0.309508
    + Submision score: 0.37399 => 0.37370 => Best
- Round 100: Stack model
    + Base model: 
		* CatBoostRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, iterations=25000 => overfit
			. RMSLE without-log: 0.329556229071 => 0.321657851058
			. train time: 38062.957418203354
			. LB: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.311206223556 => 0.311292443602
			. train time: 8250.806457519531
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.311270630771 => 0.311248106626
			. train time: 7856.804503440857
			. LB: 
		* train time: 54170.570033073425
		* target_log = np.log(target)
		* added cluster_count
		* RMSLE without-log: 0.317344361133 => 0.314732800429
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* best model train round: 255
		* train time: 181
		* RMSLE without-log: 0.309508 => 0.310135	
    + Submision score: 0.37370 => 0.37391
2017-08-21:	
- Round 101: Stack model
    + Base model: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.311206223556 => 0.311292443602
			. train time: 8250.806457519531
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.311270630771 => 0.311248106626
			. train time: 7856.804503440857
			. LB: 
		* train time: 54170.570033073425
		* target_log = np.log(target)
		* added cluster_count
		* RMSLE without-log: 0.317344361133 => 0.314732800429
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* best model train round: 254
		* train time: 151.37224006652832
		* RMSLE without-log: 0.310135 => 0.310451	
    + Submision score: 0.37391 => 0.37415
- Round 102: Stack model
    + Base model: 
		* CatBoostRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, iterations=20000 
			. RMSLE without-log: 0.321657851058 => 0.327220952283
			. train time: 27385.687201976776
			. LB: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.311292443602 => 0.311292443602
			. train time: 8033.876915693283
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log:  0.311248106626 => 0.311248109447
			. train time: 7630.566361904144
			. LB: 
		* train time: 43050.1323697567
		* target_log = np.log(target)
		* added cluster_count
		* RMSLE without-log: 0.314732800429 => 0.316587168444
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* best model train round: 238
		* train time: 121
		* RMSLE without-log: 0.310135 => 0.30999	
    + Submision score: 0.37391 => 0.37394
- Round 104: Stack model
    + Base model: 
		* CatBoostRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, iterations=20000 
			. RMSLE without-log: 0.330450377426 
			. train time: 30951.256221055984
			. LB: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.31200444004
			. train time: 8543.655898332596
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.311476885066 
			. train time: 10866.278344392776
			. LB: 
		* target_log = np.log(target)
		* added cluster_count
		* added cluster + cluster_count for pickup, dropoff by weekday
		* train time: 50361.19199371338
		* RMSLE without-log: 0.317977234177
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* best model train round: 241
		* train time: 167
		* RMSLE without-log: 0.310315
    + Submision score: 0.37432
2017-08-23:	
- Round 105: Stack model
    + Base model: 
		* CatBoostRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, iterations=20000 
			. RMSLE without-log: 0.330450377426 => 0.331059048354
			. train time: 30951.256221055984 => 27202.265921354294
			. LB: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.31200444004 => 0.311905564816
			. train time: 8543.655898332596 => 6761.2394824028015
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.311476885066 => 0.311135328956
			. train time: 10866.278344392776 => 8372.475480556488
			. LB: 
		* target_log = np.log(target)
		* removed cluster_count
		* train time: 50361.19199371338 => 42335.98299074173
		* RMSLE without-log: 0.317977234177 => 0.318033314042
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* best model train round: 270
		* train time: 189.0228180885315
		* RMSLE without-log: 0.310315 => 0.310207
    + Submision score: 0.37432 => 0.37399	
- Round 106: Stack model
    + Base model: 
		* CatBoostRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, iterations=20000 
			. RMSLE without-log: 0.331059048354 => 0.327081734747
			. train time: 27202.265921354294 => 35100.287341833115
			. LB: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.311905564816 => 0.311796340695
			. train time: 6761.2394824028015 => 9192.058917284012
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.311135328956 => 0.310965002007
			. train time: 8372.475480556488 => 10166.472098588943
			. LB: 
		* target_log = np.log(target)
		* added cluster_count + total_turns
		* train time: 42335.98299074173 => 54458.82000350952
		* RMSLE without-log: 0.318033314042 => 0.31661435915
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* best model train round: 270 => 249 => (20000  all train_set) 
		* train time: 129.83565425872803 => (12184.7882008552 all train_set )
		* RMSLE without-log: 0.310207 => 0.309997 => (0.240825 all train_set) , (0.296618, N_ROUNDS=500)
    + Submision score: 0.37399 => 0.37392 => (0.37962 all train_set) , (0.37431,N_ROUNDS=500)		
- Round 107: Stack model
    + Base model: 
		* CatBoostRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, iterations=20000 
			. RMSLE without-log: 0.327081734747
			. train time: 35100.287341833115
			. LB: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.311796340695
			. train time: 9192.058917284012
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.310965002007
			. train time: 10166.472098588943
			. LB: 
		* target_log = np.log(target)
		* added cluster_count + total_turns
		* train time: 54458.82000350952
		* RMSLE without-log: 0.31661435915
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* Kfold: 5
		* train time: 793.9579651355743
		* RMSLE without-log: 0.310207 => 0.308586200073
    + Submision score: 0.37399 => 0.37378
- Round 108: Stack model (base on round 98)
    + Base model: 
		* CatBoostRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10,
			. RMSLE without-log: 0.329556229071
			. LB: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.311206223556
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.311270630771
			. LB: 
		* train time: 38184.949978113174
		* target_log = np.log(target)
		* added cluster for location + direction
		* RMSLE without-log: 0.317344361133
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* Kfold: 5
		* train time: 790.0810258388519
		* RMSLE without-log: 0.309508 => 0.308353120899
    + Submision score: 0.37370 => 0.37330 => Best	
2017-08-24:	
- Round 109: Stack model
    + Base model: 
		* CatBoostRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10,
			. RMSLE without-log: 0.329556229071 => 0.332846582116
			. time: 30358.694513320923
			. LB: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.311206223556 => 0.311576716352
			. time: 7864.157855510712
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.311270630771 => 0.31088085136
			* time: 9081.705178976059
			. LB: 
		* target_log = np.log(target)
		* remove cluster_count
		* added total_turns, direction
		* train time: 47304.55904865265
		* RMSLE without-log: 0.317344361133 => 0.318434716609
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* Kfold: 5
		* train time: 539.1456367969513
		* RMSLE without-log: 0.308353120899 => 0.308266749604
    + Submision score: 0.37330 => 0.37304 
- Round 110: Stack model
    + Base model:based round 109
		* CatBoostRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10,
			. RMSLE without-log: 0.332846582116 => 0.329483430627
			. time: 30358.694513320923 => 32345.057856321335
			. LB: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.311576716352 => 0.31083483999
			. time: 7864.157855510712 => 9749.451682329178
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.31088085136 => 0.30998224005
			* time: 9081.705178976059 => 11500.288156747818
			. LB: 
		* target_log = np.log(target)
		* added weather_data: temperature
		* train time: 47304.55904865265 => 53594.79922747612
		* RMSLE without-log: 0.318434716609 => 0.316766836889
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* Kfold: 5
		* train time: 892.1451954841614
		* RMSLE without-log: 0.308266749604 => 0.30758121862
    + Submision score: 0.37304 => 0.37273 => Best
2017-08-26:
- Round 111: Stack model
    + Base model:based round 110
		* CatBoostRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10,
			. RMSLE without-log: 0.329483430627 => 0.326463879008
			. time: 32345.057856321335 => 35648.67615222931
			. LB: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.31083483999 => 0.311240324021
			. time: 9749.451682329178 => 8374.936891555786
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.30998224005 => 0.310339158202
			* time: 11500.288156747818 => 10659.038615942001
			. LB: 
		* target_log = np.log(target)
		* added weather_data from https://www.kaggle.com/cabaki/knycmetars2016
		* train time: 53594.79922747612 => 54682.6602230072
		* RMSLE without-log: 0.316766836889 => 0.316014453744
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* Kfold: 5
		* train time: 563.8362400531769
		* RMSLE without-log: 0.30758121862 => 0.308120459306
    + Submision score: 0.37273 => 0.37277
- Round 112: Stack model
    + Base model:based round 111
		* CatBoostRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10,
			. RMSLE without-log: 0.326463879008 => 0.330071245717
			. time: 35648.67615222931 => 32208.90213918686
			. LB: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.311240324021 => 0.310999208504
			. time: 8374.936891555786 => 9519.866947174072
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.310339158202 => 0.310733608631
			* time: 10659.038615942001 => 10014.470539569855
			. LB: 
		* target_log = np.log(target)
		* added weather_data from https://www.kaggle.com/cabaki/knycmetars2016: col_use= Temp., snow
		* train time: 54682.6602230072 => 51743.27180624008
		* RMSLE without-log: 0.316014453744 => 0.317268020951
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* Kfold: 5
		* train time: 544.7495741844177
		* RMSLE without-log: 0.308120459306 => 0.307957028127
    + Submision score: 0.37277 => 0.37305
2017-08-28:
- Round 113: Stack model + kfold
    + Base model:based round 112
		* CatBoostRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10,
			. RMSLE without-log: 0.326463879008 => 0.326045268915
			. time: 35648.67615222931 => 38644.04991841316
			. LB: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.311240324021 => 0.310714971694
			. time: 8374.936891555786 => 9818.439695596695
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.310339158202 => 0.310226857841
			* time: 10659.038615942001 => 11729.922056436539
			. LB: 
		* target_log = np.log(target)
		* added weather_data from https://www.kaggle.com/cabaki/knycmetars2016: 
		     . col_use=  'Temp.', 'Precip', 'Wind Speed',
                         'Dew Point', 'Visibility',
                         'snow', 'rain', 'fog', 'heavy_snow', 'heavy_rain'
		* train time: 54682.6602230072 => 60192.413803100586
		* RMSLE without-log: 0.316014453744 => 0.31566236615
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* Kfold: 5
		* train time: 
		* RMSLE without-log: 0.308120459306 => 
    + Submision score: 0.37277 => 0.37257 => Best
2017-08-29:
- Round 114: Stack model + kfold
    + Base model:based round 113
		* CatBoostRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10,
			. RMSLE without-log: 0.326045268915 => 0.327237582388
			. time: 38644.04991841316 => 40676.020208358765
			. LB: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.310714971694 => 0.311017144962
			. time: 9818.439695596695 => 12038.630622148514
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.310226857841 => 0.307364091206
			* time: 11729.922056436539 => 13481.448738336563
			. LB: 
		* target_log = np.log(target)
		* added feature: PCA for location
		* train time: 60192.413803100586 => 66196.11234974861
		* RMSLE without-log: 0.31566236615 => 0.315206272852
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* Kfold: 5
		* train time: 590.0392031669617
		* RMSLE without-log: 0.308120459306 => 0.306161598024
	+ Total time: 66849.52451658249	
    + Submision score: 0.37257 => 0.37157 => Best
2017-08-30:
- Round 115: Stack model + kfold
    + Base model:based round 114
		* CatBoostRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10,
			. RMSLE without-log: 0.327237582388 => 0.387628629723
			. time: 40676.020208358765 => 40250.2110350132
			. LB: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.311017144962 => 0.373226444542
			. time: 12038.630622148514 => 9782.530477762222
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.307364091206 => 0.371007161689
			* time: 13481.448738336563 => 9020.854282855988
			. LB: 
		* target_log = np.log(target)
		* added feature: PCA for location
		* removed: cleanup invalid trip_duration
		* train time: 66196.11234974861 => 59053.59773540497
		* RMSLE without-log: 0.315206272852 => 0.377287411985
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* Kfold: 5
		* train time: 716.1195282936096
		* RMSLE without-log: 0.306161598024 => 0.368994450248
	+ Total time: 66849.52451658249	=> 59823.65178966522
    + Submision score: 0.37157 => 0.37050 => Best
2017-08-31:
- Round 116: Stack model + kfold
    + Base model:based round 115
		* CatBoostRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10,
			. RMSLE without-log: 0.387628629723 => 0.333471172946
			. time: 40250.2110350132 => 61397.0281829834
			. LB: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.373226444542 => 0.325821441544
			. time: 9782.530477762222 => 13375.370455026627
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.371007161689 => 0.323636529191
			* time: 9020.854282855988 => 13432.284525871277
			. LB: 
		* target_log = np.log(target)
		* added feature: PCA for location
		* cleanup trip_duration> 22*3600
		* train time: 59053.59773540497 => 88204.68500208855
		* RMSLE without-log: 0.377287411985 => 0.327643047894
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* Kfold: 5
		* train time: 863.30850481987
		* RMSLE without-log: 0.368994450248 => 0.322028878075
	+ Total time: 59823.65178966522 => 89125.10751652718
    + Submision score: 0.37050 => 0.36829 => Best
		