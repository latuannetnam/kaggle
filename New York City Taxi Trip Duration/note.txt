
0.: Requirements
 - use pipreqs ./ to regenerate requirements file
 - pip3 install -r requirments.txt
 - Ubuntu:
	+ sudo apt-get install libspatialindex-dev
	+ sudo apt-get install libgeos-dev
 - CentOS:
	+ yum install -y spatialindex-devel
	+ yum install -y geos-devel
	+ yum -y install python34-tkinter
0.1:
 - Best model: round 3: 0.417
 - Best model: round 8: 0.401 (added distance using haversine)
 - Best model: round 11: 0.390 (added distance using OSM + haversine distance, speed_mean, total_distance_mean)
 - Best model: round 21: 0.387 (removed feature: speed_mean and duration_mean by haversine)
 - Best model: round 22: 0.386 (KFold=5)
 - Best model: round 24: 0.381 (optimized model hyper-params: 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10)
 - Best model: round 28: 0.380 (cleanup data: (data[label] < 22 * 3600) & (data[label] > 10) + 'learning_rate': 0.1, 'max_depth': 10, 'colsample_bytree': 0.9, 'min_child_weight': 1)
 - Best model: round 95: 0.37627 (Stack model: XGBRegressor (0.1) + LGBMRegressor (0.01) => XGBRegressor (0.03) )
 - Best model: round 96: 0.37399 (Stack model: XGBRegressor (0.1) + LGBMRegressor (0.01) => XGBRegressor (0.03)) + new feature: directon

 
1. 2017-07-21
- Round 1:
    + Base model: 
        * XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,
       max_depth=5, min_child_weight=1, missing=None, n_estimators=2000,
       n_jobs=-1, nthread=None, objective='reg:linear', random_state=0,
       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
       silent=True, subsample=1)
	* use xgboost.cv
	* train_size = 0.85
	* target_log = np.log(target)
	* train time: 924.1382086277008
	* RMSLE without-log: 0.423621130054
    + Submision score: 0.427 
- Round 2:
    + Base model: 
        * XGBRegressor(n_estimators=5000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove outliner: train_set[label] > 5000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 5000
	* train time: 2399.4174466133118
	* RMSLE without-log: 0.374140060512
    + Submision score: 0.419
- Round 3:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.3, min_child_weight=1, n_jobs=-1)
	* remove outliner: train_set[label] > 5000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 3921
	* train time: 1854.232224702835
	* RMSLE without-log: 0.374140060512 => 0.371386768324
    + Submision score: 0.419 =>  0.417
- Round 4:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* all train data (not remove outliners)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 5093
	* train time: 2379.7198255062103
	* RMSLE without-log: 0.374140060512 => 0.41737086459
    + Submision score: 0.421
- Round 5:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.3, min_child_weight=1, n_jobs=-1)
	* feature scaling
	* remove pickup_year (correl = 0)
	* remove outliner: train_set[label] > 5000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1403
	* train time: 668.2613804340363
	* RMSLE without-log: 0.37665957093
    + Submision score: 0.417 => 0.422
2. 2017--7-23:
- Round 6:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* feature scaling
	* remove pickup_year (correl = 0)
	* remove outliner: train_set[label] > 5000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2413
	* train time: 1149.2097866535187
	* RMSLE without-log: 0.37665957093 => 0.379491160168
    + Submision score: 0.422 => 0.424
- Round 7:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 3107
	* train time: 1545.7049226760864
	* RMSLE without-log: 0.419480465158 
    + Submision score: 0.424
- Round 8:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* Added distance between pickup and dropoff using haversine
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2983 
	* train time: 1751.8312721252441
	* RMSLE without-log: 0.419480465158 => 0.395957129069
    + Submision score: 0.424 => 0.401
3: 2017-07-26:
- Round 9:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* Added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 3124
	* train time: 1665.9864552021027
	* RMSLE without-log: 0.388475183465
    + Submision score: 0.393
- Round 10:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* Added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* Added distance between pickup and dropoff using haversine dataset
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1874
	* train time: 1238.130537033081
	* RMSLE without-log: 0.388475183465 => 0.38546321876
    + Submision score: 0.393 => 0.391
- Round 11:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* Added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* Added distance between pickup and dropoff using haversine dataset
	* added speed_mean, total_distance_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2853
	* train time: 1934.7547209262848
	* RMSLE without-log: 0.38546321876 => 0.383703421188
    + Submision score: 0.391 => 0.390
4: 2017-07-27:
- Round 12:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* remove outliners: train_set['total_distance_mean] = 0
	* Added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* Added distance between pickup and dropoff using haversine dataset
	* correct speed_mean formular
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2985
	* train time: 1968.406958580017
	* RMSLE without-log: 0.383703421188 => 0.384153
    + Submision score:  
5: 2017-07-28:	
- Round 13:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove outliners: train_set[label] > 1800000
	* added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* added distance between pickup and dropoff using haversine dataset
	* added number_of_streets, starting_street, end_street
	* predict total_distance (for total_distance=0)
	* predict starting_street, end_street (for NaN starting_street, end_street)
	* added pickup_hour_speed_mean, pickup_weekday_speed_mean, pickup_day_speed_mean, pickup_month_speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2567
	* train time: 1943.313801765442
	* RMSLE without-log: 0.384153 => 0.369416883687
    + Submision score:  0.384153 => 0.428		
6: 2017-07-29:
- Round 14:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove outliners: train_set[label] > 1800000
	* added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* added distance between pickup and dropoff using haversine dataset
	* added number_of_streets, starting_street, end_street
	* predict total_distance (for total_distance=0)
	* predict starting_street, end_street (for NaN starting_street, end_street)
	* added pickup_hour_speed_mean, pickup_weekday_speed_mean, pickup_day_speed_mean, pickup_month_speed_mean
	* added hour_duration_mean, weekday_duration_mean, day_duration_mean, month_duration_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 3001
	* train time: 3092.225347518921
	* RMSLE without-log: 0.370031041363
    + Submision score: 0.422 
- Round 15:
    + Base model: Round 14
	* Set trip_duration=0 if haversin_distance=0
    + Submision score: 0.422 => 0.504
- Round 16:
    + Base model: Round 14
	* No feature engineering for total_distance + trip_duration (check if haversine=0)
	* best model train round: 2263
	* train time: 2406.979811668396
	* RMSLE without-log: 0.3824341923
    + Submision score: 0.388
- Round 17:
    + Base model: Round 16
	* change to haversine_np for faster computation time
	* best model train round: 2971
	* train time: 3083.501992702484
	* RMSLE without-log: 0.38154559936 
    + Submision score: 0.388 => 0.387
- Round 18:
    + Base model: Round 17
	* added new feature: speed_mean and duration_mean by haversine
	* best model train round: 
	* train time: 
	* RMSLE without-log: 0.38154559936 => 0.382880991297
    + Submision score: 0.387 => 0.389
7: 2017-07-30:
- Round 19:
    + Base model: Round 18
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.3, min_child_weight=1, n_jobs=-1)
	* best model train round: 723
	* train time: 1064.4129366874695
	* RMSLE without-log: 0.38513216986
    + Submision score: 0.390
- Round 20:
    + Base model: Round 18
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* new data set: fastest_route
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1130
	* train time: 1774.263885974884
	* RMSLE without-log: 0.382655151773
    + Submision score: 0.390
- Round 21:
    + Base model: Round 20
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* new data set: fastest_route
	* removed feature: speed_mean and duration_mean by haversine
	* removed pickup_year
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2546
	* train time: 2927.3975234031677
	* RMSLE without-log: 0.382655151773 => 0.378934064175
    + Submision score: 0.387 
- Round 22:
    + Base model: Round 21
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* new data set: fastest_route
	* used Kfold = 5
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2522 + 2127 + 2264 + 1953 + 2706 
	* train time: 2552.5379943847656 + 2201.945223093033 + 2397.255286216736 + 2121.966084718704 + 2982.0862629413605 =  12304.544
	* RMSLE without-log: (0.384708311796 + 0.380577074349 + 0.384441915744 + 0.38211835455 + 0.379893830343)/5 = 0.382347897357
    + Submision score: 0.387 => 0.386
- Round 23:
    + Base model: Round 22
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* new data set: fastest_route
	* used Kfold = 5 + aggregate results
	* stack train model 
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round:
	* best stack model train round: 434
	* train time: 257.1784813404083
	* RMSLE without-log:  0.331885656985 (stack model)
    + Submision score: 0.396

7: 2017-07-31:
- Round 24:
    + Base model: Round 21
	* model = XGBRegressor(n_estimators=10000, max_depth=10,
                     learning_rate=0.1, min_child_weight=5, n_jobs=-1)
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 805
	* train time: 1558.1954226493835
	* RMSLE without-log: 0.373046115317
    + Submision score: 0.381
- Round 25:
    + Base model: Round 24
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* used Kfold = 5
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1180 + 878 + 814 + 1100 + 1148 
	* train time: 2103.3532185554504 + 2112.0196216106415 + 1463.224598646164 + 2277.286563396454 + 1997.4008457660675 = 9953.284847974777
	* RMSLE without-log: (0.378574601563 + 0.374263204893 + 0.377778683159 + 0.374609691487 + 0.373672468293 )/5 = 0.375779729879
    + Submision score: 0.381 => 0.381	
- Round 26:
    + Base model: Round 24
	* new data set: fastest_route
	# 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 10}
	* remove haversin_distance
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2682
	* train time: 2100.6572234630585
	* RMSLE without-log: 0.383296609142
    + Submision score:  
- Round 27:
    + Base model: Round 24
	* new data set: fastest_route
	* 'max_depth': 5, 'learning_rate': 0.1, 'min_child_weight': 5
	* added starting_street_tf_speed_mean, end_street_tf_speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1669
	* train time: 1449.917546749115
	* RMSLE without-log: 0.3808412094
    + Submision score: 
- Round 28:
    + Base model: Round 27
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1054
	* train time: 2144.4699709415436
	* RMSLE without-log: 0.373046115317 => 0.372649050678
    + Submision score: 	
- Round 29:
    + Base model: Round 28
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* added  starting_street_tf_duration_mean, end_street_tf_duration_mean
	* cleanup data (trip_duration> 1800000) before pre-process
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1121
	* train time: 3381.792198896408
	* RMSLE without-log: 0.372649050678 => 0.373172177982
    + Submision score: 0.382		
- Round 30:
    + Base model: Round 29
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* remove duration_mean	
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 636
	* train time: 1033.3630764484406
	* RMSLE without-log: 0.373172177982 => 0.373016098776
    + Submision score: 
- Round 31:
    + Base model: Round 30
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* added number_of_steps_speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 611
	* train time: 1452.7844805717468
	* RMSLE without-log: 0.373172177982 => 0.372985129101
    + Submision score: 0.382
- Round 32:
    + Base model: Round 31
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* added distance_per_step
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 763
	* train time: 1941.8781354427338
	* RMSLE without-log: 0.372985129101 => 0.373206600463
    + Submision score: 0.382 =>  0.382	

8: 2017-08-01:
- Round 33:
    + Base model: Round 32
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* removed distance_per_step
	* added manhattan_distance
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 851
	* train time: 2343.4766597747803
	* RMSLE without-log: 0.373206600463 => 0.373096205348
    + Submision score: 0.381
- Round 34:
    + Base model: Round 33
	* new data set: fastest_route + weather
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 882
	* train time: 2633.0243830680847
	* RMSLE without-log: 0.373096205348 => 0.372926464895
    + Submision score: 0.381
- Round 35:
    + Base model: Round 34
	* new data set: fastest_route 
	* removed data set: weather
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* added new features: total_turns (from step_maneuvers)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1101
	* train time: 2497.3055131435394
	* RMSLE without-log: 0.373137069604
    + Submision score: 0.381
- Round 36:
    + Base model: Round 35
	* new data set: fastest_route 
	* removed data set: weather
	* 'learning_rate': 0.1, 'max_depth': 10, 'colsample_bytree': 0.9, 'min_child_weight': 1
	* removed features: total_turns (from step_maneuvers)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 891
	* train time: 2022.018221616745
	* RMSLE without-log: 0.372083580258
    + Submision score:	0.382
9: 2017-08-03:	
- Round 37:
    + Base model: Round 36
	* new data set: fastest_route 
		* 'learning_rate': 0.1, 'max_depth': 10, 'colsample_bytree': 0.9, 'min_child_weight': 1
	* added features: left_turns, right_turns (from step_direction)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 832
	* train time: 825.822839975357
	* RMSLE without-log: 0.372427951911 
    + Submision score:	0.382 => 0.381	
- Round 38:
    + Base model: Round 37
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'colsample_bytree': 0.9, 'min_child_weight': 1
	* cleanup data: (data[label] < 22 * 3600) & (data[label] > 10)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 990
	* train time: 964.5637571811676
	* RMSLE without-log: 0.372427951911 => 0.315914025687
    + Submision score:	0.381 => 0.380		
- Round 39:
    + Base model: Round 38
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1166
	* train time: 1204.0661432743073
	* RMSLE without-log: 0.315914025687 => 0.315582605322
    + Submision score:	0.380 => 0.380			
- Round 40:
    + Base model: Round 39
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* added new features: total_turns (from step_maneuvers)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1175
	* train time: 1242.2003087997437 
	* RMSLE without-log: 0.315582605322 => 0.316186051761
    + Submision score:	0.380 => 0.380				
- Round 41:
    + Base model: Round 40
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* removed features: total_turns (from step_maneuvers)
	* added *_hv_speed_mean (based on haversine_distance)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1400
	* train time: 1599.0449421405792
	* RMSLE without-log: 0.316186051761 => 0.315559158281
    + Submision score:	0.380 => 0.380 					
- Round 42:
    + Base model: Round 41
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* removed *_*_speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1199
	* train time: 1146.5380535125732
	* RMSLE without-log: 0.315559158281 => 0.316582022318
    + Submision score:	0.380 => 0.381						
- Round 43:
    + Base model: Round 42
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* added trip_delay_mean by columns (pickup_*, starting_street, end_street)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1031
	* train time: 1515.8221554756165
	* RMSLE without-log: 0.316582022318 => 0.316532398454
    + Submision score:	0.380
- Round 44:
    + Base model: Round 44
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* added different between total_distance and haversine_distance
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1666
	* train time: 1875.7382152080536
	* RMSLE without-log: 0.316532398454 => 0.315660428816
    + Submision score:	0.380

10: 2017-08-04:
- Round 45:
    + Base model: Round 44
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* removed hv_distance_diff, trip_delay_mean, speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1199
	* train time: 1082.713295698166
	* RMSLE without-log: 0.315660428816 => 0.316582022318
    + Submision score:	0.380 => 0.381
- Round 46:
    + Base model: Round 45
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* added trip_delay_mean, speed_mean, duration_mean, distance_per_step
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 919
	* train time: 2218.202323913574
	* RMSLE without-log: 0.316582022318 => 0.317227755451
    + Submision score:	0.381 => 0.382 	
11: 2017-08-06:
- Round 47:
    + Base model: Round 46
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* removed trip_delay_mean, speed_mean, duration_mean, distance_per_step
	* added pickup_whour
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1465
	* train time: 3059.0503962039948 
	* RMSLE without-log: 0.317227755451 => 0.316086616132
    + Submision score:	0.382 => 0.380
13: 2017-08-07:		
- Round 48:
    + Base model: VWRegressor
	* 'learning_rate': 0.5, passes = 10
	* target_log = np.log(target)
	* train time: 
	* RMSLE without-log: 0.52
    + Submision score:	0.380 => 0.653
14: 2017-08-07:			
- Round 49:
    + Base model: XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* correct right_turns feature
	* target_log = np.log(target)
	* best model train round: 1583
	* train time:  1914.6729001998901
	* RMSLE without-log: 0.315640291626
    + Submision score: 0.380
15: 2017-08-08:				
- Round 50:
    + Base model: XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 
	* added starting_street_cluster, end_street_cluster
	* target_log = np.log(target)
	* best model train round: 1180
	* train time: 1289.420259475708
	* RMSLE without-log: 0.315640291626 => 0.315394751588
    + Submision score: 0.380 => 0.380	
- Round 51:
    + Base model: XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 
	* added location_cluster ('starting_street_tf', 'end_street_tf'), N_CLUSTERS = 1000
	* target_log = np.log(target)
	* best model train round: 1710
	* train time: 1937.9622585773468
	* RMSLE without-log: 0.315640291626 => 0.314919531323
    + Submision score: 0.380 => 0.380		
- Round 52:
    + Base model: XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 
	* added location_cluster ('starting_street_tf', 'end_street_tf'), N_CLUSTERS = 200
	* added pickup_cluster ('pickup_latitude', 'pickup_longitude'), dropoff_cluster ('dropoff_latitude', 'dropoff_longitude')
	* target_log = np.log(target)
	* best model train round: 1495
	* train time: 1577.4168784618378
	* RMSLE without-log: 0.314919531323 => 0.31608909162
    + Submision score: 0.380 => 0.380			
- Round 53:
    + Base model: CatBoostRegressor
	* iterations=10000, learning_rate=0.1, depth=10 
	* removed Kmeans cluster 
	* target_log = np.log(target)
	* best model train round: 10000
	* train time: 2500
	* RMSLE without-log: 0.343456689581
    + Submision score: 0.380 => 0.406				
16: 2017-08-09:
- Round 54:
    + Base model: CatBoostRegressor
	* iterations=20000, learning_rate=0.1, depth=10 
	* target_log = np.log(target)
	* best model train round: 19990
	* train time: 5350.181067705154
	* RMSLE without-log: 0.328954457603
    + Submision score: 0.406 => 0.393
- Round 55:
    + Base model: CatBoostRegressor
	* iterations=50000, learning_rate=0.03, depth=10
	* target_log = np.log(target)
	* best model train round: 49887
	* train time: 14021.58697462082
	* RMSLE without-log: 0.349743035723
    + Submision score: 0.393 => 0.412
17: 2017-08-10:	
- Round 56:
    + Base model: CatBoostRegressor
	* iterations=30000, learning_rate=0.1, depth=10 
	* cat_features = ['vendor_id', 'store_and_fwd_flag', 'pickup_month',
                        'pickup_weekday', 'pickup_day', 'pickup_hour',
                        'pickup_whour', 'pickup_minute'
                        ]
	* target_log = np.log(target)
	* best model train round: 
	* train time: 
	* RMSLE without-log: 
    + Submision score: 
- Round 57:
    + Base model: XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 
	* added location_cluster ('starting_street_tf', 'end_street_tf'), N_CLUSTERS = 200
	* added pickup_cluster ('pickup_latitude', 'pickup_longitude'), dropoff_cluster ('dropoff_latitude', 'dropoff_longitude')
	* target_log = np.log(target)
	* best model train round: 1387 
	* train time: 3020.7012190818787
	* RMSLE without-log: 0.315679178625
    + Submision score: 0.380 => 0.380
- Round 58:
    + Base model: LGBMRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 
	* added location_cluster ('starting_street_tf', 'end_street_tf'), N_CLUSTERS = 200
	* added pickup_cluster ('pickup_latitude', 'pickup_longitude'), dropoff_cluster ('dropoff_latitude', 'dropoff_longitude')
	* target_log = np.log(target)
	* best model train round: 4940
	* train time: 198.60693311691284
	* RMSLE without-log: 0.324367347584
    + Submision score: 0.388
- Round 59:
    + Base model: Round 58
	* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 5
	* target_log = np.log(target)
	* best model train round: 19252
	* train time: 752.7515609264374
	* RMSLE without-log: 0.324367347584 => 0.322236424969
    + Submision score: 0.388 => 0.386	
18: 2017-08-11:
- Round 60:
    + Base model: Round 59 
	* LGBMRegressor
	* 'learning_rate': 0.1, num_leaves=512
	* target_log = np.log(target)
	* best model train round:1451 
	* train time: 140.3382053375244
	* RMSLE without-log: 0.322236424969 => 0.321227
    + Submision score: 0.386 => 			
- Round 61:
    + Base model: Round 60 
	* LGBMRegressor
	* 'learning_rate': 0.1, num_leaves=1024
	* target_log = np.log(target)
	* best model train round: 818
	* train time: 140.1048858165741
	* RMSLE without-log: 0.321227 => 0.32146863452
    + Submision score: 0.386 => 0.387				
- Round 62:
    + Base model: Round 60 
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=512
	* target_log = np.log(target)
	* best model train round: 4629
	* train time: 446.7575452327728
	* RMSLE without-log: 0.32146863452 => 0.318067211916
    + Submision score: 0.387 => 0.384			
- Round 63:
    + Base model: Round 60 
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1024 => Best
	* target_log = np.log(target)
	* best model train round: 3553
	* train time: 562.9755690097809
	* RMSLE without-log: 0.318067211916 => 0.31787662896
    + Submision score: 0.384 => 0.383				
- Round 64:
    + Base model: Round 60 
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=2048
	* target_log = np.log(target)
	* best model train round: 1605
	* train time: 554.6046388149261
	* RMSLE without-log: 0.31787662896 => 0.318766680087
    + Submision score: 0.383 => 
- Round 65:
    + Base model: Round 60 
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1536
	* target_log = np.log(target)
	* best model train round: 2900
	* train time:650.9595715999603
	* RMSLE without-log: 0.31787662896 => 0.318134380655
    + Submision score: 0.383 => 			
- Round 66:
    + Base model: Round 60 
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1024
	*  cat_features = ['vendor_id', 'store_and_fwd_flag', 'pickup_month',
                        'pickup_weekday', 'pickup_day', 'pickup_hour',
                        'pickup_whour', 'pickup_minute'
                        ]
	* target_log = np.log(target)
	* best model train round: 1646
	* train time: 575.8021259307861
	* RMSLE without-log: 0.31787662896 => 0.335223829409
    + Submision score: 0.383 => 0.398				
- Round 67:
    + Base model: Round 66 
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=2048
	* target_log = np.log(target)
	* best model train round: 966
	* train time: 309.1839175224304
	* RMSLE without-log: 0.335223829409 => 0.339737112562
    + Submision score: 0.383 => 
- Round 68:
    + Base model: Round 66 
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1024, max_bin=512
	* target_log = np.log(target)
	* best model train round: 1408
	* train time: 317.5527174472809
	* RMSLE without-log: 0.31787662896 => 0.337128140915
    + Submision score: 0.383 => 	
- Round 69:
    + Base model: Round 66 
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1024, max_bin=1024
	* cat_features
	* target_log = np.log(target)
	* best model train round: 2025
	* train time: 433.9591565132141
	* RMSLE without-log: (0.31787662896) 0.337128140915 => 0.336225641011
    + Submision score: 0.383 => 		
- Round 70:
    + Base model: Round 66
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1024, max_bin=2048
	* cat_features
	* target_log = np.log(target)
	* best model train round: 1653
	* train time: 440.6618022918701
	* RMSLE without-log: (0.31787662896) 0.336225641011 => 0.335734921887
    + Submision score: 0.383 => 			
- Round 71:
    + Base model: Round 66
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1024, max_bin=8192
	* cat_features
	* target_log = np.log(target)
	* best model train round: 1826
	* train time: 896.5467207431793
	* RMSLE without-log: (0.31787662896) 0.335734921887 => 0.335780889774
    + Submision score: 0.383 => 				
- Round 72:
    + Base model: Round 66
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1024, max_bin=4096
	* cat_features
	* target_log = np.log(target)
	* best model train round: 1659
	* train time: 579.672604560852
	* RMSLE without-log: (0.31787662896) 0.335734921887 => 0.335573925783
    + Submision score: 0.383 => 					
- Round 73:
    + Base model: Round 66
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=1024, max_bin=2048
	* no cat_features
	* target_log = np.log(target)
	* best model train round: 3673
	* train time: 795.7316427230835
	* RMSLE without-log: (0.31787662896) => 0.319262749763
    + Submision score: 0.383 => 						
- Round 74:
    + Base model: Round 73
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=2048, max_bin=2048
	* no cat_features
	* target_log = np.log(target)
	* best model train round: 2039
	* train time: 873.2032713890076
	* RMSLE without-log: (0.31787662896) =>0.319737233272
    + Submision score: 0.383 => 0.398							
- Round 75:
    + Base model: Round 74
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=4096, max_bin=1024
	* no cat_features
	* target_log = np.log(target)
	* best model train round: 1789
	* train time: 779.1963686943054
	* RMSLE without-log: (0.31787662896) => 0.320930091473
    + Submision score: 0.383 => 0.385
- Round 76:
    + Base model: Round 75
	* LGBMRegressor
	* 'learning_rate': 0.03, num_leaves=4096, max_bin=1024, min_data_in_leaf=100,
	* no cat_features
	* target_log = np.log(target)
	* best model train round: 651
	* train time: 430.4107961654663
	* RMSLE without-log: 0.320930091473 => 0.320515793758
    + Submision score: 0.383 => 0.384	
- Round 77:
    + Base model: Round 76
	* LGBMRegressor
	* 'learning_rate': 0.01, num_leaves=4096, max_bin=1024, min_data_in_leaf=100,
	* no cat_features
	* target_log = np.log(target)
	* best model train round: 1814
	* train time: 1210.5160865783691
	* RMSLE without-log:0.320515793758 => 0.320260748897
    + Submision score: 0.383 => 0.383
- Round 78:
    + Base model: Round 77
	* LGBMRegressor
	* 'learning_rate': 0.01, num_leaves=16384, max_bin=1024, min_data_in_leaf=100,
	* no cat_features
	* target_log = np.log(target)
	* best model train round: 1381
	* train time: 1887.764794588089
	* RMSLE without-log:0.320260748897 => 0.322163547467
    + Submision score: 0.383 => 0.385
- Round 79:
    + Base model: Round 78
	* LGBMRegressor
	* 'learning_rate': 0.01, num_leaves=8192, max_bin=1024, min_data_in_leaf=100,
	* no cat_features
	* target_log = np.log(target)
	* best model train round:1394 
	* train time: 1689.6206121444702
	* RMSLE without-log:0.320260748897 => 0.321646406478
    + Submision score: 0.383 => 0.384
19: 2017-08-11: XGBRegressor
- Round 80:
    + Base model: Round 57
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* added n_tree_limit in xgboost.predict	 
	* target_log = np.log(target)
	* best model train round: 1387 => 1492
	* train time: 2079.768365383148
	* RMSLE without-log: 0.315679178625 => 0.315402829663
    + Submision score: 0.380 => 0.38045
- Round 81:
    + Base model: Round 80
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5, 'gamma': 1
	* added n_tree_limit in xgboost.predict	 
	* target_log = np.log(target)
	* best model train round: 324
	* train time: 507.14076113700867
	* RMSLE without-log: 0.315402829663 => 0.321074187444
    + Submision score: 0.380 => 0.38488	
- Round 82:
    + Base model: Round 81
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5, 'gamma': 0.2
	* added n_tree_limit in xgboost.predict	 
	* target_log = np.log(target)
	* best model train round: 702
	* train time: 851.6674802303314
	* RMSLE without-log: 0.315402829663 => 0.317003309189
    + Submision score: 0.38045 => 0.38090
- Round 83:
    + Base model: Round 82
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 6, 'min_child_weight': 1, 'gamma': 0
	* added n_tree_limit in xgboost.predict	 
	* target_log = np.log(target)
	* best model train round: 2652
	* train time: 2110.9427256584167
	* RMSLE without-log: 0.317003309189 => 0.320264523865
    + Submision score: 0.38045 => 0.38090
- Round 84:
    + Base model: Round 82
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, 'gamma': 0
	* added n_tree_limit in xgboost.predict	 
	* target_log = np.log(target)
	* best model train round: 1513
	* train time: 2125.335698366165
	* RMSLE without-log: 0.320264523865 => 0.315268059834
    + Submision score: 0.38090 => 0.38017 => Best	
- Round 85:
    + Base model: Round 84
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 15, 'min_child_weight': 1, 'gamma': 0
	* added n_tree_limit in xgboost.predict	 
	* target_log = np.log(target)
	* best model train round: 510
	* train time: 1139.539 seconds
	* RMSLE without-log: 0.315268059834 => 0.316688266073
    + Submision score: 0.38017 => 0.38124 => Overfit		
- Round 86:
    + Base model: Round 85
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 20, 'min_child_weight': 5, 'gamma': 0
	* added n_tree_limit in xgboost.predict	 
	* target_log = np.log(target)
	* best model train round: 496
	* train time: 1277.2605113983154
	* RMSLE without-log: 0.315268059834 => 0.32044403701
    + Submision score: 0.38017 => 0.38381
- Round 86:
    + Base model: Round 85
	* XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 2, 'gamma': 0
	* added n_tree_limit in xgboost.predict	 
	* target_log = np.log(target)
	* best model train round: 1312.
	* train time: 1429.1299285888672
	* RMSLE without-log: 0.315268059834 => 0.315325770346
    + Submision score: 0.38017 => 0.38053

20: 2017-08-15: LGBMRegressor
- Round 87:
    + Base model: Round 77
	* LGBMRegressor
	* 'learning_rate': 0.01, num_leaves=4096, max_bin=1024, min_data_in_leaf=100,
	* no cat_features
	* target_log = np.log(target)
	* best model train round: 1814
	* train time: 1510.8735432624817
	* RMSLE without-log:0.320260748897 => 0.317506379624
    + Submision score: 0.383 => 0.38132
21: 2017-08-16: Stack models
- Round 88: Stack model
    + Base model: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.313979901429
			. LB: 0.38017
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=4096, max_bin=1024, min_data_in_leaf=100
			. RMSLE without-log: 0.316250790001
			. LB: 0.38132
		* train time: 19649.848478794098
		* target_log = np.log(target)
		* RMSLE without-log: 0.315115345715	
	+ Stack model: LGBMRegressor
		* 'learning_rate': 0.03, num_leaves=1024
		* best model train round: 146
		* train time: 15.97771143913269
		* RMSLE without-log: 0.315313
    + Submision score: 0.37789 
- Round 89: Stack model
    + Base model: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.313979901429
			. LB: 0.38017
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=4096, max_bin=1024, min_data_in_leaf=100
			. RMSLE without-log: 0.316250790001
			. LB: 0.38132
		* train time: 19649.848478794098
		* target_log = np.log(target)
		* RMSLE without-log: 0.315115345715	
	+ Stack model: LGBMRegressor
		* 'learning_rate': 0.01, num_leaves=1024  => Best 
		* best model train round: 454
		* train time: 35.55270266532898
		* RMSLE without-log: 0.315306
    + Submision score: 0.37789 => 0.37786 => Best	
- Round 90: Stack model
    + Base model: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024, random_state=1024
			. RMSLE without-log: 0.316645074098
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024, random_state=123
			. RMSLE without-log: 0.316771299548
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024, random_state=789
			. RMSLE without-log: 0.316723905823		
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, random_state=1000
			. RMSLE without-log: 0.313979901429	
		* train time: 27751.25788140297
		* target_log = np.log(target)
		* RMSLE without-log: 0.316030045225 
	+ Stack model: LGBMRegressor
		* 'learning_rate': 0.01, num_leaves=1024  => Best 
		* best model train round: 416
		* train time: 31.391589164733887
		* RMSLE without-log: 
    + Submision score: 0.37786 => 0.37848
- Round 91: Stack model
    + Base model: 
		* XGBRegressor:  
			. 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=456
			. RMSLE without-log: 0.311739403458 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, random_state=1000
			. RMSLE without-log: 0.313979901429 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024, random_state=1024
			. RMSLE without-log: 0.316645057696
		* train time: 38799.84138727188
		* target_log = np.log(target)
		* RMSLE without-log: 0.314121454194 
	+ Stack model: LGBMRegressor
		* 'learning_rate': 0.01, num_leaves=1024  => Best 
		* best model train round: 399
		* train time: 29.3829271793365
		* RMSLE without-log: 0.315103
    + Submision score: 0.37786 => 0.37798
- Round 92: Stack model
    + Base model: 
		* XGBRegressor:  
			. 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=456
			. RMSLE without-log: 0.31168934339
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024, random_state=1024
			. RMSLE without-log: 0.316246238577
		* train time: 31932.34571838379
		* target_log = np.log(target)
		* RMSLE without-log: 0.314192231109
	+ Stack model: LGBMRegressor
		* 'learning_rate': 0.01, num_leaves=1024  => Best 
		* best model train round: 432
		* train time: 34
		* RMSLE without-log: 0.314625 
    + Submision score: 0.37786 => 0.37807
- Round 93: Stack model
    + Base model: 
		* XGBRegressor:  
			. 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=456
			. RMSLE without-log: 0.31168934339
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024, random_state=1024
			. RMSLE without-log: 0.316246238577
		* train time: 31932.34571838379
		* target_log = np.log(target)
		* RMSLE without-log: 0.314192231109
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* best model train round: 64
		* train time: 35.700355768203735
		* RMSLE without-log: 0.313657
    + Submision score: 0.37786 => 0.37646 
- Round 94: Stack model
    + Base model: 
		* XGBRegressor:  
			. 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=456
			. RMSLE without-log: 0.31168934339
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024, random_state=1024
			. RMSLE without-log: 0.316246238577
		* train time: 31932.34571838379
		* target_log = np.log(target)
		* RMSLE without-log: 0.314192231109
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* best model train round: 227
		* train time: 106.15630888938904 
		* RMSLE without-log: 0.31359 
    + Submision score: 0.37646 => 0.37637 
- Round 95: Stack model
    + Base model: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.313979901429
			. LB: 0.38017
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0.316250790001 => 0. 316645070975
			. LB: 0.38132
		* train time: 16250.022941589355
		* target_log = np.log(target)
		* RMSLE without-log: 0.315115345715	 => 0.315312486202
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* best model train round: 237 
		* train time: 
		* RMSLE without-log: 0.31359 => 0.314354
    + Submision score: 0.37637 =>  0.37627
2017-08-19:	
- Round 96: Stack model
    + Base model: 
		* XGBRegressor:  
			. 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1
			. RMSLE without-log: 0.313979901429 => 0.3112062235560
			. LB: 
		* LGBMRegressor: 
			. 'learning_rate': 0.01, num_leaves=1024
			. RMSLE without-log: 0. 316645070975 => 0.311270633371
			. LB: 
		* train time: 15186.800484418869
		* target_log = np.log(target)
		* added cluster for location + direction
		* RMSLE without-log: 0.315312486202 => 0.311238428463
	+ Stack model: XGBRegressor
		* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 1, random_state=567
		* best model train round: 232
		* train time: 
		* RMSLE without-log: 0.314354 => 0.310241
    + Submision score: 0.37627 => 0.37399 => Best