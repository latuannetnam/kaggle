
0.: Requirements
 - use pipreqs ./ to regenerate requirements file
 - pip3 install -r requirments.txt
 - Ubuntu:
	+ sudo apt-get install libspatialindex-dev
	+ sudo apt-get install libgeos-dev
 - CentOS:
	+ yum install -y spatialindex-devel
	+ yum install -y geos-devel
	+ yum -y install python34-tkinter
0.1:
 - Best model: round 3: 0.417
 - Best model: round 8: 0.401 (added distance using haversine)
 - Best model: round 11: 0.390 (added distance using OSM + haversine distance, speed_mean, total_distance_mean)
 - Best model: round 21: 0.387 (removed feature: speed_mean and duration_mean by haversine)
 - Best model: round 22: 0.386 (KFold=5)
 - Best model: round 24: 0.381 (optimized model hyper-params: 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10)
 - Best model: round 28: 0.380 (cleanup data: (data[label] < 22 * 3600) & (data[label] > 10) + 'learning_rate': 0.1, 'max_depth': 10, 'colsample_bytree': 0.9, 'min_child_weight': 1)
 
1. 2017-07-21
- Round 1:
    + Base model: 
        * XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,
       max_depth=5, min_child_weight=1, missing=None, n_estimators=2000,
       n_jobs=-1, nthread=None, objective='reg:linear', random_state=0,
       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
       silent=True, subsample=1)
	* use xgboost.cv
	* train_size = 0.85
	* target_log = np.log(target)
	* train time: 924.1382086277008
	* RMSLE without-log: 0.423621130054
    + Submision score: 0.427 
- Round 2:
    + Base model: 
        * XGBRegressor(n_estimators=5000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove outliner: train_set[label] > 5000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 5000
	* train time: 2399.4174466133118
	* RMSLE without-log: 0.374140060512
    + Submision score: 0.419
- Round 3:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.3, min_child_weight=1, n_jobs=-1)
	* remove outliner: train_set[label] > 5000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 3921
	* train time: 1854.232224702835
	* RMSLE without-log: 0.374140060512 => 0.371386768324
    + Submision score: 0.419 =>  0.417
- Round 4:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* all train data (not remove outliners)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 5093
	* train time: 2379.7198255062103
	* RMSLE without-log: 0.374140060512 => 0.41737086459
    + Submision score: 0.421
- Round 5:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.3, min_child_weight=1, n_jobs=-1)
	* feature scaling
	* remove pickup_year (correl = 0)
	* remove outliner: train_set[label] > 5000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1403
	* train time: 668.2613804340363
	* RMSLE without-log: 0.37665957093
    + Submision score: 0.417 => 0.422
2. 2017--7-23:
- Round 6:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* feature scaling
	* remove pickup_year (correl = 0)
	* remove outliner: train_set[label] > 5000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2413
	* train time: 1149.2097866535187
	* RMSLE without-log: 0.37665957093 => 0.379491160168
    + Submision score: 0.422 => 0.424
- Round 7:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 3107
	* train time: 1545.7049226760864
	* RMSLE without-log: 0.419480465158 
    + Submision score: 0.424
- Round 8:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* Added distance between pickup and dropoff using haversine
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2983 
	* train time: 1751.8312721252441
	* RMSLE without-log: 0.419480465158 => 0.395957129069
    + Submision score: 0.424 => 0.401
3: 2017-07-26:
- Round 9:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* Added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 3124
	* train time: 1665.9864552021027
	* RMSLE without-log: 0.388475183465
    + Submision score: 0.393
- Round 10:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* Added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* Added distance between pickup and dropoff using haversine dataset
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1874
	* train time: 1238.130537033081
	* RMSLE without-log: 0.388475183465 => 0.38546321876
    + Submision score: 0.393 => 0.391
- Round 11:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* Added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* Added distance between pickup and dropoff using haversine dataset
	* added speed_mean, total_distance_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2853
	* train time: 1934.7547209262848
	* RMSLE without-log: 0.38546321876 => 0.383703421188
    + Submision score: 0.391 => 0.390
4: 2017-07-27:
- Round 12:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove pickup_year (correl = 0)
	* remove outliners: train_set[label] > 1800000
	* remove outliners: train_set['total_distance_mean] = 0
	* Added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* Added distance between pickup and dropoff using haversine dataset
	* correct speed_mean formular
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2985
	* train time: 1968.406958580017
	* RMSLE without-log: 0.383703421188 => 0.384153
    + Submision score:  
5: 2017-07-28:	
- Round 13:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove outliners: train_set[label] > 1800000
	* added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* added distance between pickup and dropoff using haversine dataset
	* added number_of_streets, starting_street, end_street
	* predict total_distance (for total_distance=0)
	* predict starting_street, end_street (for NaN starting_street, end_street)
	* added pickup_hour_speed_mean, pickup_weekday_speed_mean, pickup_day_speed_mean, pickup_month_speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2567
	* train time: 1943.313801765442
	* RMSLE without-log: 0.384153 => 0.369416883687
    + Submision score:  0.384153 => 0.428		
6: 2017-07-29:
- Round 14:
    + Base model: 
        * XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* remove outliners: train_set[label] > 1800000
	* added distance between pickup and dropoff using outside dataset (OpenStreetMap - osmr)
	* added distance between pickup and dropoff using haversine dataset
	* added number_of_streets, starting_street, end_street
	* predict total_distance (for total_distance=0)
	* predict starting_street, end_street (for NaN starting_street, end_street)
	* added pickup_hour_speed_mean, pickup_weekday_speed_mean, pickup_day_speed_mean, pickup_month_speed_mean
	* added hour_duration_mean, weekday_duration_mean, day_duration_mean, month_duration_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 3001
	* train time: 3092.225347518921
	* RMSLE without-log: 0.370031041363
    + Submision score: 0.422 
- Round 15:
    + Base model: Round 14
	* Set trip_duration=0 if haversin_distance=0
    + Submision score: 0.422 => 0.504
- Round 16:
    + Base model: Round 14
	* No feature engineering for total_distance + trip_duration (check if haversine=0)
	* best model train round: 2263
	* train time: 2406.979811668396
	* RMSLE without-log: 0.3824341923
    + Submision score: 0.388
- Round 17:
    + Base model: Round 16
	* change to haversine_np for faster computation time
	* best model train round: 2971
	* train time: 3083.501992702484
	* RMSLE without-log: 0.38154559936 
    + Submision score: 0.388 => 0.387
- Round 18:
    + Base model: Round 17
	* added new feature: speed_mean and duration_mean by haversine
	* best model train round: 
	* train time: 
	* RMSLE without-log: 0.38154559936 => 0.382880991297
    + Submision score: 0.387 => 0.389
7: 2017-07-30:
- Round 19:
    + Base model: Round 18
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.3, min_child_weight=1, n_jobs=-1)
	* best model train round: 723
	* train time: 1064.4129366874695
	* RMSLE without-log: 0.38513216986
    + Submision score: 0.390
- Round 20:
    + Base model: Round 18
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* new data set: fastest_route
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1130
	* train time: 1774.263885974884
	* RMSLE without-log: 0.382655151773
    + Submision score: 0.390
- Round 21:
    + Base model: Round 20
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* new data set: fastest_route
	* removed feature: speed_mean and duration_mean by haversine
	* removed pickup_year
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2546
	* train time: 2927.3975234031677
	* RMSLE without-log: 0.382655151773 => 0.378934064175
    + Submision score: 0.387 
- Round 22:
    + Base model: Round 21
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* new data set: fastest_route
	* used Kfold = 5
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2522 + 2127 + 2264 + 1953 + 2706 
	* train time: 2552.5379943847656 + 2201.945223093033 + 2397.255286216736 + 2121.966084718704 + 2982.0862629413605 =  12304.544
	* RMSLE without-log: (0.384708311796 + 0.380577074349 + 0.384441915744 + 0.38211835455 + 0.379893830343)/5 = 0.382347897357
    + Submision score: 0.387 => 0.386
- Round 23:
    + Base model: Round 22
	* model = XGBRegressor(n_estimators=10000, max_depth=5,
                     learning_rate=0.1, min_child_weight=1, n_jobs=-1)
	* new data set: fastest_route
	* used Kfold = 5 + aggregate results
	* stack train model 
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round:
	* best stack model train round: 434
	* train time: 257.1784813404083
	* RMSLE without-log:  0.331885656985 (stack model)
    + Submision score: 0.396

7: 2017-07-31:
- Round 24:
    + Base model: Round 21
	* model = XGBRegressor(n_estimators=10000, max_depth=10,
                     learning_rate=0.1, min_child_weight=5, n_jobs=-1)
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 805
	* train time: 1558.1954226493835
	* RMSLE without-log: 0.373046115317
    + Submision score: 0.381
- Round 25:
    + Base model: Round 24
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* used Kfold = 5
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1180 + 878 + 814 + 1100 + 1148 
	* train time: 2103.3532185554504 + 2112.0196216106415 + 1463.224598646164 + 2277.286563396454 + 1997.4008457660675 = 9953.284847974777
	* RMSLE without-log: (0.378574601563 + 0.374263204893 + 0.377778683159 + 0.374609691487 + 0.373672468293 )/5 = 0.375779729879
    + Submision score: 0.381 => 0.381	
- Round 26:
    + Base model: Round 24
	* new data set: fastest_route
	# 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 10}
	* remove haversin_distance
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 2682
	* train time: 2100.6572234630585
	* RMSLE without-log: 0.383296609142
    + Submision score:  
- Round 27:
    + Base model: Round 24
	* new data set: fastest_route
	* 'max_depth': 5, 'learning_rate': 0.1, 'min_child_weight': 5
	* added starting_street_tf_speed_mean, end_street_tf_speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1669
	* train time: 1449.917546749115
	* RMSLE without-log: 0.3808412094
    + Submision score: 
- Round 28:
    + Base model: Round 27
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1054
	* train time: 2144.4699709415436
	* RMSLE without-log: 0.373046115317 => 0.372649050678
    + Submision score: 	
- Round 29:
    + Base model: Round 28
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* added  starting_street_tf_duration_mean, end_street_tf_duration_mean
	* cleanup data (trip_duration> 1800000) before pre-process
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1121
	* train time: 3381.792198896408
	* RMSLE without-log: 0.372649050678 => 0.373172177982
    + Submision score: 0.382		
- Round 30:
    + Base model: Round 29
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* remove duration_mean	
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 636
	* train time: 1033.3630764484406
	* RMSLE without-log: 0.373172177982 => 0.373016098776
    + Submision score: 
- Round 31:
    + Base model: Round 30
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* added number_of_steps_speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 611
	* train time: 1452.7844805717468
	* RMSLE without-log: 0.373172177982 => 0.372985129101
    + Submision score: 0.382
- Round 32:
    + Base model: Round 31
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* added distance_per_step
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 763
	* train time: 1941.8781354427338
	* RMSLE without-log: 0.372985129101 => 0.373206600463
    + Submision score: 0.382 =>  0.382	

8: 2017-08-01:
- Round 33:
    + Base model: Round 32
	* new data set: fastest_route
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* removed distance_per_step
	* added manhattan_distance
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 851
	* train time: 2343.4766597747803
	* RMSLE without-log: 0.373206600463 => 0.373096205348
    + Submision score: 0.381
- Round 34:
    + Base model: Round 33
	* new data set: fastest_route + weather
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 882
	* train time: 2633.0243830680847
	* RMSLE without-log: 0.373096205348 => 0.372926464895
    + Submision score: 0.381
- Round 35:
    + Base model: Round 34
	* new data set: fastest_route 
	* removed data set: weather
	* 'learning_rate': 0.1, 'min_child_weight': 5, 'max_depth': 10
	* added new features: total_turns (from step_maneuvers)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1101
	* train time: 2497.3055131435394
	* RMSLE without-log: 0.373137069604
    + Submision score: 0.381
- Round 36:
    + Base model: Round 35
	* new data set: fastest_route 
	* removed data set: weather
	* 'learning_rate': 0.1, 'max_depth': 10, 'colsample_bytree': 0.9, 'min_child_weight': 1
	* removed features: total_turns (from step_maneuvers)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 891
	* train time: 2022.018221616745
	* RMSLE without-log: 0.372083580258
    + Submision score:	0.382
9: 2017-08-03:	
- Round 37:
    + Base model: Round 36
	* new data set: fastest_route 
		* 'learning_rate': 0.1, 'max_depth': 10, 'colsample_bytree': 0.9, 'min_child_weight': 1
	* added features: left_turns, right_turns (from step_direction)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 832
	* train time: 825.822839975357
	* RMSLE without-log: 0.372427951911 
    + Submision score:	0.382 => 0.381	
- Round 38:
    + Base model: Round 37
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'colsample_bytree': 0.9, 'min_child_weight': 1
	* cleanup data: (data[label] < 22 * 3600) & (data[label] > 10)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 990
	* train time: 964.5637571811676
	* RMSLE without-log: 0.372427951911 => 0.315914025687
    + Submision score:	0.381 => 0.380		
- Round 39:
    + Base model: Round 38
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1166
	* train time: 1204.0661432743073
	* RMSLE without-log: 0.315914025687 => 0.315582605322
    + Submision score:	0.380 => 0.380			
- Round 40:
    + Base model: Round 39
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* added new features: total_turns (from step_maneuvers)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1175
	* train time: 1242.2003087997437 
	* RMSLE without-log: 0.315582605322 => 0.316186051761
    + Submision score:	0.380 => 0.380				
- Round 41:
    + Base model: Round 40
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* removed features: total_turns (from step_maneuvers)
	* added *_hv_speed_mean (based on haversine_distance)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1400
	* train time: 1599.0449421405792
	* RMSLE without-log: 0.316186051761 => 0.315559158281
    + Submision score:	0.380 => 0.380 					
- Round 42:
    + Base model: Round 41
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* removed *_*_speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1199
	* train time: 1146.5380535125732
	* RMSLE without-log: 0.315559158281 => 0.316582022318
    + Submision score:	0.380 => 0.381						
- Round 43:
    + Base model: Round 42
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* added trip_delay_mean by columns (pickup_*, starting_street, end_street)
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1031
	* train time: 1515.8221554756165
	* RMSLE without-log: 0.316582022318 => 0.316532398454
    + Submision score:	0.380
- Round 44:
    + Base model: Round 44
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* added different between total_distance and haversine_distance
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1666
	* train time: 1875.7382152080536
	* RMSLE without-log: 0.316532398454 => 0.315660428816
    + Submision score:	0.380

10: 2017-08-04:
- Round 45:
    + Base model: Round 44
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* removed hv_distance_diff, trip_delay_mean, speed_mean
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1199
	* train time: 1082.713295698166
	* RMSLE without-log: 0.315660428816 => 0.316582022318
    + Submision score:	0.380 => 0.381
- Round 46:
    + Base model: Round 45
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* added trip_delay_mean, speed_mean, duration_mean, distance_per_step
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 919
	* train time: 2218.202323913574
	* RMSLE without-log: 0.316582022318 => 0.317227755451
    + Submision score:	0.381 => 0.382 	
11: 2017-08-06:
- Round 47:
    + Base model: Round 46
	* new data set: fastest_route 
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* removed trip_delay_mean, speed_mean, duration_mean, distance_per_step
	* added pickup_whour
	* train_size = 0.85
	* target_log = np.log(target)
	* best model train round: 1465
	* train time: 3059.0503962039948 
	* RMSLE without-log: 0.317227755451 => 0.316086616132
    + Submision score:	0.382 => 0.380
13: 2017-08-07:		
- Round 48:
    + Base model: VWRegressor
	* 'learning_rate': 0.5, passes = 10
	* target_log = np.log(target)
	* train time: 
	* RMSLE without-log: 0.52
    + Submision score:	0.380 => 0.653
14: 2017-08-07:			
- Round 49:
    + Base model: XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5
	* correct right_turns feature
	* target_log = np.log(target)
	* best model train round: 1583
	* train time:  1914.6729001998901
	* RMSLE without-log: 0.315640291626
    + Submision score: 0.380
15: 2017-08-08:				
- Round 50:
    + Base model: XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 
	* added starting_street_cluster, end_street_cluster
	* target_log = np.log(target)
	* best model train round: 1180
	* train time: 1289.420259475708
	* RMSLE without-log: 0.315640291626 => 0.315394751588
    + Submision score: 0.380 => 0.380	
- Round 51:
    + Base model: XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 
	* added location_cluster ('starting_street_tf', 'end_street_tf'), N_CLUSTERS = 1000
	* target_log = np.log(target)
	* best model train round: 1710
	* train time: 1937.9622585773468
	* RMSLE without-log: 0.315640291626 => 0.314919531323
    + Submision score: 0.380 => 0.380		
- Round 52:
    + Base model: XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 
	* added location_cluster ('starting_street_tf', 'end_street_tf'), N_CLUSTERS = 200
	* added pickup_cluster ('pickup_latitude', 'pickup_longitude'), dropoff_cluster ('dropoff_latitude', 'dropoff_longitude')
	* target_log = np.log(target)
	* best model train round: 1495
	* train time: 1577.4168784618378
	* RMSLE without-log: 0.314919531323 => 0.31608909162
    + Submision score: 0.380 => 0.380			
- Round 53:
    + Base model: CatBoostRegressor
	* iterations=10000, learning_rate=0.1, depth=10 
	* removed Kmeans cluster 
	* target_log = np.log(target)
	* best model train round: 10000
	* train time: 2500
	* RMSLE without-log: 0.343456689581
    + Submision score: 0.380 => 0.406				
16: 2017-08-09:
- Round 54:
    + Base model: CatBoostRegressor
	* iterations=20000, learning_rate=0.1, depth=10 
	* target_log = np.log(target)
	* best model train round: 19990
	* train time: 5350.181067705154
	* RMSLE without-log: 0.328954457603
    + Submision score: 0.406 => 0.393
- Round 55:
    + Base model: CatBoostRegressor
	* iterations=50000, learning_rate=0.03, depth=10
	* target_log = np.log(target)
	* best model train round: 49887
	* train time: 14021.58697462082
	* RMSLE without-log: 0.349743035723
    + Submision score: 0.393 => 0.412
17: 2017-08-10:	
- Round 56:
    + Base model: CatBoostRegressor
	* iterations=30000, learning_rate=0.1, depth=10 
	* cat_features = ['vendor_id', 'store_and_fwd_flag', 'pickup_month',
                        'pickup_weekday', 'pickup_day', 'pickup_hour',
                        'pickup_whour', 'pickup_minute'
                        ]
	* target_log = np.log(target)
	* best model train round: 
	* train time: 
	* RMSLE without-log: 
    + Submision score: 
- Round 57:
    + Base model: XGBRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 
	* added location_cluster ('starting_street_tf', 'end_street_tf'), N_CLUSTERS = 200
	* added pickup_cluster ('pickup_latitude', 'pickup_longitude'), dropoff_cluster ('dropoff_latitude', 'dropoff_longitude')
	* target_log = np.log(target)
	* best model train round: 1387 
	* train time: 3020.7012190818787
	* RMSLE without-log: 0.315679178625
    + Submision score: 0.380 => 0.380
- Round 58:
    + Base model: LGBMRegressor
	* 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 
	* added location_cluster ('starting_street_tf', 'end_street_tf'), N_CLUSTERS = 200
	* added pickup_cluster ('pickup_latitude', 'pickup_longitude'), dropoff_cluster ('dropoff_latitude', 'dropoff_longitude')
	* target_log = np.log(target)
	* best model train round: 4940
	* train time: 198.60693311691284
	* RMSLE without-log: 0.324367347584
    + Submision score: 0.388
- Round 59:
    + Base model: Round 58
	* 'learning_rate': 0.03, 'max_depth': 10, 'min_child_weight': 5
	* target_log = np.log(target)
	* best model train round: 19252
	* train time: 752.7515609264374
	* RMSLE without-log: 0.324367347584 => 0.322236424969
    + Submision score: 0.388 => 0.386	