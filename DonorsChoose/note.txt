1. Keras + FastText
R1.1:
- params:
    VOCAB_SIZE = 4000
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300  # use with pretrained word2vec
    KERAS_LEARNING_RATE = 0.03
    KERAS_N_ROUNDS = 20
    KERAS_BATCH_SIZE = 64
    KERAS_DROPOUT_RATE = 0.2
    KERAS_REGULARIZER = 0.04
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 4096
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    word2vec = Glove 300d
    Text processing: Regexp Tokenizer
 - best round: 1
 - metric (AUC): 0.563912623796
 - LB: 0.57123

 R1.2:
- params:
    VOCAB_SIZE = 4000
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300  # use with pretrained word2vec
    KERAS_LEARNING_RATE = 0.01
    KERAS_N_ROUNDS = 20
    KERAS_BATCH_SIZE = 64
    KERAS_DROPOUT_RATE = 0.2
    KERAS_REGULARIZER = 0.04
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 4096
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    word2vec = Glove 300d
    Text processing: Regexp Tokenizer
 - best round: 4
 - metric (AUC): 0.577089000631
 - LB: 

 R1.3:
- params:
    VOCAB_SIZE = 4000
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300  # use with pretrained word2vec
    KERAS_LEARNING_RATE = 0.01
    KERAS_N_ROUNDS = 20
    KERAS_BATCH_SIZE = 64
    KERAS_DROPOUT_RATE = 0.2
    KERAS_REGULARIZER = 0.04
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 4096
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    word2vec = Glove 300d
    Text processing: STEM
 - best round: 4
 - metric (AUC): 0.576524975482
 - LB: 

 
  R1.4:
- params:
    VOCAB_SIZE = 4000
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300  # use with pretrained word2vec
    KERAS_LEARNING_RATE = 0.003
    KERAS_N_ROUNDS = 20
    KERAS_BATCH_SIZE = 64
    KERAS_DROPOUT_RATE = 0.2
    KERAS_REGULARIZER = 0.04
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 4096
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    word2vec = Glove 300d
    Text processing: Regexp Tokenizer
 - best round: 4
 - metric (AUC): 0.579012278068
 - LB: 

 R1.5:
- params:
    VOCAB_SIZE = 4000
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300  # use with pretrained word2vec
    KERAS_LEARNING_RATE = 0.003
    KERAS_N_ROUNDS = 20
    KERAS_BATCH_SIZE = 64
    KERAS_DROPOUT_RATE = 0.2
    KERAS_REGULARIZER = 0.04
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 4096
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    word2vec = Glove 300d
    Text processing: Regexp Tokenizer
    model_choice2 = Dense
    embedding features = None
 - best round: 2
 - metric (AUC): 0.66067992685
 - LB: 

 R1.6:
- params:
    VOCAB_SIZE = 4000
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300  # use with pretrained word2vec
    KERAS_LEARNING_RATE = 0.003
    KERAS_N_ROUNDS = 20
    KERAS_BATCH_SIZE = 64
    KERAS_DROPOUT_RATE = 0.2
    KERAS_REGULARIZER = 0.04
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 4096
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    word2vec = Glove 300d
    Text processing: Regexp Tokenizer
    model_choice2 = None
    embedding features = All
 - best round: 1
 - metric (AUC): 3.0833e-05
 - LB: 

 R1.7:
- params:
    VOCAB_SIZE = 4000
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300  # use with pretrained word2vec
    KERAS_LEARNING_RATE = 0.003
    KERAS_N_ROUNDS = 20
    KERAS_LAYERS = 2
    KERAS_BATCH_SIZE = 64
    KERAS_DROPOUT_RATE = 0.2
    KERAS_REGULARIZER = 0.04
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 4096
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    word2vec = Glove 300d
    Text processing: Regexp Tokenizer
    model_choice2 = Dense
    embedding features = All
 - best round: 4
 - metric (AUC): :0.63016195666
 - LB: 

 R1.8:
- params:
    VOCAB_SIZE = 4000
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300  # use with pretrained word2vec
    KERAS_LEARNING_RATE = 0.003
    KERAS_N_ROUNDS = 20
    KERAS_LAYERS = 2
    KERAS_BATCH_SIZE = 64
    KERAS_DROPOUT_RATE = 0.2
    KERAS_REGULARIZER = 0.04
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 4096
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    word2vec = Glove 300d
    Text processing: Regexp Tokenizer
    model_choice2 = FastText
    embedding features = All
 - best round: 3
 - metric (AUC): 0.593587478534
 - LB: 

 R1.9:
- params:
    VOCAB_SIZE = 4000
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300  # use with pretrained word2vec
    KERAS_LEARNING_RATE = 0.003
    KERAS_N_ROUNDS = 20
    KERAS_LAYERS = 2
    KERAS_BATCH_SIZE = 64
    KERAS_DROPOUT_RATE = 0.2
    KERAS_REGULARIZER = 0.04
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 4096
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    word2vec = Glove 300d
    Text processing: Regexp Tokenizer + combine (project essay) => new
    model_choice2 = Dense
    embedding features = All
 - best round: 5
 - metric (AUC): :0.63016195666 => 0.672653617517 (Best)
 - LB: 0.66834 (Best)

 R1.10:
- params:
    VOCAB_SIZE = 4000
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300  # use with pretrained word2vec
    KERAS_LEARNING_RATE = 0.003
    KERAS_N_ROUNDS = 20
    KERAS_LAYERS = 2
    KERAS_BATCH_SIZE = 64
    KERAS_DROPOUT_RATE = 0.2
    KERAS_REGULARIZER = 0.04
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 4096
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    word2vec = Glove 300d
    Text processing: Regexp Tokenizer + combine (project essay)
    Numerical features: + count, std => new
    model_choice2 = Dense
    embedding features = All
 - best round: 3
 - metric (AUC): :0.672653617517 (Best) => 0.662279305747
 - LB: 0.66834 (Best) =>

 #----------------------------------------------------------------------------
 2. Keras + CNN
 R2.1:
- params:
    VOCAB_SIZE = 4000
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300  # use with pretrained word2vec
    KERAS_LEARNING_RATE = 0.003
    KERAS_N_ROUNDS = 20
    KERAS_LAYERS = 2
    KERAS_BATCH_SIZE = 64
    KERAS_DROPOUT_RATE = 0.2
    KERAS_REGULARIZER = 0.04
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 4096
    # ConvNet
    KERAS_FILTERS = 32  # => Best
    KERAS_POOL_SIZE = 1
    KERAS_KERNEL_SIZE = 1
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    word2vec = Glove 300d
    Text processing: Regexp Tokenizer + combine (project essay)
    Numerical features: + count, std
    model_choice2 = Dense
    embedding features = All
 - best round: 3
 - metric (AUC): :0.672653617517 (Best) => 0.666279349731
 - LB: 0.66834 (Best) => 0.65988


 #----------------------------------------------------------------------------
 3. Keras + CNN2
 R3.1:
- params:
    VOCAB_SIZE = 4000
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300  # use with pretrained word2vec
    KERAS_LEARNING_RATE = 0.003
    KERAS_N_ROUNDS = 20
    KERAS_LAYERS = 2
    KERAS_BATCH_SIZE = 64
    KERAS_DROPOUT_RATE = 0.2
    KERAS_REGULARIZER = 0.04
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 4096
    # ConvNet
    KERAS_FILTERS = 32  # => Best
    KERAS_POOL_SIZE = 1
    KERAS_KERNEL_SIZE = 1
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    word2vec = Glove 300d
    Text processing: Regexp Tokenizer + combine (project essay)
    Numerical features: + count, std
    model_choice2 = Dense
    embedding features = All
 - best round: 3
 - metric (AUC): :0.672653617517 (Best) => 0.666279349731
 - LB: 0.66834 (Best) => 0.65988

 