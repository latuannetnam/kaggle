1. Keras + Embeding layer + ConvNet
R1.1:
- params:
    VOCAB_SIZE = 50
    SEQUENCE_LENGTH = 50
    OUTPUT_DIM = 64
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
 - metric: 0.579236915698
 - best round: 2   

R1.2:
- params:
    SEQUENCE_LENGTH = 100 => new
    OUTPUT_DIM = 100 => new 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
 - pre-train word Embeding: glove.6B.100d.txt => new   
 - metric: 0.499857726818
 - best round: 2    
 - LB: 0.85310

R1.3:
- params:
    SEQUENCE_LENGTH = 300 => new
    OUTPUT_DIM = 300 => new 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
 - pre-train word Embeding: glove.6B.300d.txt => new   
 - metric: 0.492418925597
 - best round: 1    
 - LB: 0.85310

R1.3:
- params:
    SEQUENCE_LENGTH = 50 => new
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.49979255821
 - best round: 3    
 - LB: 

R1.4:
- params:
    SEQUENCE_LENGTH = 100 => new
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3 => new
    KERAS_DROPOUT_RATE = 0.5
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
 - convnet: => new   
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.378412799767
 - best round: 2   
 - LB: 0.65309

R1.5:
- params:
    SEQUENCE_LENGTH = 300 => new
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 128
    KERAS_KERNEL_SIZE = 3
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
 - convnet layer: KERAS_LAYERS
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.380222548503
 - best round: 2   
 - LB: 

R1.6:
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 128
    KERAS_KERNEL_SIZE = 1 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
 - convnet layer: KERAS_LAYERS
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.326236153982
 - best round: 3 
 - LB: 

R1.7:
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 300
    KERAS_KERNEL_SIZE = 1 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
 - convnet layer: KERAS_LAYERS
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.310678364612
 - best round: 3 
 - time/round: GPU: 7, CPU: 124
 - LB: 

R1.8:
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 300
    KERAS_KERNEL_SIZE = 1 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE => new
 - convnet layer: KERAS_LAYERS
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.322434487272
 - best round: 3 
 - time/round: GPU: 7, CPU: 124
 - LB: 


R1.9:
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 300
    KERAS_KERNEL_SIZE = 2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - convnet layer: KERAS_LAYERS
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.372936702329
 - best round: 2 
 - time/round: GPU: 7, CPU: 124
 - LB: 

R1.10:
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 300
    KERAS_KERNEL_SIZE = 2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - convnet layer: KERAS_LAYERS
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.373682625873
 - best round: 2 
 - time/round: GPU: 20, CPU: 124
 - LB: 

 R1.11:
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - convnet layer: KERAS_LAYERS
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.373682625873 => 0.275215755109
 - best round: 2 
 - time/round: GPU: 20, CPU: 124
 - LB: 

2. LSTM
R2.1:
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM => new
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.63139
 - best round: 2 
 - time/round: GPU: 20, CPU: 124
 - LB: 

3. LSTM + ConvNet
R3.1: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 128 => new
    KERAS_KERNEL_SIZE = 3 => new
    KERAS_POOL_SIZE = 2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM => new
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.631326545186
 - best round: 4 
 - time/round: GPU: 8, CPU: 124
 - LB:  

R3.2: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 5 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 128
    KERAS_KERNEL_SIZE = 3 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM 
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.406627525653
 - best round: 4
 - time/round: GPU: 8, CPU: 124
 - LB:   

R3.3: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 10 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 128
    KERAS_KERNEL_SIZE = 3 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM 
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.406627525653
 - best round: 4
 - time/round: GPU: 8, CPU: 124
 - LB:    

R3.4: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 256
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM 
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.406627525653
 - best round: 4
 - time/round: GPU: 8, CPU: 124
 - LB:     

R3.5: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 256
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM 
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.349785176251
 - best round: 28
 - time/round: GPU: 8, CPU: 124
 - LB:      

 R3.6: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 256
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM: CuDNNLSTM => new 
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.355708068128
 - best round: 15
 - time/round: GPU: 8, CPU: 124
 - LB: 0.61363     

R3.7: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 256
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM: CuDNNLSTM
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text: => new
 - metric: 0.366884521067
 - best round: 23
 - time/round: GPU: 8, CPU: 124
 - LB:       

 R3.8: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM => new
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM: CuDNNLSTM
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text
 - metric: 0.376698306529
 - best round: 14
 - time/round: GPU: 8, CPU: 124
 - LB:       

 
 R3.9: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM: CuDNNLSTM
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval => new
 - metric: 0.380847901258
 - best round: 14
 - time/round: GPU: 8, CPU: 124
 - LB:       

R3.10: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM: CuDNNLSTM
 - BatchNormalization
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.484863280185
 - best round: 30
 - time/round: GPU: 8, CPU: 124
 - LB:       

 R3.11: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM: CuDNNLSTM
   pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.382735130553 
 - best round: 11
 - time/round: GPU: 8, CPU: 124
 - LB:       

R3.12: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop => new
 - LSTM: CuDNNLSTM
   pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.400282736544
 - best round: 6
 - time/round: GPU: 8, CPU: 124
 - LB:       

 R3.13: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop 
 - LSTM: Bidirectional + LSTM => new
   pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.37331814828
 - best round: 10
 - time/round: GPU: 8, CPU: 124
 - LB:       

R3.14: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop 
 - LSTM: Bidirectional + LSTM => new
   pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.468209576071
 - best round: 7
 - time/round: GPU: 8, CPU: 124
 - LB:       

 
 R3.15: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop 
 - LSTM: cuDNNLSTM  => new
   pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.631248601671
 - best round: 6
 - time/round: GPU: 8, CPU: 124
 - LB:       

 R3.16: 
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 5 => new
    KERAS_DROPOUT_RATE = 0.2 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5 => new
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop 
 - LSTM: cuDNNLSTM  => new
   pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.631248601671 => 0.362492487626
 - best round: 13
 - time/round: GPU: 8, CPU: 124
 - LB:       

R3.17: 
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 5
    KERAS_DROPOUT_RATE = 0.4 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop 
 - LSTM: cuDNNLSTM  => new
   pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.362492487626 => 0.4445614907
 - best round: 9
 - time/round: GPU: 8, CPU: 124
 - LB:       

R3.18: 
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 8 => new
    KERAS_DROPOUT_RATE = 0.4
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop 
 - LSTM: cuDNNLSTM 
   pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.362492487626 => 0.421547583893
 - best round: 6
 - time/round: GPU: 8, CPU: 124
 - LB:        

#------------------------------------------------------------------------------------------------
4. FastText
 R4.1: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop 
 - GlobalAveragePooling1D => new
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.46230863295
 - best round: 745
 - time/round: GPU: 8, CPU: 124
 - LB:       

  R4.2: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop 
 - GlobalAveragePooling1D 
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.332864423359
 - best round: 90
 - time/round: GPU: 8, CPU: 124
 - LB: 0.53563      

 
   R4.3: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam => new
 - GlobalAveragePooling1D 
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.32892090108
 - best round: 71
 - time/round: GPU: 8, CPU: 124
 - LB: 

 R4.4: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 2 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
 - GlobalAveragePooling1D 
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.35287947968
 - best round: 11
 - time/round: GPU: 8, CPU: 124
 - LB: 

 R4.5: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
 - GlobalAveragePooling1D 
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.343765002532
 - best round: 5
 - time/round: GPU: 8, CPU: 124
 - LB: 

 R4.5: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2 => new
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
 - GlobalAveragePooling1D 
 - BatchNormalization => new
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.338085559951
 - best round: 11
 - time/round: GPU: 8, CPU: 124
 - LB: 0.55539 

R4.6: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3 
    KERAS_DROPOUT_RATE = 0.6 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
 - GlobalAveragePooling1D 
 - BatchNormalization
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.338085559951 => 0.35389522817
 - best round: 11
 - time/round: GPU: 8, CPU: 124
 - LB: 

R4.7: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 5 => new 
    KERAS_DROPOUT_RATE = 0.6
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
 - GlobalAveragePooling1D 
 - BatchNormalization
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.392434343285
 - best round: 15
 - time/round: GPU: 8, CPU: 124
 - LB: 

R4.8: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new 
    KERAS_DROPOUT_RATE = 0.6
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5 => new
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
 - GlobalAveragePooling1D 
 - BatchNormalization
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.392434343285 => 0.33216654125
 - best round: 26
 - time/round: GPU: 8, CPU: 124
 - LB: 0.53728

 R4.9: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.6
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.33216654125 => 0.531762149349
 - best round: 12
 - time/round: GPU: 8, CPU: 124
 - LB: 

R4.10: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.6
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 5 => new
    KERAS_POOL_SIZE = 5 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.33216654125 => 0.442304251599
 - best round: 56
 - time/round: GPU: 8, CPU: 124
 - LB: 

R4.11: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.6
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 1 => new
    KERAS_POOL_SIZE = 1 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.33216654125 => 0.427536922042
 - best round: 39
 - time/round: GPU: 8, CPU: 124
 - LB:  

R4.12: 
- params:
    SEQUENCE_LENGTH = 1000 => new
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.6
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 => new
    KERAS_POOL_SIZE = 2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.33216654125 => 0.417364623074
 - best round: 55
 - time/round: GPU: 8, CPU: 124

R4.13: 
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.2 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.33216654125 => 0.318572466339
 - best round: 21
 - time/round: GPU: 8, CPU: 124 
 - LB: 0.51538

 R4.14: 
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.2
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
 - pre-train word Embeding: None => new
 - sterm text: train + eval
 - metric: 0.318572466339 => 0.269996912505
 - best round: 2 
 - time/round: GPU: 8, CPU: 124 
 - LB: 0.51538 => 0.42727

 R4.15: 
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.269996912505 => 0.286037315692
 - best round: 2 
 - time/round: GPU: 8, CPU: 124 
 - LB: 0.42727 => 
 
  R4.16: 
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.2 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.269996912505 => 0.275742126985
 - best round: 9 
 - time/round: GPU: 8, CPU: 124 
 - LB: 0.42727 => 
 
  R4.17: 
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.2
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.269996912505 => 0.243458519095
 - best round: 5 
 - time/round: GPU: 8, CPU: 124 
 - LB: 0.42727 => 0.38881
 
  R4.18: 
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.242463040187 => 0.236067233308
 - best round: 2 
 - time/round: GPU: 8, CPU: 124 
 - LB:  0.38881 => 
 
 #------------------------------------------------------------------------------------------
 R5.1: CuDNNLSTM
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # CuDNNLSTM
    CuDNNLSTM => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.236067233308 => 0.631791848206
 - best round: 1 
 - time/round: GPU: 20, CPU: 124 
 - LB:  0.38881 => 

R5.2: LSTM
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    #LSTM
    LSTM => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.236067233308 => 0.631701666196
 - best round: 5 
 - time/round: GPU: 120, CPU: 
 - LB:  0.38881 =>  

 R5.3: LSTM
 - params:
    SEQUENCE_LENGTH = 1000 => new 
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    #LSTM
    CuDNNLSTM => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.236067233308 => 0.631900757216
 - best round: 
 - time/round: GPU: 120, CPU: 
 - LB:  0.38881 =>  

 R5.4: FastText
 - params:
    SEQUENCE_LENGTH = 1000 => new 
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.236067233308 => 0.235975071377
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 =>  

R5.5: FastText
 - params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.7 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.235975071377 => 0.236836764828
 - best round: 3
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 =>   

 R5.6: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.7
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 4 => new
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.235975071377 => 0.276280252832
 - best round: 5
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 =>   
 
 R5.7: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.7
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1 => new
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.235975071377 => 0.244614203051
 - best round: 8
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 0.41884  

 R5.8: FastText
 - params:
    SEQUENCE_LENGTH = 300 => new
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.7
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.235975071377 => 0.240969415309
 - best round: 3
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

 R5.9: FastText
 - params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 2 => new
    KERAS_DROPOUT_RATE = 0.7
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.235975071377 => 0.249451954926
 - best round: 3
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

 R5.10: FastText
 - params:
    SEQUENCE_LENGTH = 500 => new
    OUTPUT_DIM = 500 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.7
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.235975071377 => 0.24110223671
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

 R5.11: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.7
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.235975071377 => 0.240053286987
 - best round: 4
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

 R5.12: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.7
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - sterm text: None => new
 - metric: 0.235975071377 => 0.249260364319
 - best round: 5
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

 R5.13: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.2 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - sterm text: train + eval => new
 - metric: 0.235975071377 => 0.243304741166
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

  R5.14: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.2
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 3 => new
 - pre-train word Embeding: None
 - preprocess text: sterm text: train + eval
 - metric: 0.235975071377 => 0.254781091055
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

 R5.15: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.2
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: WordNetLemmatizer: train + eval => new
 - metric: 0.235975071377 => 0.265565212127
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 0.45244 

 R5.16: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: sterm text: train + eval => new
 - metric: 0.235975071377 => 0.23722454044
 - best round: 6
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

 R5.17: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: WordNetLemmatizer: train + eval => new
 - metric: 0.235975071377 => 0.261052718993
 - best round: 6
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

  R5.18: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: None => new
 - metric: 0.235975071377 => 0.239110268051
 - best round: 6
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

 R5.19: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180

   R5.20: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.232865729876
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 0.39073 (overfit)

   R5.21: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.8 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.265580962761
 - best round: 10
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

 R5.22: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: LancasterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.245088436326
 - best round: 6
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

 R5.23: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: Glove => new
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.51597236418
 - best round: 16
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

 R5.24: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1 => new
 - pre-train word Embeding: Glove => new
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.516139192954
 - best round: 16
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

 R5.25: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1 => new
 - pre-train word Embeding: Gensim word2vec => new
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.623842889673
 - best round: 15
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

 R5.26: ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    GlobalMaxPooling1D => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None => new
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.32440519753
 - best round: 4
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

 R5.27: ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    GlobalMaxPooling1D => new
    Dense(OUTPUT_DIM) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.23447303719 => 0.631977681668
 - best round: 4
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

R5.28: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    Dense(OUTPUT_DIM) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.2728556534
 - best round: 4
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.29: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization  => new
    Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.236972940833
 - best round: 9
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.30: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization  => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.246415000664
 - best round: 4
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.31: ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    GlobalMaxPooling1D => new
    Dense(OUTPUT_DIM) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.23447303719 => 0.334276219767
 - best round: 4
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

  R5.32: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.24280435547
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.33: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.7 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.280898526874
 - best round: 9
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.34: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - tokenize(lower=False) => new
 - metric: 0.23447303719 => 0.237423548751
 - best round: 5
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.35: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 3 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - tokenize(lower=False)
 - metric: 0.23447303719 => 0.268282536467
 - best round: 
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.36: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 4 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.280480397369
 - best round: 
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.37: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.245227689428
 - best round: 9
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.38: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use text_to_matrix: => new
 - metric: 0.23447303719 => 0.631673088356
 - best round: 0
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.39: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 5 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use text_to_matrix: => new
 - metric: 0.23447303719 => 0.631672369569
 - best round: 0
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

R5.40: ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 5 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
     # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    GlobalMaxPooling1D => new
    Dense(OUTPUT_DIM) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use text_to_matrix: => new
 - metric: 0.23447303719 => 0.631664193078
 - best round: 9
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

R5.40: CuDNNLSTM
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 32 => new
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
     # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    GlobalMaxPooling1D => new
    Dense(OUTPUT_DIM) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use text_to_matrix: => new
 - metric: 0.23447303719 => 0.631690374738
 - best round: 9
 - time/round: GPU: 600, CPU: 
 - LB:  0.38180 =>  

  R5.41: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use spacy preprocessing: => new
 - metric: 0.23447303719 => 0.631842183385
 - best round: 0
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.42: ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 1 => new
    KERAS_POOL_SIZE = 1 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    KERAS_EMBEDDING = False => new
    optimizer = Adam
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use spacy preprocessing: => new
 - metric: 0.23447303719 => 0.465714377268
 - best round: 27
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.43: ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 5 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 1 => new
    KERAS_POOL_SIZE = 1 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    KERAS_EMBEDDING = False => new
    optimizer = Adam
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use spacy preprocessing: => new
 - metric: 0.23447303719 => 0.476869234421
 - best round: 44
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.44: ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 1 => new
    KERAS_POOL_SIZE = 1 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    KERAS_EMBEDDING = False => new
    optimizer = Adam
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use text2matrix: => new
 - metric: 0.23447303719 => 0.303608571789
 - best round: 4
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

R5.45: ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 1 => new
    KERAS_POOL_SIZE = 1 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    KERAS_EMBEDDING = False => new
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use SEQUENCE: => new
 - metric: 0.23447303719 => 0.303608571789 => 0.626665557795
 - best round: 4
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 
 
 R5.46: FastText
 - params:
    SEQUENCE_LENGTH = 100 => new
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.255184522232
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

R5.46: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH) => new
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.257638220676
 - best round: 16
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

 R5.47: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH) => new
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.242975967775
 - best round: 16
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

  R5.48: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH) => new
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.250000824769
 - best round: 5
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

 R5.49: ConvNet
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH) => new
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.293371948633
 - best round: 9
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

   R5.50: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 1000 => new 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH) => new
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.251162870623
 - best round: 14
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 0.40492
 
  R5.51: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 1000 => new 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH) => new
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.251162870623 => 0.246708579871
 - best round: 13
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 0.40158
 
  R5.52: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 1000 => new 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization => new
    removed => Dense(OUTPUT_DIM) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH) => new
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 =>  0.237877243736
 - best round: 33
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

R5.53: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 500 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 32 => new
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 =>  0.236488598603
 - best round: 31
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>  0.38154

 R5.54: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 500 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64 => new
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 =>  0.233992549537
 - best round: 49
 - time/round: GPU: 40, CPU: 
 - LB: 0.38154 => 0.37334

 R5.55: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 500 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.233992549537 => 0.241582637864
 - best round: 3
 - time/round: GPU: 40, CPU: 
 - LB: 0.37334 =>
 
 R5.56: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 1000 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.233992549537 => 0.243585992624
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB: 0.37334 =>

 R5.57: FastText
 - params:
    SEQUENCE_LENGTH = 1000 => new
    OUTPUT_DIM = 1000 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.233992549537 => 0.237676707607
 - best round: 45
 - time/round: GPU: 40, CPU: 
 - LB: 0.37334 =>

  R5.58: FastText
 - params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 1000 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: WordNetLemmatizer: train + eval => new
 - metric: 0.233992549537 => 0.254304450732
 - best round: 36
 - time/round: GPU: 40, CPU: 
 - LB: 0.37334 =>

 R5.59: FastText
 - params:
    SEQUENCE_LENGTH = 500 => new
    OUTPUT_DIM = 500 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: WordNetLemmatizer: train + eval => new
 - metric: 0.233992549537 => 0.249650611271 
 - best round: 36
 - time/round: GPU: 40, CPU: 
 - LB: 0.37334 =>
 
 #------------------------------------------------------------------------
 R6.1: Kfold + FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5 => new
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.233992549537 => 0.243456385118
 - time/round: GPU: 501, CPU: 
 - total time: 3010.8007311820984 
 - LB: 0.37334 => 0.38130

R6.2: Kfold + FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5 => new
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.233992549537 => 0.225453577797
 - time/round: GPU: 3539, CPU: 
 - total time: 15796
 - LB: 0.37334 => 0.35343

 #-----------------------------------------------------------------------
 R7.1: FastText + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Concatenate
    model3 = model_fasttext_combined + model_input2_dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.233992549537 => 0.234144493298
 - time/round: GPU: 40, CPU: 
 - total time: 587
 - LB: 0.37334 => 0.38605

 R7.2: FastText + ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Concatenate
    model3 = model_fasttext_ + model_cnn => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.233992549537 => 0.273421258495
 - time/round: GPU: 40, CPU: 
 - total time: 348
 - LB: 0.37334 => 

 R7.3: ConvNet + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # Concatenate
    model3 = model_cnn + model_input2_dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.233992549537 => 0.267486659762
 - time/round: GPU: 40, CPU: 
 - total time: 313
 - LB: 0.37334 => 

 R7.4: FastText + CuDNNLSTM
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Concatenate
    model3 = FastText + CuDNNLSTM = new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.233992549537 => 0.297750015546
 - time/round: GPU: 40, CPU: 
 - total time: 348
 - LB: 0.37334 => 

 R7.5: CuDNNLSTM + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Concatenate
    model3 = CuDNNLSTM + model_input2_dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.233992549537 => 0.315910797928
 - time/round: GPU: 40, CPU: 
 - total time: 728
 - LB: 0.37334 => 

R7.6: FastText + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Merge => Dot
    model3 = Dot(model_fasttext + model_input2_dense, normalize=True) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.234144493298 => 0.375530476437
 - time/round: GPU: 40, CPU: 
 - total time: 587
 - LB: 0.38605 =>

 R7.7: FastText + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Merge => Average
    model3 = Average(model_fasttext + model_input2_dense) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.234144493298 => 0.245775830058
 - best round: 63
 - time/round: GPU: 40, CPU: 
 - total time: 705
 - LB: 0.38605 =>

 R7.8: FastText + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # model_input2_dense
    feature_input + dense + dropout => new
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + model_input2_dense) => new
    out_model = GlobalAveragePooling1D + dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df
 - metric: 0.234144493298 => 0.224523078692
 - best round: 43
 - time/round: GPU: 40, CPU: 
 - total time: 1100
 - LB: 0.38605 => 0.38693
 #------------------------------------------------------------------------------------
  R8.1: Kfold + FastText + model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Concatenate
    model3 = model_fasttext_combined + model_input2_dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5 => new
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.243456385118 => 0.235364179136 
 - time/round: GPU: 860, CPU: 
 - total time:  3542
 - LB: 0.38130 => 0.36900

 R8.2: Kfold + FastText + model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Concatenate
    model3 = model_fasttext + model_input2_dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5 => new
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.225453577797 => 0.22625076306
 - time/round: GPU: 4007, CPU: 
 - total time:  16347
 - LB: 0.35343 => 0.34809

 R8.3: Kfold + FastText + model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Merge = Average
    model3 = Average(model_fasttext_combined + model_input2_dense) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5 => new
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.235364179136 => 0.244762213326
 - time/round: GPU: 941, CPU: 
 - total time:  4510
 - LB: 0.36900 => 0.38505

R8.4: Kfold + FastText + model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # model_input2_dense
    feature_input + dense + dropout => new
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + model_input2_dense) => new
    out_model = GlobalAveragePooling1D + dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.235364179136 => 0.235244993635
 - time/round: GPU: 1258, CPU: 
 - total time:  5721
 - LB: 0.36900 => 0.38213

R8.5: Kfold + FastText + model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # model_input2_dense
    feature_input + dense + dropout => new
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + model_input2_dense) => new
    out_model = GlobalAveragePooling1D + dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.244762213326 => 0.228545237713
 - time/round: GPU:4907 , CPU: 
 - total time:  23206
 - LB: 0.34809 => 0.36316 