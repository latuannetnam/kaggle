1. Keras + Embeding layer + ConvNet
R1.1:
- params:
    VOCAB_SIZE = 50
    SEQUENCE_LENGTH = 50
    OUTPUT_DIM = 64
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
 - metric: 0.579236915698
 - best round: 2   

R1.2:
- params:
    SEQUENCE_LENGTH = 100 => new
    OUTPUT_DIM = 100 => new 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
 - pre-train word Embeding: glove.6B.100d.txt => new   
 - metric: 0.499857726818
 - best round: 2    
 - LB: 0.85310

R1.3:
- params:
    SEQUENCE_LENGTH = 300 => new
    OUTPUT_DIM = 300 => new 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
 - pre-train word Embeding: glove.6B.300d.txt => new   
 - metric: 0.492418925597
 - best round: 1    
 - LB: 0.85310

R1.3:
- params:
    SEQUENCE_LENGTH = 50 => new
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.49979255821
 - best round: 3    
 - LB: 

R1.4:
- params:
    SEQUENCE_LENGTH = 100 => new
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3 => new
    KERAS_DROPOUT_RATE = 0.5
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    VERBOSE = True
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
 - convnet: => new   
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.378412799767
 - best round: 2   
 - LB: 0.65309

R1.5:
- params:
    SEQUENCE_LENGTH = 300 => new
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 128
    KERAS_KERNEL_SIZE = 3
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
 - convnet layer: KERAS_LAYERS
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.380222548503
 - best round: 2   
 - LB: 

R1.6:
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 128
    KERAS_KERNEL_SIZE = 1 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
 - convnet layer: KERAS_LAYERS
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.326236153982
 - best round: 3 
 - LB: 

R1.7:
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 300
    KERAS_KERNEL_SIZE = 1 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
 - convnet layer: KERAS_LAYERS
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.310678364612
 - best round: 3 
 - time/round: GPU: 7, CPU: 124
 - LB: 

R1.8:
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 300
    KERAS_KERNEL_SIZE = 1 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE => new
 - convnet layer: KERAS_LAYERS
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.322434487272
 - best round: 3 
 - time/round: GPU: 7, CPU: 124
 - LB: 


R1.9:
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 300
    KERAS_KERNEL_SIZE = 2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - convnet layer: KERAS_LAYERS
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.372936702329
 - best round: 2 
 - time/round: GPU: 7, CPU: 124
 - LB: 

R1.10:
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 300
    KERAS_KERNEL_SIZE = 2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - convnet layer: KERAS_LAYERS
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.373682625873
 - best round: 2 
 - time/round: GPU: 20, CPU: 124
 - LB: 

 R1.11:
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - convnet layer: KERAS_LAYERS
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.373682625873 => 0.275215755109
 - best round: 2 
 - time/round: GPU: 20, CPU: 124
 - LB: 

 R1.12:
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 128 => new
    KERAS_KERNEL_SIZE = 2 => 
    KERAS_POOL_SIZE = 2 => new
    model = model_cnn2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
  - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.275215755109 => 0.262290551178
 - best round: 3 
 - time/round: GPU: 40, CPU: 124
 - LB: 

 R1.13:
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 64 => new
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    model = model_cnn2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    n-gram = 1
  - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.262290551178 => 0.27112 => 0.264601086815
 - best round: 3 
 - time/round: GPU: 40, CPU: 124
 - LB: 

 R1.14:
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0.01 => new
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 64 => new
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    model = model_cnn2 => new
    Dense(regularizers.l2(KERAS_REGULARIZER)) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    n-gram = 1
  - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.262290551178 => 0.332753630593 => 0.387640166697
 - best round: 6
 - time/round: GPU: 40, CPU: 124
 - LB: 

 R1.15:
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0.01 => new
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 64 => new
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    model = model_cnn2 => new
    remove => Dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    n-gram = 1
  - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.262290551178 => 0.247402917331
 - best round: 2
 - time/round: GPU: 40, CPU: 124
 - LB: 

 R1.16:
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0.01 => new
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 64
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    model = model_cnn2 => new
    remove => Dense
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    n-gram = 2 => new
  - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.247402917331 => 0.33099449655
 - best round: 1
 - time/round: GPU: 40, CPU: 124
 - LB: 

 R1.17:
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0.01 => new
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 32 => new
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    model = model_cnn2 => new
    remove => Dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    n-gram = 1
  - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.247402917331 => 0.248782909927
 - best round: 2
 - time/round: GPU: 40, CPU: 124
 - LB: 

R1.18:
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0.01 => new
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 32 => new
    KERAS_KERNEL_SIZE = [3,4,5] => new
    KERAS_POOL_SIZE = 3 => new
    model = model_cnn2 => new
    remove => Dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    n-gram = 1
  - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.247402917331 => 0.247807594924 => 
 - best round: 2
 - time/round: GPU: 40, CPU: 124
 - LB: 

 R1.19:
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0.01 => new
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 32 => new
    KERAS_KERNEL_SIZE = [3,4,5] => new
    KERAS_POOL_SIZE = 3 => new
    model = model_cnn2 => new
    remove => Dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    n-gram = 2
  - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.247402917331 => 0.245861293164
 - best round: 1
 - time/round: GPU: 489, CPU: 124
 - LB: 

 R1.20:
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0.01 => new
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 32 
    KERAS_KERNEL_SIZE = [3,4,5]
    KERAS_POOL_SIZE = 5 => new
    model = model_cnn2 => new
    remove => Dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    n-gram = 2
  - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.245861293164 => 0.251998870986
 - best round: 1
 - time/round: GPU: 489, CPU: 124
 - LB: 

 R1.21:
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0.01 => new
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 32 => new
    KERAS_KERNEL_SIZE = [3,4,5] => new
    KERAS_POOL_SIZE = 3 => new
    model = model_cnn2 => new
    remove => Dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    n-gram = 1
  - pre-train word Embeding: Glove 300d => new
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.247402917331 => 0.247039908439
 - best round:4 
 - time/round: GPU: 40, CPU: 124
 - LB: 
#--------------------------------------------------------------
2. LSTM
R2.1:
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM => new
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.63139
 - best round: 2 
 - time/round: GPU: 20, CPU: 124
 - LB: 

3. LSTM + ConvNet
R3.1: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 128 => new
    KERAS_KERNEL_SIZE = 3 => new
    KERAS_POOL_SIZE = 2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM => new
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.631326545186
 - best round: 4 
 - time/round: GPU: 8, CPU: 124
 - LB:  

R3.2: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 5 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 128
    KERAS_KERNEL_SIZE = 3 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM 
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.406627525653
 - best round: 4
 - time/round: GPU: 8, CPU: 124
 - LB:   

R3.3: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 10 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 128
    KERAS_KERNEL_SIZE = 3 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM 
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.406627525653
 - best round: 4
 - time/round: GPU: 8, CPU: 124
 - LB:    

R3.4: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 256
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM 
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.406627525653
 - best round: 4
 - time/round: GPU: 8, CPU: 124
 - LB:     

R3.5: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 256
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM 
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.349785176251
 - best round: 28
 - time/round: GPU: 8, CPU: 124
 - LB:      

 R3.6: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 256
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM: CuDNNLSTM => new 
 - pre-train word Embeding: glove.6B.300d.txt
 - metric: 0.355708068128
 - best round: 15
 - time/round: GPU: 8, CPU: 124
 - LB: 0.61363     

R3.7: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 256
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM: CuDNNLSTM
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text: => new
 - metric: 0.366884521067
 - best round: 23
 - time/round: GPU: 8, CPU: 124
 - LB:       

 R3.8: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM => new
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM: CuDNNLSTM
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text
 - metric: 0.376698306529
 - best round: 14
 - time/round: GPU: 8, CPU: 124
 - LB:       

 
 R3.9: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM: CuDNNLSTM
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval => new
 - metric: 0.380847901258
 - best round: 14
 - time/round: GPU: 8, CPU: 124
 - LB:       

R3.10: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM: CuDNNLSTM
 - BatchNormalization
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.484863280185
 - best round: 30
 - time/round: GPU: 8, CPU: 124
 - LB:       

 R3.11: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
 - LSTM: CuDNNLSTM
   pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.382735130553 
 - best round: 11
 - time/round: GPU: 8, CPU: 124
 - LB:       

R3.12: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop => new
 - LSTM: CuDNNLSTM
   pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.400282736544
 - best round: 6
 - time/round: GPU: 8, CPU: 124
 - LB:       

 R3.13: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop 
 - LSTM: Bidirectional + LSTM => new
   pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.37331814828
 - best round: 10
 - time/round: GPU: 8, CPU: 124
 - LB:       

R3.14: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 6
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop 
 - LSTM: Bidirectional + LSTM => new
   pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.468209576071
 - best round: 7
 - time/round: GPU: 8, CPU: 124
 - LB:       

 
 R3.15: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop 
 - LSTM: cuDNNLSTM  => new
   pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.631248601671
 - best round: 6
 - time/round: GPU: 8, CPU: 124
 - LB:       

 R3.16: 
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 5 => new
    KERAS_DROPOUT_RATE = 0.2 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5 => new
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop 
 - LSTM: cuDNNLSTM  => new
   pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.631248601671 => 0.362492487626
 - best round: 13
 - time/round: GPU: 8, CPU: 124
 - LB:       

R3.17: 
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 5
    KERAS_DROPOUT_RATE = 0.4 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop 
 - LSTM: cuDNNLSTM  => new
   pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.362492487626 => 0.4445614907
 - best round: 9
 - time/round: GPU: 8, CPU: 124
 - LB:       

R3.18: 
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 8 => new
    KERAS_DROPOUT_RATE = 0.4
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop 
 - LSTM: cuDNNLSTM 
   pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.362492487626 => 0.421547583893
 - best round: 6
 - time/round: GPU: 8, CPU: 124
 - LB:        

#------------------------------------------------------------------------------------------------
4. FastText
 R4.1: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop 
 - GlobalAveragePooling1D => new
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.46230863295
 - best round: 745
 - time/round: GPU: 8, CPU: 124
 - LB:       

  R4.2: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = RMSprop 
 - GlobalAveragePooling1D 
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.332864423359
 - best round: 90
 - time/round: GPU: 8, CPU: 124
 - LB: 0.53563      

 
   R4.3: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam => new
 - GlobalAveragePooling1D 
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.32892090108
 - best round: 71
 - time/round: GPU: 8, CPU: 124
 - LB: 

 R4.4: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 2 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
 - GlobalAveragePooling1D 
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.35287947968
 - best round: 11
 - time/round: GPU: 8, CPU: 124
 - LB: 

 R4.5: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
 - GlobalAveragePooling1D 
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.343765002532
 - best round: 5
 - time/round: GPU: 8, CPU: 124
 - LB: 

 R4.5: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2 => new
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
 - GlobalAveragePooling1D 
 - BatchNormalization => new
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.338085559951
 - best round: 11
 - time/round: GPU: 8, CPU: 124
 - LB: 0.55539 

R4.6: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3 
    KERAS_DROPOUT_RATE = 0.6 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
 - GlobalAveragePooling1D 
 - BatchNormalization
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.338085559951 => 0.35389522817
 - best round: 11
 - time/round: GPU: 8, CPU: 124
 - LB: 

R4.7: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 5 => new 
    KERAS_DROPOUT_RATE = 0.6
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 2
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
 - GlobalAveragePooling1D 
 - BatchNormalization
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.392434343285
 - best round: 15
 - time/round: GPU: 8, CPU: 124
 - LB: 

R4.8: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new 
    KERAS_DROPOUT_RATE = 0.6
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5 => new
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
 - GlobalAveragePooling1D 
 - BatchNormalization
 -  pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.392434343285 => 0.33216654125
 - best round: 26
 - time/round: GPU: 8, CPU: 124
 - LB: 0.53728

 R4.9: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.6
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.33216654125 => 0.531762149349
 - best round: 12
 - time/round: GPU: 8, CPU: 124
 - LB: 

R4.10: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.6
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 5 => new
    KERAS_POOL_SIZE = 5 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.33216654125 => 0.442304251599
 - best round: 56
 - time/round: GPU: 8, CPU: 124
 - LB: 

R4.11: 
- params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.6
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 1 => new
    KERAS_POOL_SIZE = 1 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.33216654125 => 0.427536922042
 - best round: 39
 - time/round: GPU: 8, CPU: 124
 - LB:  

R4.12: 
- params:
    SEQUENCE_LENGTH = 1000 => new
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.6
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2 => new
    KERAS_POOL_SIZE = 2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.33216654125 => 0.417364623074
 - best round: 55
 - time/round: GPU: 8, CPU: 124

R4.13: 
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.2 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
 - pre-train word Embeding: glove.6B.300d.txt
 - sterm text: train + eval
 - metric: 0.33216654125 => 0.318572466339
 - best round: 21
 - time/round: GPU: 8, CPU: 124 
 - LB: 0.51538

 R4.14: 
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.2
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
 - pre-train word Embeding: None => new
 - sterm text: train + eval
 - metric: 0.318572466339 => 0.269996912505
 - best round: 2 
 - time/round: GPU: 8, CPU: 124 
 - LB: 0.51538 => 0.42727

 R4.15: 
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.269996912505 => 0.286037315692
 - best round: 2 
 - time/round: GPU: 8, CPU: 124 
 - LB: 0.42727 => 
 
  R4.16: 
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.2 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.269996912505 => 0.275742126985
 - best round: 9 
 - time/round: GPU: 8, CPU: 124 
 - LB: 0.42727 => 
 
  R4.17: 
- params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.2
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.269996912505 => 0.243458519095
 - best round: 5 
 - time/round: GPU: 8, CPU: 124 
 - LB: 0.42727 => 0.38881
 
R4.18: 
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.242463040187 => 0.236067233308
 - best round: 2 
 - time/round: GPU: 8, CPU: 124 
 - LB:  0.38881 => 

 R4.19: 
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # FastText
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1 => new
 - pre-train word Embeding: Gensim w2v
 - sterm text: train + eval
 - metric: 0.236067233308 => 0.243617636997
 - best round: 39
 - time/round: GPU: 8, CPU: 124 
 - LB:  0.38881 => 
 
 #------------------------------------------------------------------------------------------
 R5.1: CuDNNLSTM
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # CuDNNLSTM
    CuDNNLSTM => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.236067233308 => 0.631791848206
 - best round: 1 
 - time/round: GPU: 20, CPU: 124 
 - LB:  0.38881 => 

R5.2: LSTM
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    #LSTM
    LSTM => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.236067233308 => 0.631701666196
 - best round: 5 
 - time/round: GPU: 120, CPU: 
 - LB:  0.38881 =>  

 R5.3: LSTM
 - params:
    SEQUENCE_LENGTH = 1000 => new 
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    #LSTM
    CuDNNLSTM => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.236067233308 => 0.631900757216
 - best round: 
 - time/round: GPU: 120, CPU: 
 - LB:  0.38881 =>  

 R5.4: FastText
 - params:
    SEQUENCE_LENGTH = 1000 => new 
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.236067233308 => 0.235975071377
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 =>  

R5.5: FastText
 - params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.7 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.235975071377 => 0.236836764828
 - best round: 3
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 =>   

 R5.6: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.7
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 4 => new
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.235975071377 => 0.276280252832
 - best round: 5
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 =>   
 
 R5.7: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.7
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1 => new
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.235975071377 => 0.244614203051
 - best round: 8
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 0.41884  

 R5.8: FastText
 - params:
    SEQUENCE_LENGTH = 300 => new
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.7
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.235975071377 => 0.240969415309
 - best round: 3
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

 R5.9: FastText
 - params:
    SEQUENCE_LENGTH = 300
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 2 => new
    KERAS_DROPOUT_RATE = 0.7
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.235975071377 => 0.249451954926
 - best round: 3
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

 R5.10: FastText
 - params:
    SEQUENCE_LENGTH = 500 => new
    OUTPUT_DIM = 500 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.7
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.235975071377 => 0.24110223671
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

 R5.11: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.7
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - sterm text: train + eval
 - metric: 0.235975071377 => 0.240053286987
 - best round: 4
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

 R5.12: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.7
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - sterm text: None => new
 - metric: 0.235975071377 => 0.249260364319
 - best round: 5
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

 R5.13: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.2 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - sterm text: train + eval => new
 - metric: 0.235975071377 => 0.243304741166
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

  R5.14: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.2
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 3 => new
 - pre-train word Embeding: None
 - preprocess text: sterm text: train + eval
 - metric: 0.235975071377 => 0.254781091055
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

 R5.15: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.2
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: WordNetLemmatizer: train + eval => new
 - metric: 0.235975071377 => 0.265565212127
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 0.45244 

 R5.16: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: sterm text: train + eval => new
 - metric: 0.235975071377 => 0.23722454044
 - best round: 6
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

 R5.17: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: WordNetLemmatizer: train + eval => new
 - metric: 0.235975071377 => 0.261052718993
 - best round: 6
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

  R5.18: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: None => new
 - metric: 0.235975071377 => 0.239110268051
 - best round: 6
 - time/round: GPU: 40, CPU: 
 - LB:  0.38881 => 

 R5.19: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180

   R5.20: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.232865729876
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 0.39073 (overfit)

   R5.21: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.8 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.265580962761
 - best round: 10
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

 R5.22: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: LancasterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.245088436326
 - best round: 6
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

 R5.23: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: Glove => new
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.51597236418
 - best round: 16
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

 R5.24: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1 => new
 - pre-train word Embeding: Glove => new
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.516139192954
 - best round: 16
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

 R5.25: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1 => new
 - pre-train word Embeding: Gensim word2vec => new
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.623842889673
 - best round: 15
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

 R5.26: ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    GlobalMaxPooling1D => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None => new
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.32440519753
 - best round: 4
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

 R5.27: ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    GlobalMaxPooling1D => new
    Dense(OUTPUT_DIM) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.23447303719 => 0.631977681668
 - best round: 4
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

R5.28: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    Dense(OUTPUT_DIM) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.2728556534
 - best round: 4
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.29: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization  => new
    Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.236972940833
 - best round: 9
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.30: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization  => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.246415000664
 - best round: 4
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.31: ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    GlobalMaxPooling1D => new
    Dense(OUTPUT_DIM) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.23447303719 => 0.334276219767
 - best round: 4
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

  R5.32: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.24280435547
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.33: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.7 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.280898526874
 - best round: 9
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.34: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5 => new
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - tokenize(lower=False) => new
 - metric: 0.23447303719 => 0.237423548751
 - best round: 5
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.35: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 3 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - tokenize(lower=False)
 - metric: 0.23447303719 => 0.268282536467
 - best round: 
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.36: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 4 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.280480397369
 - best round: 
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.37: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.245227689428
 - best round: 9
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.38: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use text_to_matrix: => new
 - metric: 0.23447303719 => 0.631673088356
 - best round: 0
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.39: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 5 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use text_to_matrix: => new
 - metric: 0.23447303719 => 0.631672369569
 - best round: 0
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

R5.40: ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 5 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
     # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    GlobalMaxPooling1D => new
    Dense(OUTPUT_DIM) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use text_to_matrix: => new
 - metric: 0.23447303719 => 0.631664193078
 - best round: 9
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

R5.40: CuDNNLSTM
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 32 => new
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
     # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    GlobalMaxPooling1D => new
    Dense(OUTPUT_DIM) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use text_to_matrix: => new
 - metric: 0.23447303719 => 0.631690374738
 - best round: 9
 - time/round: GPU: 600, CPU: 
 - LB:  0.38180 =>  

  R5.41: FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    Dense(OUTPUT_DIM)
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use spacy preprocessing: => new
 - metric: 0.23447303719 => 0.631842183385
 - best round: 0
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.42: ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 1 => new
    KERAS_POOL_SIZE = 1 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    KERAS_EMBEDDING = False => new
    optimizer = Adam
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use spacy preprocessing: => new
 - metric: 0.23447303719 => 0.465714377268
 - best round: 27
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.43: ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 5 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 1 => new
    KERAS_POOL_SIZE = 1 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    KERAS_EMBEDDING = False => new
    optimizer = Adam
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use spacy preprocessing: => new
 - metric: 0.23447303719 => 0.476869234421
 - best round: 44
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

 R5.44: ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 1 => new
    KERAS_POOL_SIZE = 1 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    KERAS_EMBEDDING = False => new
    optimizer = Adam
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use text2matrix: => new
 - metric: 0.23447303719 => 0.303608571789
 - best round: 4
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

R5.45: ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 1 => new
    KERAS_POOL_SIZE = 1 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    KERAS_EMBEDDING = False => new
    optimizer = Adam
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - use SEQUENCE: => new
 - metric: 0.23447303719 => 0.303608571789 => 0.626665557795
 - best round: 4
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 
 
 R5.46: FastText
 - params:
    SEQUENCE_LENGTH = 100 => new
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.255184522232
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>

R5.46: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH) => new
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.257638220676
 - best round: 16
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

 R5.47: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH) => new
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.242975967775
 - best round: 16
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

  R5.48: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH) => new
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.250000824769
 - best round: 5
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

 R5.49: ConvNet
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 100 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH) => new
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.293371948633
 - best round: 9
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

   R5.50: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 1000 => new 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    BatchNormalization
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH) => new
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.251162870623
 - best round: 14
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 0.40492
 
  R5.51: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 1000 => new 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH) => new
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 => 0.251162870623 => 0.246708579871
 - best round: 13
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 0.40158
 
  R5.52: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 1000 => new 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization => new
    removed => Dense(OUTPUT_DIM) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH) => new
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 =>  0.237877243736
 - best round: 33
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 => 

R5.53: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 500 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 32 => new
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 =>  0.236488598603
 - best round: 31
 - time/round: GPU: 40, CPU: 
 - LB:  0.38180 =>  0.38154

 R5.54: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 500 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64 => new
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval => new
 - metric: 0.23447303719 =>  0.233992549537
 - best round: 49
 - time/round: GPU: 40, CPU: 
 - LB: 0.38154 => 0.37334

 R5.55: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 500 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.233992549537 => 0.241582637864
 - best round: 3
 - time/round: GPU: 40, CPU: 
 - LB: 0.37334 =>
 
 R5.56: FastText
 - params:
    SEQUENCE_LENGTH = 100
    OUTPUT_DIM = 1000 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.233992549537 => 0.243585992624
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - LB: 0.37334 =>

 R5.57: FastText
 - params:
    SEQUENCE_LENGTH = 1000 => new
    OUTPUT_DIM = 1000 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.233992549537 => 0.237676707607
 - best round: 45
 - time/round: GPU: 40, CPU: 
 - LB: 0.37334 =>

  R5.58: FastText
 - params:
    SEQUENCE_LENGTH = 1000
    OUTPUT_DIM = 1000 
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: WordNetLemmatizer: train + eval => new
 - metric: 0.233992549537 => 0.254304450732
 - best round: 36
 - time/round: GPU: 40, CPU: 
 - LB: 0.37334 =>

 R5.59: FastText
 - params:
    SEQUENCE_LENGTH = 500 => new
    OUTPUT_DIM = 500 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: WordNetLemmatizer: train + eval => new
 - metric: 0.233992549537 => 0.249650611271 
 - best round: 36
 - time/round: GPU: 40, CPU: 
 - LB: 0.37334 =>
 
 #------------------------------------------------------------------------
 R6.1: Kfold + FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5 => new
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.233992549537 => 0.243456385118
 - time/round: GPU: 501, CPU: 
 - total time: 3010.8007311820984 
 - LB: 0.37334 => 0.38130

R6.2: Kfold + FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5 => new
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.233992549537 => 0.225453577797
 - time/round: GPU: 3539, CPU: 
 - total time: 15796
 - LB: 0.37334 => 0.35343

R6.3: Kfold + CNN2
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0.01 => new
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 3 => new
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 32 => new
    KERAS_KERNEL_SIZE = [3,4,5] => new
    KERAS_POOL_SIZE = 3 => new
    model = model_cnn2 => new
    remove => Dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5 => new
    n-gram = 1
  - pre-train word Embeding: Glove 300d => new
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.243456385118 => 0.258691140343
  - time/round: GPU:144 , CPU: 124
 - total time: 770
 - LB: 

R6.4: Kfold + FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5 => new
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: Glove 300d => new
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.233992549537 => 0.243456385118 => 0.24513177753
 - time/round: GPU: 340, CPU: 
 - total time: 1653
 - LB: 0.37334 => 

 R6.5: Kfold + FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5 => new
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 2 => new
 - pre-train word Embeding: Glove 300d => new
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.225453577797 => 0.233632279046
 - time/round: GPU: 2666, CPU: 
 - total time: 11581
 - LB: 0.35343 => 0.36384
 
R6.6: Kfold + CNN2
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0.01 => new
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 3 => new
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 32 => new
    KERAS_KERNEL_SIZE = [3,4,5] => new
    KERAS_POOL_SIZE = 3 => new
    model = model_cnn2 => new
    remove => Dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    n-gram = 2 => new
  - pre-train word Embeding: Glove 300d => new
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.243456385118 => 0.293640187522
  - time/round: GPU:312 , CPU: 124
 - total time: 1285
 - LB: 

R6.7: Kfold + FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5 => new
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: Gensim word2vec => new
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.233992549537 =>  0.24090725485
 - avg best round: 48
 - time/round: GPU: 364, CPU:
 - total time: 1698
 - LB: 0.37334 => 0.37974

 R6.8: Kfold + CNN2
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0.01
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 3
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 32
    KERAS_KERNEL_SIZE = [3,4,5]
    KERAS_POOL_SIZE = 3
    model = model_cnn2 => new
    
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    n-gram = 1 => new
  - pre-train word Embeding: Gensim word2vec => new
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.243456385118 => 0.26251352489
 - avg best round: 2
 - time/round: GPU:87 , CPU: 124
  - total time: 447
 - LB: 0.37974 => 0.37474

 R6.9: Kfold + CNN2
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0.01
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 3
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = 32
    KERAS_KERNEL_SIZE = [3,4,5]
    KERAS_POOL_SIZE = 3
    model = model_cnn2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    n-gram = 2 => new
  - pre-train word Embeding: Gensim word2vec => new
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.243456385118 => 0.284678618094
 - avg best round: 1.0
 - time/round: GPU:223 , CPU: 124
  - total time: 1126
 - LB: 0.37474 =>

R6.10: Kfold + FastText
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 300 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 2 => new
 - pre-train word Embeding: Gensim word2vec => new
 - preprocess text: PorterStemmer: train + eval
 - metric: 0.225453577797 => 0.243634324644
 - avg best round: 39
 - time/round: GPU: 1738, CPU: 
 - total time: 7018
 - LB: 0.35343 => 0.37908
 #-----------------------------------------------------------------------
 R7.1: FastText + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Concatenate
    model3 = model_fasttext_combined + model_input2_dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.233992549537 => 0.234144493298
 - time/round: GPU: 40, CPU: 
 - total time: 587
 - LB: 0.37334 => 0.38605

 R7.2: FastText + ConvNet
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Concatenate
    model3 = model_fasttext_ + model_cnn => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.233992549537 => 0.273421258495
 - time/round: GPU: 40, CPU: 
 - total time: 348
 - LB: 0.37334 => 

 R7.3: ConvNet + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # Concatenate
    model3 = model_cnn + model_input2_dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.233992549537 => 0.267486659762
 - time/round: GPU: 40, CPU: 
 - total time: 313
 - LB: 0.37334 => 

 R7.4: FastText + CuDNNLSTM
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Concatenate
    model3 = FastText + CuDNNLSTM = new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.233992549537 => 0.297750015546
 - time/round: GPU: 40, CPU: 
 - total time: 348
 - LB: 0.37334 => 

 R7.5: CuDNNLSTM + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Concatenate
    model3 = CuDNNLSTM + model_input2_dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.233992549537 => 0.315910797928
 - time/round: GPU: 40, CPU: 
 - total time: 728
 - LB: 0.37334 => 

R7.6: FastText + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Merge => Dot
    model3 = Dot(model_fasttext + model_input2_dense, normalize=True) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.234144493298 => 0.375530476437
 - time/round: GPU: 40, CPU: 
 - total time: 587
 - LB: 0.38605 =>

 R7.7: FastText + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Merge => Average
    model3 = Average(model_fasttext + model_input2_dense) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.234144493298 => 0.245775830058
 - best round: 63
 - time/round: GPU: 40, CPU: 
 - total time: 705
 - LB: 0.38605 =>

 R7.8: FastText + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # model_input2_dense
    feature_input + dense + dropout => new
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + model_input2_dense) => new
    out_model = GlobalAveragePooling1D + dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df
 - metric: 0.234144493298 => 0.224523078692
 - best round: 43
 - time/round: GPU: 40, CPU: 
 - total time: 1100
 - LB: 0.38605 => 0.38693

 R7.9: FastText + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText
    GlobalAveragePooling1D 
    # model_input2_dense
    feature_input + dense + dropout => new
    nodes = train_df.shape[1] * 2 => new
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + model_input2_dense)
    out_model = GlobalAveragePooling1D + dropout
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df
 - metric: 0.224523078692 => 0.23345371717
 - best round: 76
 - time/round: GPU: 40, CPU: 
 - total time: 829
 - LB: 0.38693 =>

 R7.10: FastText + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText
    GlobalAveragePooling1D 
    # model_input2_dense
    feature_input + dense + dropout => new
    nodes = train_df.shape[1] * 2 => new
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + model_input2_dense)
    out_model = dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df
 - metric: 0.224523078692 => 0.233607849165
 - best round: 67
 - time/round: GPU: 40, CPU: 
 - total time: 829
 - LB: 0.38693 =>

 R7.11: MODEL_LSTM_ATTRNN + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # MODEL_LSTM_ATTRNN
    model = Bidirectional(GRU) + AttentionLayer + dropout => new
    # Merge => Concatenate
    model3 = Concatenate(MODEL_LSTM_ATTRNN + model_input2_dense)
    out_model = dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df
 - metric: 0.247854546337 => 0.247604691093
 - best round: 3
 - time/round: GPU: 360, CPU: 
 - total time: 829
 - LB: 0.38693 => 0.43196

 R7.12: FastText + MODEL_LSTM_ATTRNN
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText
    GlobalAveragePooling1D 
    # MODEL_LSTM_ATTRNN
    model = Bidirectional(GRU) + AttentionLayer + dropout => new
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + MODEL_LSTM_ATTRNN) => new
    out_model = dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df
 - metric: 0.233607849165 => 0.261458906983
 - best round: 3
 - time/round: GPU: 360, CPU: 
 - total time: 2320
 - LB: 0.38693 =>

 R7.13: FastText + MODEL_LSTM_ATTRNN
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText
    GlobalAveragePooling1D 
    # MODEL_LSTM_ATTRNN
    model = Bidirectional(GRU) + AttentionLayer + dropout => new
    # Merge => Average
    model3 = Average(model_fasttext + MODEL_LSTM_ATTRNN) => new
    out_model = dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df
 - metric: 0.233607849165 => 0.258402486302
 - best round: 2
 - time/round: GPU: 360, CPU: 
 - total time: 2368
 - LB: 0.38693 =>

 R7.14: FastText + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText
    GlobalAveragePooling1D 
    # model_input2_dense
    nodes = min(n_features * 2, OUTPUT_DIM) => new
    feature_input + dense + dropout
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + model_input2_dense)
    out_model = dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 => new
 - metric: 0.233607849165 => 0.30722242061
 - best round: 2
 - time/round: GPU: 40, CPU: 
 - total time: 79
 - LB: 0.38693 => 

 R7.15: FastText + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText
    GlobalAveragePooling1D 
    # model_input2_dense
    nodes = min(n_features * 2, OUTPUT_DIM)
    feature_input + dense + dropout
    # Merge => Average => new
    model3 = Average(model_fasttext + model_input2_dense)
    out_model = dropout
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 => new
 - metric: 0.233607849165 => 0.318385487055
 - best round: 4
 - time/round: GPU: 10, CPU: 
 - total time: 79
 - LB: 0.38693 => 

 R7.16: FastText + model_input2_cnn
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText
    GlobalAveragePooling1D 
    # model_input2_cnn => new
    nodes = min(n_features*2, OUTPUT_DIM)
    Conv1D(nodes) + GlobalMaxPooling1D + dropout + BatchNormalization => new
    KERAS_KERNEL_SIZE = 2 => new
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + model_input2_dense)
    out_model = dropout
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 => new
 - metric: 0.233607849165 => 0.304725236255
 - best round: 25
 - time/round: GPU: 40, CPU: 
 - total time: 350
 - LB: 0.38693 => 

 

 R7.17: FastText + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText
    GlobalAveragePooling1D 
    # model_input2_dense
    nodes = min(n_features * 2, OUTPUT_DIM) => new
    feature_input + dense + dropout + BatchNormalization => new
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + model_input2_dense)
    out_model = dropout
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 => new
 - metric: 0.233607849165 => 0.313511806379
 - best round: 14
 - time/round: GPU: 40, CPU: 
 - total time: 
 - LB: 0.38693 => 

 R7.18: FastText + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText
    GlobalAveragePooling1D 
    # model_input2_dense
    nodes = min(n_features * 2, OUTPUT_DIM) => new
    feature_input + dense + dropout + BatchNormalization => new
    # Merge => Average => new
    model3 = Average(model_fasttext + model_input2_dense)
    out_model = dropout
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 => new
 - metric: 0.233607849165 => 0.364663020096
 - best round: 11
 - time/round: GPU: 40, CPU: 
 - total time: 
 - LB: 0.38693 => 

 R7.19: FastText + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText
    GlobalAveragePooling1D 
    # model_input2_dense
    nodes = min(n_features * 2, OUTPUT_DIM)
    feature_input + dense + dropout + BatchNormalization
    # Merge => Average => new (R9.13)
    model3 = Average(model_fasttext + model_input2_dense)
    out_model = dropout
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 + PCA(OUTPUT_DIM => new
 - metric: 0.233607849165 => 0.3271392084
 - best round: 99
 - time/round: GPU: 40, CPU: 
 - total time: 
 - LB: 0.38693 => 

 R7.20: FastText + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText
    GlobalAveragePooling1D 
    # model_input2_dense
    nodes = min(n_features * 2, OUTPUT_DIM)
    feature_input + dense + dropout + BatchNormalization
    # Merge => Concatenate => new (R9.13)
    model3 = Concatenate(model_fasttext + model_input2_dense)
    out_model = dropout
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 + PCA(OUTPUT_DIM => new
 - metric: 0.233607849165 => 0.243191976134
 - best round: 46
 - time/round: GPU: 40, CPU: 
 - total time: 
 - LB: 0.38693 => 

 R7.21: FastText + model_input2_dense
 - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText
    GlobalAveragePooling1D 
    # model_input2_dense
    nodes = min(n_features * 2, OUTPUT_DIM)
    feature_input + dense + dropout + BatchNormalization
    # Merge => Concatenate => new (R9.13)
    model3 = Concatenate(model_fasttext + model_input2_dense)
    out_model = dropout
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 + no_scaled => new
 - metric: 0.233607849165 => 0.234134222289
 - best round: 73
 - time/round: GPU: 15 CPU: 
 - total time: 926
 - LB: 0.38693 => 

 R7.22: ConvNet + model_input2_cnn
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0.01 => new
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet (R1.15)
    KERAS_FILTERS = 64 => new
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    model = model_cnn2 => new
    remove => Dense => new
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + model_input2_dense)
    out_model = dropout
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    n-gram = 1
  - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 => new
 - metric: 0.233607849165 => 0.248478247041
 - best round: 2
 - time/round: GPU: 40, CPU: 124
 - LB: 

 R7.23: ConvNet + model_input2_cnn
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0.01 => new
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet (R1.20) => new
    KERAS_FILTERS = 32 => new
    KERAS_KERNEL_SIZE = [3,4,5]
    KERAS_POOL_SIZE = 5 => new
    model = model_cnn2 => new
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + model_input2_dense)
    out_model = dropout
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    n-gram = 2
  - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 => new
 - metric: 0.233607849165 => 0.31053
 - best round: 2
 - time/round: GPU: 40, CPU: 124
 - LB: 

 R7.24: ConvNet + model_input2_cnn
- params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 1000
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 3
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0.01 => new
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 10
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet (R1.19) => new
    KERAS_FILTERS = 32 => new
    KERAS_KERNEL_SIZE = [3,4,5]
    KERAS_POOL_SIZE = 3 => new
    model = model_cnn2 => new
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + model_input2_dense)
    out_model = dropout
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    n-gram = 1
  - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 => new
 - metric: 0.233607849165 => 0.25031
 - best round: 2
 - time/round: GPU: 40, CPU: 124
 - LB: 
 #------------------------------------------------------------------------------------
  R8.1: Kfold + FastText + model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Concatenate
    model3 = model_fasttext_combined + model_input2_dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5 => new
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.243456385118 => 0.235364179136 
 - time/round: GPU: 860, CPU: 
 - total time:  3542
 - LB: 0.38130 => 0.36900

 R8.2: Kfold + FastText + model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Concatenate
    model3 = model_fasttext + model_input2_dense => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5 => new
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.225453577797 => 0.22625076306
 - time/round: GPU: 4007, CPU: 
 - total time:  16347
 - LB: 0.35343 => 0.34809

 R8.3: Kfold + FastText + model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # Merge = Average
    model3 = Average(model_fasttext_combined + model_input2_dense) => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5 => new
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.235364179136 => 0.244762213326
 - time/round: GPU: 941, CPU: 
 - total time:  4510
 - LB: 0.36900 => 0.38505

R8.4: Kfold + FastText + model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # model_input2_dense
    feature_input + dense + dropout => new
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + model_input2_dense) => new
    out_model = GlobalAveragePooling1D + dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.235364179136 => 0.235244993635
 - time/round: GPU: 1258, CPU: 
 - total time:  5721
 - LB: 0.36900 => 0.38213

R8.5: Kfold + FastText + model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # model_input2_dense
    feature_input + dense + dropout => new
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + model_input2_dense) => new
    out_model = GlobalAveragePooling1D + dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.244762213326 => 0.228545237713
 - time/round: GPU:4907 , CPU: 
 - total time:  23206
 - LB: 0.34809 => 0.36316 

 R8.6: Kfold + FastText + model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # model_input2_dense
    feature_input + dense + dropout => new
    nodes = train_df.shape[1] * 2 => new
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + model_input2_dense) => new
    out_model = GlobalAveragePooling1D + dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.235244993635 => 0.234896860061
 - time/round: GPU: 1117, CPU: 
 - total time:  5721 => 4933
 - LB: 0.36900 => 0.37040

 R8.7: Kfold + FastText + model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # model_input2_dense
    feature_input + dense + dropout => new
    nodes = train_df.shape[1] * 2 => new
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + model_input2_dense) => new
    out_model = dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.234896860061 => 0.234719080041
 - time/round: GPU: 1117, CPU: 
 - total time:  5721 => 4195
 - LB: 0.36900 => 0.36884

 R8.8: Kfold + FastText + model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # model_input2_dense
    feature_input + dense + dropout => new
    nodes = train_df.shape[1] * 2 => new
    # Merge => Concatenate
    model3 = Concatenate(model_fasttext + model_input2_dense) => new
    out_model = dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 2 => new
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - metric: 0.22625076306 => 0.223104115334
 - time/round: GPU: 4080, CPU: 
 - total time:  16347 => 20012
 - LB: 0.34809 => 0.34542

 R8.9: Kfold + FastText + model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # model_input2_dense
    nodes = min(n_features * 2, OUTPUT_DIM)
    feature_input + dense + dropout + BatchNormalization
    # Merge => Concatenate (R9.13) => new
    model3 = Concatenate(model_fasttext + model_input2_dense) => new
    out_model = dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 + PCA(OUTPUT_DIM) => new
 - metric: 0.234719080041 => 0.23209200965
 - time/round: GPU:973 , CPU: 
 - total time: 4222 
 - LB: 0.36884 => 0.36631

 R8.10: Kfold + FastText + model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # model_input2_dense
    nodes = min(n_features * 2, OUTPUT_DIM)
    feature_input + dense + dropout + BatchNormalization
    # Merge => Concatenate (R9.13) => new
    model3 = Concatenate(model_fasttext + model_input2_dense) => new
    out_model = dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 + no_scaled => new
 - metric: 0.23209200965 => 0.296880601897
 - time/round: GPU:236 , CPU: 
 - total time: 3213 
 - LB: 0.36631 => 0.44596


R8.11: Kfold + FastText + model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 0 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # ConvNet
    KERAS_FILTERS = OUTPUT_DIM
    KERAS_KERNEL_SIZE = 2
    KERAS_POOL_SIZE = 2
    # FastText => new
    GlobalAveragePooling1D 
    removed => BatchNormalization
    removed => Dense(OUTPUT_DIM)
    # model_input2_dense
    nodes = min(n_features * 2, OUTPUT_DIM)
    feature_input + dense + dropout + BatchNormalization
    # Merge => Concatenate (R9.13) => new
    model3 = Concatenate(model_fasttext + model_input2_dense) => new
    out_model = dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 2
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 + PCA(OUTPUT_DIM) => new
 - metric: 0.23209200965 => 0.228692740287
 - time/round: GPU:3249 , CPU: 
 - total time: 17933 
 - LB: 0.34542 => 0.35503
 #--------------------------------------------------------
 R9.1: model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # model_input2_dense
    nodes = OUTPUT_DIM
    feature_input + dense + dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - best round: 36
 - metric: 0.584740362201
 - time/round: GPU:4907 , CPU: 
 - total time:  
 - LB: 

 R9.2: model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1 
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # model_input2_dense
    nodes = train_df.shape[1] * 2 => new
    feature_input + dense + dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - best round: 94
 - metric: 0.584740362201 => 0.600371912137 => 0.592250554749 (model-R-9-3-2)
 - time/round: GPU:4907 , CPU: 
 - total time:  
 - LB: 

R9.3: model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 2
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # model_input2_dense
    nodes = train_df.shape[1] * 2 => new
    feature_input + dense + dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df => new
 - best round: 94
 - metric: 0.592250554749 => 0.594534111948
 - time/round: GPU:4907 , CPU: 
 - total time:  
 - LB:  

 R9.4: model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # model_input2_dense
    nodes = min(n_features * 2, OUTPUT_DIM) => new
    feature_input + dense + dropout
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 => new
 - best round: 2
 - metric: 0.592250554749 => 0.31720
 - time/round: GPU:4907 , CPU: 
 - total time:  
 - LB:  

 R9.5: model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 2 => new
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # model_input2_dense
    nodes = min(n_features * 2, OUTPUT_DIM) => new
    feature_input + dense + dropout
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 => new
 - best round: 2
 - metric: 0.31720 => 0.320337775414
 - time/round: GPU:5 , CPU: 
 - total time:  
 - LB:  

 R9.6: model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # model_input2_dense
    nodes = min(n_features * 2, OUTPUT_DIM* 2) => new
    feature_input + dense + dropout
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 => new
 - best round: 1
 - metric: 0.31720 => 0.319189828433
 - time/round: GPU:5 , CPU: 
 - total time:  
 - LB:  

 R9.7: model_input2_cnn
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # model_input2_cnn
    Conv1D + GlobalMaxPooling1D + dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 => new
 - best round: 3
 - metric: 0.31720 => 0.318681809553
 - time/round: GPU:5 , CPU: 
 - total time:  
 - LB:  

R9.8: model_input2_cnn
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # model_input2_cnn
    Conv1D + GlobalMaxPooling1D + dropout => new
    KERAS_KERNEL_SIZE = 2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 => new
 - best round: 4
 - metric: 0.31720 => 0.308727309823
 - time/round: GPU:5 , CPU: 
 - total time:  
 - LB:   

 R9.9: model_input2_cnn
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # model_input2_cnn
    Conv1D + GlobalMaxPooling1D + dropout + BatchNormalization => new
    KERAS_KERNEL_SIZE = 2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 => new
 - best round: 4
 - metric: 0.308727309823 => 0.317062879729
 - time/round: GPU:5 , CPU: 
 - total time:  
 - LB:   

 R9.10: model_input2_cnn
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # model_input2_cnn
    Conv1D + GlobalMaxPooling1D + dropout + Dense + BatchNormalization => new
    KERAS_KERNEL_SIZE = 2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 => new
 - best round: 4
 - metric: 0.308727309823 => 0.330456400916 => 0.325591651424
 - time/round: GPU:5 , CPU: 
 - total time:  
 - LB:   

 R9.11: model_input2_cnn
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # model_input2_cnn
    nodes = max(n_features//KERAS_KERNEL_SIZE, OUTPUT_DIM)
    Conv1D(nodes) + GlobalMaxPooling1D + dropout + Dense + BatchNormalization => new
    KERAS_KERNEL_SIZE = 2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 => new
 - best round: 3
 - metric: 0.308727309823 => 0.330276714434
 - time/round: GPU:5 , CPU: 
 - total time:  
 - LB:   

R9.12: model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # model_input2_dense
    nodes = min(n_features * 2, OUTPUT_DIM) => new
    feature_input + dense + dropout + BatchNormalization => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2 => new
 - best round: 9
 - metric: 0.31720 => 0.316722304086
 - time/round: GPU:5 , CPU: 
 - total time:  
 - LB:   
 
 R9.13: model_input2_dense
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 1
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # model_input2_dense
    nodes = min(n_features * 2, OUTPUT_DIM) => new
    feature_input + dense + dropout + BatchNormalization => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - feature egineering: train_df + eval_df + train_df2 + eval_df2  + PCA(OUTPUT_DIM) => new
 - best round: 
 - metric: 0.316722304086 => 0.621869914865 => 0.450674982644
 - time/round: GPU:5 , CPU: 
 - total time:  
 - LB:   

 #--------------------------------------------------------
 R10.1: MODEL_LSTM_ATTRNN
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 200 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 2
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # MODEL_LSTM_ATTRNN
    model = Bidirectional(GRU) + AttentionLayer + dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - best round: 2
 - metric: 0.362492487626 => 0.271734277884
 - time/round: GPU:300 , CPU: 
 - total time:  
 - LB:  

 R10.2: MODEL_LSTM_ATTRNN
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 2
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # MODEL_LSTM_ATTRNN
    model = model_lstm_attrnn2 => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - best round: 2
 - metric: 0.271734277884 => 0.250843730688
 - time/round: GPU:300 , CPU: 
 - total time:  
 - LB:  

 R10.3: MODEL_LSTM_ATTRNN
  - params:
    SEQUENCE_LENGTH = 500
    OUTPUT_DIM = 500 => new
    KERAS_LEARNING_RATE = 0.001
    KERAS_N_ROUNDS = 100
    KERAS_BATCH_SIZE = 64
    KERAS_NODES = 1024
    KERAS_LAYERS = 2
    KERAS_DROPOUT_RATE = 0.5
    # KERAS_REGULARIZER = KERAS_LEARNING_RATE/10
    KERAS_REGULARIZER = 0
    KERAS_VALIDATION_SPLIT = 0.2
    KERAS_EARLY_STOPPING = 5
    KERAS_MAXNORM = 3
    KERAS_PREDICT_BATCH_SIZE = 1024
    # MODEL_LSTM_ATTRNN
    model = model_lstm_attrnn2 + dropout => new
    # Other keras params
    decay = KERAS_LEARNING_RATE / KERAS_N_ROUNDS
    dropout = KERAS_DROPOUT_RATE
    N_FOLDS = 5
    optimizer = Adam
    input_length= max(max_train_sequence, max_test_sequence, SEQUENCE_LENGTH)
    n-gram = 1
 - pre-train word Embeding: None
 - preprocess text: PorterStemmer: train + eval
 - best round: 2
 - metric: 0.250843730688 => 0.247854546337
 - time/round: GPU:300 , CPU: 
 - total time:  
 - LB:  


#-----------------------------------------------------------------------------

 


