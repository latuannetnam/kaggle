{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalized Medicine: Redefining Cancer Treatment\n",
    "- Predict the effect of Genetic Variants to enable Personalized Medicine\n",
    "- https://www.kaggle.com/c/msk-redefining-cancer-treatment/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ------------------ Math libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from math import sqrt\n",
    "from scipy import stats\n",
    "from scipy import sparse\n",
    "# ------------------- SKlearn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, learning_curve, validation_curve, KFold\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from mlxtend.evaluate import confusion_matrix\n",
    "from mlxtend.plotting import plot_decision_regions, plot_learning_curves, plot_confusion_matrix\n",
    "\n",
    "# --------------- NLP ----------------\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "# spaCy \n",
    "#from spacy.en import English\n",
    "# gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter, defaultdict\n",
    "# --------- System ---------\n",
    "import datetime\n",
    "import sys\n",
    "from inspect import getsourcefile\n",
    "import os.path\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "# Input data files are available in the DATA_DIR directory.\n",
    "DATA_DIR = \"data-temp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data. Download from: https://www.kaggle.com/c/msk-redefining-cancer-treatment/data\n",
    "train_text = pd.read_csv(DATA_DIR + \"/training_text.csv\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\n",
    "train_class = pd.read_csv(DATA_DIR + \"/training_variants.csv\")\n",
    "test_text = pd.read_csv(DATA_DIR + \"/test_text.csv\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\n",
    "test_class = pd.read_csv(DATA_DIR + \"/test_variants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Train text:\", train_text.shape, \" train class:\", train_class.shape)\n",
    "print(\"Test text:\", test_text.shape, \" test class:\", test_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_class[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "      <td>1</td>\n",
       "      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>3</td>\n",
       "      <td>Recent evidence has demonstrated that acquired...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>4</td>\n",
       "      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Gene             Variation  Class  \\\n",
       "0   0  FAM58A  Truncating Mutations      1   \n",
       "1   1     CBL                 W802*      2   \n",
       "2   2     CBL                 Q249E      2   \n",
       "3   3     CBL                 N454D      3   \n",
       "4   4     CBL                 L399V      4   \n",
       "\n",
       "                                                Text  \n",
       "0  Cyclin-dependent kinases (CDKs) regulate a var...  \n",
       "1   Abstract Background  Non-small cell lung canc...  \n",
       "2   Abstract Background  Non-small cell lung canc...  \n",
       "3  Recent evidence has demonstrated that acquired...  \n",
       "4  Oncogenic mutations in the monomeric Casitas B...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine train_text + train_class\n",
    "train_full = train_class.merge(train_text, how=\"inner\", left_on=\"ID\", right_on=\"ID\")\n",
    "train_full.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ACSL4</td>\n",
       "      <td>R570S</td>\n",
       "      <td>2. This mutation resulted in a myeloproliferat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NAGLU</td>\n",
       "      <td>P521L</td>\n",
       "      <td>Abstract The Large Tumor Suppressor 1 (LATS1)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PAH</td>\n",
       "      <td>L333F</td>\n",
       "      <td>Vascular endothelial growth factor receptor (V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ING1</td>\n",
       "      <td>A148D</td>\n",
       "      <td>Inflammatory myofibroblastic tumor (IMT) is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>TMEM216</td>\n",
       "      <td>G77A</td>\n",
       "      <td>Abstract Retinoblastoma is a pediatric retina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     Gene Variation                                               Text\n",
       "0   0    ACSL4     R570S  2. This mutation resulted in a myeloproliferat...\n",
       "1   1    NAGLU     P521L   Abstract The Large Tumor Suppressor 1 (LATS1)...\n",
       "2   2      PAH     L333F  Vascular endothelial growth factor receptor (V...\n",
       "3   3     ING1     A148D  Inflammatory myofibroblastic tumor (IMT) is a ...\n",
       "4   4  TMEM216      G77A   Abstract Retinoblastoma is a pediatric retina..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine test_text + test_class\n",
    "test_full = test_class.merge(test_text, how=\"inner\", left_on=\"ID\", right_on=\"ID\")\n",
    "test_full.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3321.000000\n",
       "mean        4.365854\n",
       "std         2.309781\n",
       "min         1.000000\n",
       "25%         2.000000\n",
       "50%         4.000000\n",
       "75%         7.000000\n",
       "max         9.000000\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = \"Class\"\n",
    "# Initial features\n",
    "o_features = test_full.columns.values\n",
    "# Store eval_id\n",
    "eval_data_id = test_full['ID'].values\n",
    "#Seperate input and label from train_data\n",
    "input_data = train_full[o_features]\n",
    "target = train_full[label]\n",
    "target.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">train</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>Recent evidence has demonstrated that acquired...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID    Gene             Variation  \\\n",
       "train 0   0  FAM58A  Truncating Mutations   \n",
       "      1   1     CBL                 W802*   \n",
       "      2   2     CBL                 Q249E   \n",
       "      3   3     CBL                 N454D   \n",
       "      4   4     CBL                 L399V   \n",
       "\n",
       "                                                      Text  \n",
       "train 0  Cyclin-dependent kinases (CDKs) regulate a var...  \n",
       "      1   Abstract Background  Non-small cell lung canc...  \n",
       "      2   Abstract Background  Non-small cell lung canc...  \n",
       "      3  Recent evidence has demonstrated that acquired...  \n",
       "      4  Oncogenic mutations in the monomeric Casitas B...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine train + eval data\n",
    "combine_data = pd.concat([input_data, test_full], keys=['train','eval'])\n",
    "data = combine_data\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Gene categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col = 'Gene'\n",
    "print(data[col].head(5))\n",
    "data[col].value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Variants categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col = 'Variation'\n",
    "print(data[col].head(5))\n",
    "data[col].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 3, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform target\n",
    "# Label encode the targets\n",
    "labels = LabelEncoder()\n",
    "target_tf = labels.fit_transform(target)\n",
    "target = target_tf\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check NaN columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_null_data(data):\n",
    "    #Get high percent of NaN data\n",
    "    null_data = data.isnull()\n",
    "    total = null_data.sum().sort_values(ascending=False)\n",
    "    percent = (null_data.sum()/null_data.count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    high_percent_miss_data = missing_data[missing_data['Percent']>0]\n",
    "    #print(missing_data)\n",
    "    print(high_percent_miss_data)\n",
    "    miss_data_cols = high_percent_miss_data.index.values\n",
    "    return miss_data_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_null_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize function based on NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Credit: http://nlpforhackers.io/recipe-text-clustering/\n",
    "stemmer = PorterStemmer()\n",
    "def tokenize_nltk(text):\n",
    "    toktok = ToktokTokenizer()\n",
    "    #tokens =[toktok.tokenize(sent) for sent in sent_tokenize(text)]\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    #print(\"Number of tokens:\", len(tokens))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize function based on spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# credit: https://nicschrading.com/project/Intro-to-NLP-with-spaCy/\n",
    "parser = English()\n",
    "# A custom stoplist\n",
    "STOPLIST = set(sw.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS))\n",
    "# List of symbols we don't care about\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"'ve\"]\n",
    "# Every step in a pipeline needs to be a \"transformer\". \n",
    "# Define a custom transformer to clean text using spaCy\n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Convert text to cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [cleanText(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "    # A custom function to clean the text before sending it into the vectorizer\n",
    "    def cleanText(self, text):\n",
    "        # get rid of newlines\n",
    "        text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "\n",
    "        # replace twitter @mentions\n",
    "        mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "        text = mentionFinder.sub(\"@MENTION\", text)\n",
    "\n",
    "        # replace HTML symbols\n",
    "        text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "\n",
    "        # lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        return text\n",
    "\n",
    "# A custom function to tokenize the text using spaCy\n",
    "# and convert to lemmas\n",
    "def tokenize_spacy(sample):\n",
    "\n",
    "    # get the tokens using spaCy\n",
    "    tokens = parser(sample)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "\n",
    "    # stoplist the tokens\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "\n",
    "    # stoplist symbols\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "\n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec tranform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# credit: https://github.com/nadbordrozd/blog_stuff/blob/master/classification_w2v/benchmarking.ipynb\n",
    "def word2vec(data, max_features=2000, min_df=5, n_jobs=4, load=False):\n",
    "    # train word2vec on all the texts - both training and test set\n",
    "    # we're not using test labels, just texts so this is fine\n",
    "    model_file = DATA_DIR + \"/word2vec\"\n",
    "    if load:\n",
    "        model = Word2Vec.load(model_file)\n",
    "    else:    \n",
    "        start = time.time()\n",
    "        model = Word2Vec(data, size=max_features, min_count=min_df, workers=n_jobs)\n",
    "        end = time.time() - start\n",
    "        model.save(model_file)\n",
    "        print(\"Word2vec training time:\", end)\n",
    "    w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}\n",
    "    return w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.values())\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        array = np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ]) \n",
    "        return sparse.csr_matrix(array) \n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X, None)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainng word2vec model for combine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020415306091308594"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = combine_data['Text'].values\n",
    "start = time.time()\n",
    "w2v = word2vec(data, load=True)\n",
    "end = time.time() - start\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_features_w2v(data, load=False):\n",
    "    filename = DATA_DIR + \"/tfidf.npz\"\n",
    "    if load:\n",
    "        with open(filename, 'rb') as infile:\n",
    "            array = pickle.load(infile)\n",
    "    else:\n",
    "        w2v = word2vec(data, load=True)\n",
    "        tfidf = TfidfEmbeddingVectorizer(w2v)\n",
    "        start = time.time()\n",
    "        array = tfidf.fit_transform(data)\n",
    "        end = time.time() - start\n",
    "        with open(filename, 'wb') as outfile:\n",
    "            pickle.dump(array, outfile, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"Text feature Transform time:\", end)\n",
    "    return array    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidf = TfidfVectorizer(min_df=5, max_features=2000, stop_words=sw.words('english'), tokenizer=tokenize, lowercase=True)\n",
    "#time = 27.434333562850952\n",
    "#vectorize = CountVectorizer(min_df=5, max_features=2000, stop_words=sw.words('english'), tokenizer=tokenize, lowercase=True)\n",
    "#0.7735180854797363\n",
    "#vectorizer = CountVectorizer(min_df=5, max_features=2000, stop_words=sw.words('english'), lowercase=True)\n",
    "# 27.707584381103516\n",
    "#vectorizer = HashingVectorizer(n_features=2000, stop_words=sw.words('english'), tokenizer=tokenize, lowercase=True)\n",
    "# 0.9716482162475586\n",
    "#vectorize = HashingVectorizer(n_features=2000, stop_words=sw.words('english'), lowercase=True)\n",
    "# 49.8653244972229 (100 records)\n",
    "vectorizer = CountVectorizer(min_df=5, max_features=2000, tokenizer=tokenize_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2946 sec\n",
    "#tfidf = TfidfVectorizer(min_df=5, max_features=2000, stop_words=sw.words('english'), tokenizer=tokenize_nltk, lowercase=True)\n",
    "# time = 78.87074208259583\n",
    "#tfidf = TfidfVectorizer(min_df=5, max_features=2000, stop_words=sw.words('english'), lowercase=True)\n",
    "# time =49.89134097099304 sec (100 records)\n",
    "#tfidf = TfidfVectorizer(min_df=5, max_features=2000, tokenizer=tokenize_spacy, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time = 68.3006434440612 (100 records)\n",
    "tfidf = TfidfEmbeddingVectorizer(w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13696599006652832"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = train_full['Text'][:100]\n",
    "start = time.time()\n",
    "#tf = vectorizer.fit_transform(data1)\n",
    "#tf = tfidf.fit_transform(data1)\n",
    "tf = text_features_w2v(data1, load=False)\n",
    "end = time.time() - start\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf size: (100, 2000)\n",
      "tf1 size: (50, 2000)\n"
     ]
    }
   ],
   "source": [
    "tf1 = tf[:50]\n",
    "tf2 = tf[5:]\n",
    "print(\"tf size:\", tf.shape)\n",
    "print(\"tf1 size:\", tf1.shape)\n",
    "#tf11 = tf1.todense()\n",
    "#tf11[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf1 = TfidfTransformer(use_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf\n",
    "#data = tfidf1.fit_transform(tf)\n",
    "y = target[:data.shape[0]]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, y, train_size=0.7, random_state=324)\n",
    "model1 = LinearSVC()\n",
    "model1.fit(X_train, Y_train)\n",
    "model1.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranform combine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text feature Transform time: 27.413230419158936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27.7857027053833"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time = 78.87074208259583 => tokenize = None\n",
    "# time = 2946.212862968445 => tokenize = word_tokenize\n",
    "# time = 5492.0656995773315 => tokenize = spacy\n",
    "data = combine_data['Text'].values\n",
    "start = time.time()\n",
    "#data_tf = tfidf.fit_transform(data)\n",
    "data_tf = text_features_w2v(data, load=False)\n",
    "end = time.time() - start\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train set and eval set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_len = len(train_full)\n",
    "train_set = data_tf[:train_len]\n",
    "eval_set = data_tf[train_len:]\n",
    "print(\"Train set:\", train_set.shape, \" eval set:\", eval_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model pipe line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Score: 0.2, submission score: 1.64639\n",
    "#model = SVC(decision_function_shape='ovo', probability=True, random_state=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(n_estimators=500, max_depth=5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessor = NLTKPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline([('tfidf', tfidf),\n",
    "                            ('model', model),\n",
    "                 ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train set in to train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = train_set\n",
    "print(data.shape)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(data, target, train_size=0.7, random_state=324)\n",
    "print(\"train size:\", X_train.shape)\n",
    "print(\"test size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# time: 298.4094178676605\n",
    "start = time.time()\n",
    "model.fit(X_train, Y_train)\n",
    "end = time.time() - start\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "score = model.score(X_test, Y_test)\n",
    "end = time.time() - start\n",
    "print(\"time:\", end, \" score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "end = time.time() - start\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_target=Y_test, \n",
    "                      y_predicted=y_pred)\n",
    "plot_confusion_matrix(conf_mat=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict eval set and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predictions\n",
    "# time = 3.4443037509918213\n",
    "data_eval = eval_set\n",
    "start = time.time()\n",
    "y_pred=model.predict_proba(data_eval)\n",
    "end = time.time() - start\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tweaking the submission file as required\n",
    "# credit: https://www.kaggle.com/punyaswaroop12/gbm-starter-top-40\n",
    "subm_file = pd.DataFrame(y_pred)\n",
    "subm_file['id'] = eval_data_id\n",
    "subm_file.columns = ['class1', 'class2', 'class3', 'class4', 'class5', 'class6', 'class7', 'class8', 'class9', 'id']\n",
    "subm_file.to_csv(DATA_DIR+ \"/submission_v2.csv\",index=False)\n",
    "subm_file.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
