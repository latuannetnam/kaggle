0. Best score:
  - 2017-07-01: Round 7, Best score:0.12909, Rank: 882, Top 46%
  - 2017-07-02: Round 9, Best score:0.12889, Rank: 872, Top 45%
1. 2017-06-30
- Rank 1: 0.01557
- Rank 2: 0.105672
- Rank 50: 0.11359

- Best score:  0.13001, Rank: 901 (top 47%)
- rmse: 0.14104798652556305
- Base models:
    + XGBRegressor(n_estimators=500, max_depth=5, n_jobs=-1)
    + GradientBoostingRegressor(n_estimators=500) => AdaBoostRegressor(base_estimator=model_temp, n_estimators=200, random_state=200)
    + ExtraTreesRegressor(n_estimators=500, n_jobs=-1)
    + DecisionTreeRegressor(max_depth=20) => AdaBoostRegressor(base_estimator=model_temp, n_estimators=200, random_state=200)
    + RandomForestRegressor(n_estimators=500, n_jobs=-1)
    + Kfolds = 5
    + n_features = 77
- Stack model:
    + GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')

2. 2017-06-30:
 -Round 1:
    + Boost: AdaBoostRegressor(base_estimator=model_temp, n_estimators=200, random_state=200)
    + XGBRegressor(n_estimators=500, max_depth=3, n_jobs=-1, random_state=123),: RMSE=0.1252588
    + ExtraTreesRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=456): RMSE= 0.143059197779
    + RandomForestRegressor(n_estimators=500, max_depth=10, n_jobs=-1, random_state=789): RMSE=0.142908791871
    + DecisionTreeRegressor(max_depth=10, random_state=146) => Boost: RMSE=0.141775155776
    + GradientBoostingRegressor(n_estimators=500, max_depth=1, random_state=357) => Boost : RMSE=0.140991803351
    + Kfolds = 5
    + n_features = 77 
    + AVG RMSE: 0.13879876178
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='lad')
    + Stack RMSE: 0.139057420149
    + score:  0.13250 
 - Round 2: (base models = Round 1)
    + AVG RMSE: 0.13879876178
    + Stack model: XGBRegressor(n_estimators=100, max_depth=1, n_jobs=-1)
    + Stack RMSE: 0.14280825157
    + score: 0.13313    
 3. 2017-07-01:
 - Round 3:
    + XGBRegressor(n_estimators=500, max_depth=3, n_jobs=-1, random_state=123)
    + AVG RMSE=0.1252588
    + Kfolds = 5
    + n_features = 77 
    + score: 0.13679
 - Round 4: (base models = Round 1)
    + AVG RMSE: 0.13879876178
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE: 0.14090396100286
    + score: 0.13018     
 - Round 5: (base models = Round 1) 
    + AVG RMSE: 0.13879876178
    + Stack Boost: AdaBoostRegressor(base_estimator=model_temp, n_estimators=500, learning_rate=0.1, random_state=200)
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber') => Boost
    + Stack RMSE:  0.1437158036472
    + score: 0.13299 < Round 4        
 -Round 6: 
    + data: non-scale features
    + Boost: AdaBoostRegressor(base_estimator=model_temp, n_estimators=200, random_state=200)
    + XGBRegressor(n_estimators=500, max_depth=3, n_jobs=-1, random_state=123),: RMSE=0.125400659888
    + ExtraTreesRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=456): RMSE= 0.143059197779
    + RandomForestRegressor(n_estimators=500, max_depth=10, n_jobs=-1, random_state=789): RMSE=0.142867999289
    + DecisionTreeRegressor(max_depth=10, random_state=146) => Boost: RMSE= 0.1414896008944
    + GradientBoostingRegressor(n_estimators=500, max_depth=1, random_state=357) => Boost : RMSE0.1432036356539
    + Kfolds = 5
    + n_features = 77 
    + AVG RMSE: 0.1392042187011
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE: 0.13831248511251
    + score:  0.13057 < 0.13018   
  -Round 7: Best score:0.12909, Rank: 882, Top 46%
    + num features: 77
    + data: non-scale features (Y) + scale features (log(Y))
    + Boost: AdaBoostRegressor(base_estimator=model_temp, n_estimators=200, random_state=200)
    + XGBRegressor(n_estimators=500, max_depth=3, n_jobs=-1, random_state=123),: RMSE=0.125400659888
    + ExtraTreesRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=456): RMSE= 0.143059197779
    + RandomForestRegressor(n_estimators=500, max_depth=10, n_jobs=-1, random_state=789): RMSE=0.142867999289
    + DecisionTreeRegressor(max_depth=10, random_state=146) => Boost: RMSE= 0.1414896008944
    + GradientBoostingRegressor(n_estimators=500, max_depth=1, random_state=357) => Boost : RMSE0.1432036356539
    + Kfolds = 5
    + n_features = 77 
    + AVG RMSE: 0.138798761786
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE: 0.138849117235557
    + score:  0.12909 => Best   
 - Round 8: (base models = Round 7)
    + num features: 80    
    + AVG RMSE:  0.13863344205998
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:   0.1391573479766
    + score: 0.13049 < Round 7        
    
 4. 2017-07-02:
 - Round 9:
    + base models level 1:
        * models: Round 7 
        * num features level 1: 80    
        * AVG RMSE:  0.13863344205998
    + base model level 2:
        * XGBRegressor(n_estimators=100, max_depth=1, n_jobs=-1, random_state=123). RMSE: 0.13074541917
        * GradientBoostingRegressor(n_estimators=200, max_depth=1, random_state=357). RMSE: 0.13404585765347
        * AVG RMSE: 0.132395638412
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:   0.14031113935
    + score: 0.12889 => Best
 
 5. 2017-07-03:    
 - Round 10:
    + fill NaN for special columns
    + base models level 1: Round 9
        * Fill null value for special columns
        * AVG RMSE:  0.13863344205998 => 0.140013612876
    + base model level 2: Round 9
        * AVG RMSE: 0.132395638412 => 0.13287786178
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:   0.14031113935   => 0.141457260264 
    + score: 0.13179
 - Round 11:
    + base models level 1: Round 9
        * Fill null value for special columns
        * AVG RMSE:  0.13863344205998 => 0.140013612876
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:  0.140411082228  
    + score: 0.13487 < Round 10
- Round 11:
    + base models level 1:
        * num features level 1: 80 
        * Ridge(): RMSE: 0.15786116707
        * Lasso(max_iter=2000, random_state=200). RMSE: 0.39956022133
        * ElasticNet(random_state=200). RMSE: 0.399560221333
        * BayesianRidge(). RMSE: 0.15345511789  
        * Kfolds = 4 
        * AVG RMSE: 0.277609181910
    + base model level 2:
        * Boost: AdaBoostRegressor(base_estimator=model_temp, n_estimators=200, random_state=200)
        * XGBRegressor(n_estimators=500, max_depth=3, n_jobs=-1, random_state=123),: RMSE=0.125400659888
        * ExtraTreesRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=456): RMSE= 0.143059197779
        * RandomForestRegressor(n_estimators=500, max_depth=10, n_jobs=-1, random_state=789): RMSE=0.142867999289
        * DecisionTreeRegressor(max_depth=10, random_state=146) => Boost: RMSE= 0.1414896008944
        * GradientBoostingRegressor(n_estimators=500, max_depth=1, random_state=357) => Boost : RMSE0.1432036356539
        * Kfolds = 4 
        * AVG RMSE: 0.14290463506
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:  0.15576363629  
    + score: 0.14517    
- Round 12:
    + base models level 1:
        * num features level 1: 80 
        * Ridge(): RMSE: 0.15786116707
        * Lasso(max_iter=2000, random_state=200). RMSE: 0.39956022133
        * ElasticNet(random_state=200). RMSE: 0.399560221333
        * BayesianRidge(). RMSE: 0.15345511789  
        * Kfolds = 4 
        * AVG RMSE: 0.277609181910
    + base model level 2:
        * Boost: AdaBoostRegressor(base_estimator=model_temp, n_estimators=200, random_state=200)
        * XGBRegressor(n_estimators=500, max_depth=3, n_jobs=-1, random_state=123),: RMSE=0.125400659888
        * ExtraTreesRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=456): RMSE= 0.143059197779
        * RandomForestRegressor(n_estimators=500, max_depth=10, n_jobs=-1, random_state=789): RMSE=0.142867999289
        * DecisionTreeRegressor(max_depth=10, random_state=146) => Boost: RMSE= 0.1414896008944
        * GradientBoostingRegressor(n_estimators=500, max_depth=1, random_state=357) => Boost : RMSE0.1432036356539
        * Kfolds = 5 
        * AVG RMSE: 0.14290463506
    + base model level 3:
        * XGBRegressor(n_estimators=100, max_depth=1, n_jobs=-1, random_state=123). RMSE: 0.13074541917
        * GradientBoostingRegressor(n_estimators=200, max_depth=1, random_state=357). RMSE: 0.13404585765347
        * AVG RMSE: 0.145246950   
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:  0.1565321638  
    + score: 0.14536    
- Round 13:
    + base models level 1:
        * num features level 1: 80
        * Boost: AdaBoostRegressor(base_estimator=model_temp, n_estimators=200, random_state=200)
        * XGBRegressor(n_estimators=500, max_depth=3, n_jobs=-1, random_state=123),: RMSE=0.125400659888
        * ExtraTreesRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=456): RMSE= 0.143059197779
        * RandomForestRegressor(n_estimators=500, max_depth=10, n_jobs=-1, random_state=789): RMSE=0.142867999289
        * DecisionTreeRegressor(max_depth=10, random_state=146) => Boost: RMSE= 0.1414896008944
        * GradientBoostingRegressor(n_estimators=500, max_depth=1, random_state=357) => Boost : RMSE0.1432036356539
        * Kfolds = 5  
        * AVG RMSE: 0.14001361287
    + base model level 2:
        * Ridge(): RMSE: 0.15786116707
        * Lasso(max_iter=2000, random_state=200). RMSE: 0.39956022133
        * ElasticNet(random_state=200). RMSE: 0.399560221333
        * BayesianRidge(). RMSE: 0.15345511789  
        * Kfolds = 4 
        * AVG RMSE: 0.143643700101
    + base model level 3:
        * XGBRegressor(n_estimators=100, max_depth=1, n_jobs=-1, random_state=123). RMSE: 0.13074541917
        * GradientBoostingRegressor(n_estimators=200, max_depth=1, random_state=357). RMSE: 0.13404585765347
        * AVG RMSE: 0.13052384489765   
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:  0.14179591521
    + score: 0.13245         
- Round 14:
    + base models level 1: Round 13
        * refined value transformation
        * AVG RMSE: 0.14001361287 => 0.13924047849100
    + base model level 2: Round 13
        * AVG RMSE: 0.143643700101 => 0.1434566227
    + base model level 3: Round 13
        * AVG RMSE: 0.13052384489765 => 0.12975319761   
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:  0.14179591521 => 0.14329289771467
    + score: 0.13245 => 0.13274            
- Round 15: => Best of day
    + base models level 1: Round 13
        * use scale features only
        * AVG RMSE: 0.13924047849100 => 0.139240478491
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:  0.14329289771467 => 0.14049318180020
    + score: 0.13274 => 0.12902     
- Round 16:
    + base models level 1: Round 13
        * use scale features only
        * AVG RMSE: 0.13924047849100 => 0.139240478491
    + base model level 2:
        * XGBRegressor(n_estimators=100, max_depth=1, n_jobs=-1, random_state=123). RMSE: 0.13074541917
        * GradientBoostingRegressor(n_estimators=200, max_depth=1, random_state=357). RMSE: 0.13404585765347
        * AVG RMSE: 0.13681841404           
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:  0.14049318180020 =>  0.14171050626484
    + score: 0.12902 => 0.13137        
- Round 17:
    + base models level 1: Round 16
        * use scale features only
        * AVG RMSE: 0.139240478491
    + base model level 2: Round 16
        * AVG RMSE: 0.13681841404           
    + Stack model: 
        * GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
        * train_predict_kfold. Kfold = 5
        * RMSE: 0.14171050626484 => 0.13380050134
    + score: 0.13137  => 0.13122
- Round 18:
    + base models level 1: Round 16
        * use scale features only
        * AVG RMSE: 0.139240478491
    + Stack model: 
        * GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
        * train_predict_kfold. Kfold = 5
        * RMSE: 0.14171050626484 => 0.12797831410
    + score: 0.12902  => 0.13095            

6. 2017-07-04:
- Round 19:
    + base models level 1: Round 16
        * use scale features only
        * AVG RMSE: 0.139240478491
    + Stack model: 
        * DeepNN(NUM_LAYERS = 4, NUM_HIDDEN_NODES = 256, NUM_EPOCHS = 200, LEARNING_RATE = 0.01, TRAIN_SPLIT = 0.7) 
        * RMSE: 0.14700643125096
    + score: 0.13234                
- Round 20:
    + base models level 1: Round 16
        * use scale features only
        * AVG RMSE: 0.139240478491
    + Stack model: 
        * DeepNN(NUM_LAYERS = 4, NUM_HIDDEN_NODES = 256, NUM_EPOCHS = 1000, LEARNING_RATE = 0.003, TRAIN_SPLIT = 0.7) 
        * RMSE: 0.14700643125096 =>  0.1434306788652 
    + score: 0.13234 => 0.13133                 
- Round 21: 
    + base models level 1: Round 16
        * use scale features only
        * AVG RMSE: 0.139240478491
    + Stack model: 
        * DeepNN(NUM_LAYERS = 5, NUM_HIDDEN_NODES = 512, NUM_EPOCHS = 1000, LEARNING_RATE = 0.003, TRAIN_SPLIT = 0.7) 
        * RMSE: 0.1434306788652 => 0.147927218428
    + score: 0.13133 =>
- Round 22:
    + base models level 1: Round 16
        * use scale features only
        * AVG RMSE: 0.139240478491
    + Stack model: 
        * DeepNN(NUM_LAYERS = 6, NUM_HIDDEN_NODES = 512, NUM_EPOCHS = 2000, LEARNING_RATE = 0.003, TRAIN_SPLIT = 0.7) 
        * RMSE: 0.147927218428 => 0.142672623445815
    + score: 0.13133 =>                                                       
- Round 23: => Best of DeepNN
    + base models level 1: Round 16
        * use scale features only
        * AVG RMSE: 0.139240478491
    + Stack model: 
        * DeepNN(NUM_LAYERS = 8, NUM_HIDDEN_NODES = 512, NUM_EPOCHS = 5000, LEARNING_RATE = 0.003, TRAIN_SPLIT = 0.7) 
        * RMSE: 0.142672623445815 => 0.14209155585
    + score: 0.13133 =>   0.13025                                                          
- Round 24:
    + base models level 1: Round 16
        * use scale features only
        * AVG RMSE: 0.139240478491
    + Stack model: 
        * DeepNN(NUM_LAYERS = 8, NUM_HIDDEN_NODES = 1024, NUM_EPOCHS = 2000, LEARNING_RATE = 0.001, TRAIN_SPLIT = 0.7)
        * Change cost function: tf.sqrt(tf.reduce_mean(tf.squared_difference(Y, Y_predicted))) 
        * RMSE: 0.14209155585 =>  0.1681132212845
    + score: 0.13157                                                              
- Round 25:
    + base models level 1: Round 16
        * use scale features only
        * AVG RMSE: 0.139240478491
    + Stack model: 
        * DeepNN(NUM_LAYERS = 8, NUM_HIDDEN_NODES = 1024, NUM_EPOCHS = 10000, LEARNING_RATE = 0.003, TRAIN_SPLIT = 0.7)
        * Change cost function: tf.sqrt(tf.reduce_mean(tf.squared_difference(Y, Y_predicted))) 
        * RMSE: 0.1681132212845 =>  0.2537479319629
    + score: 0.13157 =>
- Round 26: 
    + base models level 1: Round 16
        * use scale features only
        * fix object features transformation 
        * AVG RMSE: 0.139240478491 =>0.1355058111398
    + Stack model: 
        * DeepNN(NUM_LAYERS = 8, NUM_HIDDEN_NODES = 512, NUM_EPOCHS = 5000, LEARNING_RATE = 0.003, TRAIN_SPLIT = 0.7) 
        * RMSE: 0.14209155585 => 0.2414062362133
    + score:
7. 2017-07-04:
- Round 27: 
    + base models level 1: Round 13
        * use scale features only
        * fix object features transformation 
        * AVG RMSE: 0.1355058111398
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:  0.14049318180020 => 0.12344250891029
    + score: 0.12902 =>  0.13275
- Round 28:
    + base models level 1: Round 27
        * use scale features only
        * fix object features transformation 
        * AVG RMSE: 0.1355058111398
    + base model level 2: Round 16
        * AVG RMSE: 0.13681841404 => 0.13097810413267          
    + Stack model: 
        * GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
        * train_predict_kfold. Kfold = 5
        * RMSE: 0.13380050134 => 0.131216972978746
    + score: 0.13122 => 0.13395

8. 2017-07-05:
- Round 29: 
    + base models level 1: Round 27
        * features engineering
        * AVG RMSE: 0.1355058111398 => 0.134502529542809
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:  0.12344250891029 =>  0.12428772598089
    + score: 0.13275 => 0.13135
- Round 30: 
    + base models level 1: Round 29
        * features engineering
        * AVG RMSE: 0.134502529542809
    + base model level 2: Round 28
        * AVG RMSE: 0.13097810413267 => 0.1298229632071634             
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:  0.12428772598089 => 0.12917592235375225
    + score:0.13135 => 0.13107   
- Round 30: 
    + base models level 1: Round 29
        * add more features engineering: 90
        * AVG RMSE: 0.134502529542809 => 0.13534135362588134
    + base model level 2: Round 28
        * AVG RMSE: 0.1298229632071634 => 0.13169634265795227             
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:  0.12917592235375225 => 0.12827533932808857
    + score:0.13107 => 0.13087      