0. Best score:
  - 2017-07-01: Round 7, Best score:0.12909, Rank: 882, Top 46%
  - 2017-07-02: Round 9, Best score:0.12889, Rank: 872, Top 45%
1. 2017-06-30
- Rank 1: 0.01557
- Rank 2: 0.105672
- Rank 50: 0.11359

- Best score:  0.13001, Rank: 901 (top 47%)
- rmse: 0.14104798652556305
- Base models:
    + XGBRegressor(n_estimators=500, max_depth=5, n_jobs=-1)
    + GradientBoostingRegressor(n_estimators=500) => AdaBoostRegressor(base_estimator=model_temp, n_estimators=200, random_state=200)
    + ExtraTreesRegressor(n_estimators=500, n_jobs=-1)
    + DecisionTreeRegressor(max_depth=20) => AdaBoostRegressor(base_estimator=model_temp, n_estimators=200, random_state=200)
    + RandomForestRegressor(n_estimators=500, n_jobs=-1)
    + Kfolds = 5
    + n_features = 77
- Stack model:
    + GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')

2. 2017-06-30:
 -Round 1:
    + Boost: AdaBoostRegressor(base_estimator=model_temp, n_estimators=200, random_state=200)
    + XGBRegressor(n_estimators=500, max_depth=3, n_jobs=-1, random_state=123),: RMSE=0.1252588
    + ExtraTreesRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=456): RMSE= 0.143059197779
    + RandomForestRegressor(n_estimators=500, max_depth=10, n_jobs=-1, random_state=789): RMSE=0.142908791871
    + DecisionTreeRegressor(max_depth=10, random_state=146) => Boost: RMSE=0.141775155776
    + GradientBoostingRegressor(n_estimators=500, max_depth=1, random_state=357) => Boost : RMSE=0.140991803351
    + Kfolds = 5
    + n_features = 77 
    + AVG RMSE: 0.13879876178
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='lad')
    + Stack RMSE: 0.139057420149
    + score:  0.13250 
 - Round 2: (base models = Round 1)
    + AVG RMSE: 0.13879876178
    + Stack model: XGBRegressor(n_estimators=100, max_depth=1, n_jobs=-1)
    + Stack RMSE: 0.14280825157
    + score: 0.13313    
 3. 2017-07-01:
 - Round 3:
    + XGBRegressor(n_estimators=500, max_depth=3, n_jobs=-1, random_state=123)
    + AVG RMSE=0.1252588
    + Kfolds = 5
    + n_features = 77 
    + score: 0.13679
 - Round 4: (base models = Round 1)
    + AVG RMSE: 0.13879876178
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE: 0.14090396100286
    + score: 0.13018     
 - Round 5: (base models = Round 1) 
    + AVG RMSE: 0.13879876178
    + Stack Boost: AdaBoostRegressor(base_estimator=model_temp, n_estimators=500, learning_rate=0.1, random_state=200)
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber') => Boost
    + Stack RMSE:  0.1437158036472
    + score: 0.13299 < Round 4        
 -Round 6: 
    + data: non-scale features
    + Boost: AdaBoostRegressor(base_estimator=model_temp, n_estimators=200, random_state=200)
    + XGBRegressor(n_estimators=500, max_depth=3, n_jobs=-1, random_state=123),: RMSE=0.125400659888
    + ExtraTreesRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=456): RMSE= 0.143059197779
    + RandomForestRegressor(n_estimators=500, max_depth=10, n_jobs=-1, random_state=789): RMSE=0.142867999289
    + DecisionTreeRegressor(max_depth=10, random_state=146) => Boost: RMSE= 0.1414896008944
    + GradientBoostingRegressor(n_estimators=500, max_depth=1, random_state=357) => Boost : RMSE0.1432036356539
    + Kfolds = 5
    + n_features = 77 
    + AVG RMSE: 0.1392042187011
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE: 0.13831248511251
    + score:  0.13057 < 0.13018   
  -Round 7: Best score:0.12909, Rank: 882, Top 46%
    + num features: 77
    + data: non-scale features (Y) + scale features (log(Y))
    + Boost: AdaBoostRegressor(base_estimator=model_temp, n_estimators=200, random_state=200)
    + XGBRegressor(n_estimators=500, max_depth=3, n_jobs=-1, random_state=123),: RMSE=0.125400659888
    + ExtraTreesRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=456): RMSE= 0.143059197779
    + RandomForestRegressor(n_estimators=500, max_depth=10, n_jobs=-1, random_state=789): RMSE=0.142867999289
    + DecisionTreeRegressor(max_depth=10, random_state=146) => Boost: RMSE= 0.1414896008944
    + GradientBoostingRegressor(n_estimators=500, max_depth=1, random_state=357) => Boost : RMSE0.1432036356539
    + Kfolds = 5
    + n_features = 77 
    + AVG RMSE: 0.138798761786
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE: 0.138849117235557
    + score:  0.12909 => Best   
 - Round 8: (base models = Round 7)
    + num features: 80    
    + AVG RMSE:  0.13863344205998
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:   0.1391573479766
    + score: 0.13049 < Round 7        
    
 4. 2017-07-02:
 - Round 9:
    + base models level 1:
        * models: Round 7 
        * num features level 1: 80    
        * AVG RMSE:  0.13863344205998
    + base model level 2:
        * XGBRegressor(n_estimators=100, max_depth=1, n_jobs=-1, random_state=123). RMSE: 0.13074541917
        * GradientBoostingRegressor(n_estimators=200, max_depth=1, random_state=357). RMSE: 0.13404585765347
        * AVG RMSE: 0.132395638412
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:   0.14031113935
    + score: 0.12889 => Best
 
 5. 2017-07-03:    
 - Round 10:
    + fill NaN for special columns
    + base models level 1: Round 9
        * Fill null value for special columns
        * AVG RMSE:  0.13863344205998 => 0.140013612876
    + base model level 2: Round 9
        * AVG RMSE: 0.132395638412 => 0.13287786178
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:   0.14031113935   => 0.141457260264 
    + score: 0.13179
 - Round 11:
    + base models level 1: Round 9
        * Fill null value for special columns
        * AVG RMSE:  0.13863344205998 => 0.140013612876
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:  0.140411082228  
    + score: 0.13487 < Round 10
- Round 11:
    + base models level 1:
        * num features level 1: 80 
        * Ridge(): RMSE: 0.15786116707
        * Lasso(max_iter=2000, random_state=200). RMSE: 0.39956022133
        * ElasticNet(random_state=200). RMSE: 0.399560221333
        * BayesianRidge(). RMSE: 0.15345511789  
        * Kfolds = 4 
        * AVG RMSE: 0.277609181910
    + base model level 2:
        * Boost: AdaBoostRegressor(base_estimator=model_temp, n_estimators=200, random_state=200)
        * XGBRegressor(n_estimators=500, max_depth=3, n_jobs=-1, random_state=123),: RMSE=0.125400659888
        * ExtraTreesRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=456): RMSE= 0.143059197779
        * RandomForestRegressor(n_estimators=500, max_depth=10, n_jobs=-1, random_state=789): RMSE=0.142867999289
        * DecisionTreeRegressor(max_depth=10, random_state=146) => Boost: RMSE= 0.1414896008944
        * GradientBoostingRegressor(n_estimators=500, max_depth=1, random_state=357) => Boost : RMSE0.1432036356539
        * Kfolds = 4 
        * AVG RMSE: 0.14290463506
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:  0.15576363629  
    + score: 0.14517    
- Round 12:
    + base models level 1:
        * num features level 1: 80 
        * Ridge(): RMSE: 0.15786116707
        * Lasso(max_iter=2000, random_state=200). RMSE: 0.39956022133
        * ElasticNet(random_state=200). RMSE: 0.399560221333
        * BayesianRidge(). RMSE: 0.15345511789  
        * Kfolds = 4 
        * AVG RMSE: 0.277609181910
    + base model level 2:
        * Boost: AdaBoostRegressor(base_estimator=model_temp, n_estimators=200, random_state=200)
        * XGBRegressor(n_estimators=500, max_depth=3, n_jobs=-1, random_state=123),: RMSE=0.125400659888
        * ExtraTreesRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=456): RMSE= 0.143059197779
        * RandomForestRegressor(n_estimators=500, max_depth=10, n_jobs=-1, random_state=789): RMSE=0.142867999289
        * DecisionTreeRegressor(max_depth=10, random_state=146) => Boost: RMSE= 0.1414896008944
        * GradientBoostingRegressor(n_estimators=500, max_depth=1, random_state=357) => Boost : RMSE0.1432036356539
        * Kfolds = 5 
        * AVG RMSE: 0.14290463506
    + base model level 3:
        * XGBRegressor(n_estimators=100, max_depth=1, n_jobs=-1, random_state=123). RMSE: 0.13074541917
        * GradientBoostingRegressor(n_estimators=200, max_depth=1, random_state=357). RMSE: 0.13404585765347
        * AVG RMSE: 0.145246950   
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:  0.1565321638  
    + score: 0.14536    
- Round 13:
    + base models level 1:
        * num features level 1: 80
        * Boost: AdaBoostRegressor(base_estimator=model_temp, n_estimators=200, random_state=200)
        * XGBRegressor(n_estimators=500, max_depth=3, n_jobs=-1, random_state=123),: RMSE=0.125400659888
        * ExtraTreesRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=456): RMSE= 0.143059197779
        * RandomForestRegressor(n_estimators=500, max_depth=10, n_jobs=-1, random_state=789): RMSE=0.142867999289
        * DecisionTreeRegressor(max_depth=10, random_state=146) => Boost: RMSE= 0.1414896008944
        * GradientBoostingRegressor(n_estimators=500, max_depth=1, random_state=357) => Boost : RMSE0.1432036356539
        * Kfolds = 5  
        * AVG RMSE: 0.14001361287
    + base model level 2:
        * Ridge(): RMSE: 0.15786116707
        * Lasso(max_iter=2000, random_state=200). RMSE: 0.39956022133
        * ElasticNet(random_state=200). RMSE: 0.399560221333
        * BayesianRidge(). RMSE: 0.15345511789  
        * Kfolds = 4 
        * AVG RMSE: 0.143643700101
    + base model level 3:
        * XGBRegressor(n_estimators=100, max_depth=1, n_jobs=-1, random_state=123). RMSE: 0.13074541917
        * GradientBoostingRegressor(n_estimators=200, max_depth=1, random_state=357). RMSE: 0.13404585765347
        * AVG RMSE: 0.13052384489765   
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:  0.14179591521
    + score: 0.13245         
- Round 14:
    + base models level 1: Round 13
        * refined value transformation
        * AVG RMSE: 0.14001361287 => 0.13924047849100
    + base model level 2: Round 13
        * AVG RMSE: 0.143643700101
    + base model level 3: Round 13
        * AVG RMSE: 0.13052384489765   
    + Stack model: GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='huber')
    + Stack RMSE:  0.14179591521
    + score: 0.13245             

